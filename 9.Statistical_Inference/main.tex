
\input{../mode.tex}
\csname\pdfmode\endcsname

\mode<presentation>
{
  \usetheme{default}
  \usecolortheme{default}
  \usefonttheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]  % or "page number"
  \setbeamercolor{frametitle}{fg=white}
  \setbeamercolor{footline}{fg=black}
}

\input{../myhead.tex}

\title[]{Lecture 9: Introduction to Statistical Inference}
\author{Yi, Yung (이융)}
\institute{EE210: Probability and Introductory Random Processes\\ KAIST EE}
\date{\today}

\input{../mymath}


\begin{document}

\input{../mydefault}

\begin{frame}
  \titlepage
\end{frame}

% % Uncomment these lines for an automatically generated outline.
% \begin{frame}{Outline}
% % \tableofcontents
% \plitemsep 0.1in
% \bci
% \item

% \item
% \eci
% \end{frame}

% START START START START START START START START START START START START START

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item Overview on Statistical Inference

\item Bayesian Inference: Framework

\item Examples

\item MAP (Maximum A Posteriori) Estimator

\item LMS (Least Mean Squares) Estimator

\item LLMS (Linear LMS) Estimator

\item Classical Inference: ML Estimator

%\ece
  % \medskip

% \item Classical Inference

%   \bce
% \item ML (Maximum Likelihood) Estimator
%   \ece
  
  \ece

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(1)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \redf{Overview on Statistical Inference}

\item \grayf{Bayesian Inference: Framework

\item Examples

\item MAP (Maximum A Posteriori) Estimator

\item LMS (Least Mean Squares) Estimator

\item LLMS (Linear LMS) Estimator

\item Classical Inference: ML Estimator}

  % \ece
  % \medskip

% \item Classical Inference

%   \bce
% \item ML (Maximum Likelihood) Estimator
%   \ece
  
  \ece

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Roadmap}

% \plitemsep 0.1in

% \bci

% \item \redf{Basics on Statistic Inference}

% \item \redf{Framework of Bayesian Inference}

% \item MAP (Maximum A Posteriori) Estimator

% \item LMS (Least Mean Squares) Estimator

% \item LLMS (Linear LMS) Estimator

% \medskip
% \item Framework of Classical Inference
% \item ML (Maximum Likelihood) Estimator

% \eci

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What is Statistical Inference?}

\plitemsep 0.07in
\bci

\item Examples
  
  \bci

\item<2-> Take 1000 voters uniformly at random, and count the popularity
  of each candidate to infer the true popularity.

\item<3-> COVID-19 has spread over a collection of people, and we collect a sample of COVID-19
  infectees to infer the true source of infection. 

\item<4-> When an original signal $S$ is transmitted over the KAIST Wi-Fi
  connection, the received signal $X$ becomes $X = aS + W,$ where $ 0 <
  a < 1  $ and $W \sim \set{N}(0,1).$ If we have 10 samples of $(S,X)$
  values, what is the inferred value of $a$?

  \mypic{0.3}{L6_comm_ex.png}
  \eci

\item<5-> Process of extracting information about an \bluef{unknown variable} or
  an \bluef{unknown model} from \redf{noisy available data}

% \item<5-> Process of drawing conclusions about \bluef{population
%     parameters} based on
%   \redf{noisy samples} taken from the \redf{population}

  \eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Statistical Inference: Three Main Ideas}

\plitemsep 0.2in
\bce

\item<2-> Samples are likely to be a good represetnation of the unknown

\item<3-> There exists uncertainty (i.e., noise) as to how well the sample
  represents the unknown

\item<4-> How to obtain samples has impact on inference (e.g., when we
  need to pay for online surveys)
  
  \ece

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Inference, Real World, Probability Theory}

\begin{center}
\mypic{0.5}{L6_bigpicture.png}
\vspace{-0.7cm}
{\scriptsize Source: Introduction to Probability course by MIT}
\end{center}

\vspace{-0.5cm}
\plitemsep 0.05in
\bci
\item<2-> Inference

- Using data, probabilistic models or parameters for models are determined.

\item<3-> Why building up models?

- Analysis is possible, so that predictions and decisions are made.

\item<4-> Recently, deep learning

- Connecting big data and big model building

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What to Infer?: Unknown Model vs. Unknown Variable}

\begin{center}
\mypic{0.5}{L6_comm_ex.png}
\end{center}

\vspace{-0.5cm}
\plitemsep 0.05in
\bci
\item $X = aS + W$

\item<2-> Model building
\bci
\item know the original signal $S$, observe $X$
\item infer the model parameter $a$
\eci

\item<3-> Variable estimation
\bci
\item know $a,$ observe $X$
\item infer the original signal $S$
\eci


\item<4-> Same mathematical structure, because the parameters in models are variables in many cases

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What Kind?: Hypothesis Testing vs. Estimation}

\plitemsep 0.1in
\bci
\item<2-> \redf{Hypothesis testing}

\bci
\item<3-> Unknown: a few possible ones
\item<4-> Goal: small probability of incorrect decision
\item<5-> \bluef{(Ex)} Something detected on the radar. Is it a bird or an airplane?
\eci

\item<2-> \redf{Estimation}
\bci
\item<3-> Unknown: a value included in  an infinite,  typically continuous set
\item<4-> Goal: Finding the value close to the true value
\item<5-> \bluef{(Ex)} Biased coin with unknown probability of head $\theta \in [0,1]$. Data of heads and tails. What is $\theta$?
\item<6-> \bluef{(Note)} If you have the candidate values of $\theta = \{1/4, 1/2, 3/4 \},$ then it's a hypothesis testing problem
\eci

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Different Views: Bayesian vs. Classical (1)}

\onslide<1->{- Biased coin with parameter $\theta$ (probability of head). Assume that $\theta \in \{1/4,3/4\}.$}

\onslide<2->{- Throw the coin 3 times and get $(H,H,H).$ Goal: infer $\theta,$ $1/4$ or $3/4$?}

% \plitemsep 0.1in
% \bci
% \item
% \eci
\medskip

\mytwocols{0.55}
{
\small
\plitemsep 0.05in
\bci
\item<3-> Distribution of $\theta$ (\redf{prior}) e.g.,
\aleq{
\cbprob{\theta = {3 \over 4}} = 1/2, \quad \cbprob{\theta = \frac{1}{4}} = 1/2
}
\item<4-> Use Bayes' rule and find the \redf{posterior}:
\aleq{
\bprob{\theta = \frac{3}{4} \Big | (HHH)}=\frac{27}{28}, \ \bprob{\theta = \frac{1}{4} \Big | (HHH)} = \frac{1}{28}
}
\item<5-> Choose $\theta$ with larger posterior probability.

\item<8-> \empr{Bayesian approach} (Chapter 8)
\eci

}
{
\small
\plitemsep 0.05in
\bci
\item<6-> Find the probability of $(H,H,H),$ if $\theta=\frac{1}{4}$ or $\frac{3}{4}$ (\redf{likelihood})
\aleq{
\bprob{ (HHH) | \theta=\frac{3}{4}} &= \lf(\frac{3}{4} \ri)^3 \cr
\bprob{ (HHH) | \theta=\frac{1}{4}} &= \lf (\frac{1}{4} \ri)^3
}

\item<7-> Choose $\theta$ with a larger likelihood.

\item<8-> \empr{Classical approach} (Chapter 9)

\eci

}

\onslide<9->{
\orangef{(Note)} There are other inference methods, and here we just show
examples.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Different Views: Bayesian vs. Classical (2)}

\mytwocols{0.55}
{
\bluef{Bayesian approach}


\plitemsep 0.08in
\bci
\item<2-> \redf{Unknown}: \orangef{random variable} with some distribution (prior)
\item<3-> Unknown model as chosen randomly \magenf{from a give model class}
\item<4-> Observed data $x$ gives:

  - \greenf{posterior distribution} $p_{\Theta | X}(\theta | x)$
\item<5-> Choose $\theta$ with larger posterior probability \onslide<7->{\magenf{(other methods exist)}}
\eci
}
{
\bluef{Classical approach}

\plitemsep 0.08in
\bci
\item<2-> \redf{Unknown}: \orangef{deterministic value}

  \mbox{}
\item<3-> Unknown model as \magenf{one of multiple probabilistic models}
\item<4-> Observed data $x$ gives:

  - \greenf{likelihood} $p(X;\theta)$
\item<5-> Choose $\theta$ with larger likelihood\\ \onslide<7->{\magenf{(other methods exist)}}
\eci
}
% \medskip


% \centering
% \onslide<6->{
%   - Who is the winner? A century-long debate 
% }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Different Views: Bayesian vs. Classical (3)}


\plitemsep 0.1in
\bci
\item Fundamental difference about the nature of unknown models or variables

\item Random variable or deterministic quantity 

\item Who is the winner? A century-long debate 
  
\item Example of debate: mass of the electron by noisy measurement
  \bci
\item \redf{Classical.} while unknown, it is a constant and there is no
  justification for modeling it as a random variable. 
  
\item \redf{Bayesian.}  Prior distribution reflects our state of knowledge,
  e.g., some range of candidate values from our previous noisy
  measurements. 
  \eci

\item Particular prior? too arbitrary vs. every statistical
  procedure's hidden choices  

\item Pratical issues: Bayesian approach is often computationally
  intractable (multi-dimensional integrals)

  \eci


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(2)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Overview on Statistical Inference}

\item \redf{Bayesian Inference: Framework}

\item \grayf{Examples

\item MAP (Maximum A Posteriori) Estimator

\item LMS (Least Mean Squares) Estimator

\item LLMS (Linear LMS) Estimator

\item Classical Inference: ML Estimator}

% \ece
  % \medskip

% \item Classical Inference

%   \bce
% \item ML (Maximum Likelihood) Estimator
%   \ece
  
  \ece

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Framework of Bayesian Inference}

\begin{center}
\mypic{0.65}{L6_BI_framework.png}
\end{center}

\vspace{-0.4cm}
\mytwocols{0.55}
{
\small
  \plitemsep 0.03in
\bci
\item<2-> Unknown $\Theta$

\bci
\item physical quantity or model parameter
\item random variable
\item \redf{prior} distribution $p_{\Theta}$ and $f_{\Theta}$
\eci

\item<3-> Observations or measurements $X$

\bci
\item \redf{observation model} $p_{X|\Theta}$ and $f_{X|\Theta}$
\eci

\item<4-> That is, the \redf{joint distribution} of $X$ and $\Theta$ ($p_{X,\Theta}(x,\theta)$ and $f_{X,\Theta}(x,\theta)$) is given
\item<5-> Find the \redf{posterior} distribution $p_{\Theta|X}$ and
  $f_{\Theta|X},$ using Bayes' rule.

  \eci
}
{
\small
  \plitemsep 0.08in
\bci

\item<5-> The posterior distribution is the complete answer of the
  Bayesian inference.

\item<6-> However, one may use it for further processing,
  depending on what he/she wants, e.g., point estimation. 
  
\item<7-> \orangef{Multiple} observations and \orangef{multiple} parameters are possible
  \bci
\item   $X = (X_1, \ldots, X_n)$
  \item $\Theta = (\Theta_1, \ldots, \Theta_n)$
  \eci

  \eci
}
\end{frame}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Remind: Bayes' Rule: 4 Versions}

  \mytwocols{0.6}
  {
    \plitemsep 0.07in
    \bci
  \item $\Theta$: discrete, $X$: discrete
\aleq{
\redf{\pthcx} & = \frac{\pth \pxcth}{\px}\cr
\px &= \sum_{\theta'} p_\Theta(\theta')p_{X|\Theta}(x|\theta')
}
  \item $\Theta$: continuous, $X$: continuous
\aleq{
\redf{\fthcx} & = \frac{\fth \fxcth}{\fx}\cr
\fx &= \int f_\Theta(\theta')f_{X|\Theta}(x|\theta') d\theta'
}
    \eci
  }
  {
    \plitemsep 0.07in
    \bci
  \item $\Theta$: discrete, $X$: continuous
    \aleq{
      \redf{\pthcx} &= \frac{\redf{\pth} \bluef{\fxcth}}{\bluef{\fx}}\cr
      \bluef{\fx} &= \sum_{\theta'} \redf{p_{\Theta}(\theta')} \bluef{f_{X|\Theta}(x|\theta')}
    }

  \item $\Theta$: continuous, $X$: discrete
    \aleq{
      \bluef{\fthcx  } &= \frac{\bluef{\fth} \redf{ \pxcth }  }{\redf{\px}} \cr
      \redf{\px} &= \int \bluef{f_{\Theta}(\theta')} \redf{p_{X|\Theta}(x|\theta')} d\theta'
    }

    \eci
  }
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(3)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Overview on Statistical Inference}

\item \grayf{Bayesian Inference: Framework}

\item \redf{Examples}

\item \grayf{MAP (Maximum A Posteriori) Estimator

\item LMS (Least Mean Squares) Estimator

\item LLMS (Linear LMS) Estimator

\item Classical Inference: ML Estimator}
%\ece
  % \medskip

% \item Classical Inference

%   \bce
% \item ML (Maximum Likelihood) Estimator
%   \ece
  
  \ece

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet, Single Observation}

\plitemsep 0.05in
\bci
\item<1-> Romeo and Juliet start dating, where 
Romeo is late by $X \sim \set{U}[0,\theta].$
\item<2-> Unknown: $\theta$ modeled by a rv $\Theta \sim \set{U}[0,1].$
\item<2-> Observation: Romeo was late by $x.$
% \item<3-> \question Given the observation sample $x$, what is $\hat{\theta}_\text{MAP}$? 
% \item<4-> \bluef{Intuition.} As $x$ grows, $\hat{\theta}_\text{MAP}$
%   decreases or increases?  \onslide<5->{\magenf{Increases. Why?}}

\item<3-> Prior \onslide<4->{and observation model (likelihood)}
  \aleq{
    \onslide<3->{\fth =
      \begin{cases}
        1, & 0 \le \theta \le 1 \cr
        0, & \text{otherwise}
      \end{cases},}
    \qquad 
    \onslide<4->{\fxcth =
      \begin{cases}
        \frac{1}{\theta}, & 0 \le x \le \theta\cr
        0, & \text{otherwise}
      \end{cases}}
  }
  
\item<5-> Posterior
  \aleq{
    \onslide<5->
    { \fthcx = \frac{\fth \fxcth}{\int_0^1 f_{\Theta}(\theta')f_{X|\Theta}(x | \theta') d\theta'}
      =}
    \onslide<6->{
      \begin{cases}
        \frac{1/\theta}{\int_{x}^1 \frac{1}{\theta'} d\theta'} = \frac{1}{\theta |\log x|},&  x \le \theta \le 1, \cr
        0, & \theta < x \text{ or } \theta >1
      \end{cases}
    }
  }
  \eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet, Multiple Observations}

{\large $\circ$ What happens if we have more observation samples?}
  
  

\plitemsep 0.1in
\bci
\item Romeo was late \redf{$n$ times} by $\magenf{\vec{X}=(X_1,X_2, \ldots, X_n)}, \ X_i \sim \set{U}[0,\theta].$
\item $X_1,\ldots, X_n$ are conditionally independent, given $\Theta = \theta.$
\item Unknown: $\theta$ modeled by a rv $\Theta \sim \set{U}[0,1].$
\item Observation: Romeo was late \redf{$n$ times} by $\magenf{\vec{x} = (x_1, x_2, \ldots, x_n)}$
% \item \question Given the observation sample $\vec{x}$, what is $\hat{\theta}_\text{MAP}$? 
% \item \bluef{Intuition.} We will do more accurate inference due to
%   more observation samples. 
\item See Example 8.2 at pp. 414 for more detailed treatment.   

  \eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Spam Filtering}

\plitemsep 0.05in
\bci
\item<1-> E-mail: \redf{spam} (1) or \bluef{legitimate} (2), $\Theta  \in \{1,2\},$ with prior $p_{\Theta}(1)$ and  $p_{\Theta}(2).$

\item<2-> $\{w_1, w_2, \ldots, w_n \}$: a collection of words which suggest ``spam".

\item<3-> For each $i,$ a Bernoulli $X_i=1$ if $w_i$ appears and 0 otherwise.

\item<4-> Observation model $p_{X_i|\Theta(x_i|1)}$ and
  $p_{X_i|\Theta(x_i|2)}$ are known.

\item<4-> Assumption: Conditioned on $\Theta,$ $X_i$ are independent.
\item<5-> Posterior PMF
 \aleq{
 \bprob{\Theta = m | (x_1,\ldots, x_n)} = \frac{p_{\Theta}(m) \prod_{i=1}^n p_{X_i|\Theta}(x_i|m)  }{
 \sum_{j=1,2} p_{\Theta}(j) \prod_{i=1}^n p_{X_i|\Theta}(x_i|j) 
 }, \quad m=1,2
}
% \item<6-> MAP rule for this hypothesis testing problem. Decided that the message is \redf{spam} if
% \aleq{
% p_{\Theta}(1) \prod_{i=1}^n p_{X_i|\Theta}(x_i|1) > p_{\Theta}(2) \prod_{i=1}^n p_{X_i|\Theta}(x_i|2)
% }
\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Biased Coin with Beta Prior (1)}

\plitemsep 0.1in
\bci

\item<1-> Biased coin with probability of head $\theta$
\item<2-> Unknown $\theta$: modeled by $\Theta$ with some prior $\fth$
\item<3-> Observation $X$: number of heads out of $n$ tosses
% \item<4-> Posterior PDF
% \aleq{
% f_{\Theta | X}(\theta | k) &= c \fth \redf{p_{X|\Theta}(k | \theta)}= c \redf{{n \choose k}} \fth \redf{\theta^k(1-\theta)^{n-k}}, \ \text{$c$ the normalizing constant}
% }

\item<4-> \question Suppose that you have freedom to choose the form of the prior
  distribution. What prior will you choose? \greenf{Requirement of ``good'' priors?}

\item<5-> We will look at the prior whose distribution is something
  called the \bluef{Beta distribution}.

\eci


% \mytwocols{0.3}
% {
% \small
% }
% {
% \small
% \plitemsep 0.07in
% \bci

% \item
% \eci
% }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Background: Beta Distribution}

%\orangef{If $\Theta \sim \text{Beta}(\alpha,\beta),$ what is $\hth_\MAP$?}

  \onslide<2->{
\myblock{Beta distribution}
{
\small
A continuous rv $\Theta$ follows a beta distribution with integer parameters $\alpha,\beta >0,$ if
\aleq{
\fth =
\begin{cases}
\onslide<3->{\redf{\frac{1}{B(\alpha,\beta)}}} \theta^{\alpha -1}(1-\theta)^{\beta -1}, & 0< \theta < 1, \cr
0, & \text{otherwise},
\end{cases}
}
\onslide<3->{where \redf{$B(\alpha,\beta)$}, called Beta function, is a
  normalizing constant,}
\onslide<4->{given by
\aleq{
\redf{B(\alpha,\beta)} = \int_0^1 \theta^{\alpha -1}(1-\theta)^{\beta -1} d\theta = \frac{(\alpha-1)! (\beta -1)!}{(\alpha + \beta-1)!}
}
}
\vspace{-0.3cm}
}}


\vspace{-0.4cm}
\plitemsep 0.05in
\bci

\item<5-> See \url{https://youtu.be/8yaRt24qA1M} for
  the integration in the Beta function formula.

\item<6-> A special case of $\text{Beta}(1,1)$ is $\set{U}[0,1]$

  %\item<2-> If $\Theta \sim Beta(\alpha,\beta),$ then $\Theta|\{X=k\} \sim Beta(k+\alpha,n-k+\beta)$

% - Very useful: \redblank{3}{Beta prior $\imp$ Beta posterior} (homework problem)

% \item MAP estimate
% \aleq{
% \hth_\MAP = \arg \max_{\theta} (k \log \theta + (n-k) \log (1-\theta))
% = \frac{k}{n}
% }

% \item Conditional expectation estimate = $\frac{k+1}{n+2}$ (homework problem)

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Biased Coin with Beta Prior (2)}

\mycolorbox{
\bci
\item If $\Theta \sim \text{Beta}(\alpha,\beta),$ then $\Theta|\{X=k\} \sim \text{Beta}(\redf{k}+\alpha,\redf{n-k}+\beta)$
\item In other words, Beta prior $\imp$ Beta posterior (why useful?)
\eci
}

\plitemsep 0.07in
\bci

\item[] \redf{Proof.}
  \bce[(a)]
\item   First, the posterior pdf is given by:
$\displaystyle f_{\Theta | X}(\theta | k) = c \bluef{\fth} \redf{p_{X|\Theta}(k | \theta)}= c \redf{{n \choose k}} \bluef{\fth} \redf{\theta^k(1-\theta)^{n-k}}, \ \text{$c$ the normalizing constant}
$

\item Next, for $\displaystyle \text{Beta}(\alpha,\beta)$ prior,
$\displaystyle \bluef{\fth} = \frac{1}{B(\alpha,\beta)}\theta^{\alpha-1} (1-\theta)^{\beta
  -1}$.

\item 
Then, 
$
\displaystyle \fthck= c {n \choose k} \bluef{\fth} \theta^k(1-\theta)^{n-k} = \frac{d}{B(\alpha,\beta)} \cdot \magenf{\theta^{\alpha+k-1}(1-\theta)^{\beta + n-k -1}},
$

where $d = c{n \choose k}.$
  \ece
\eci

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example 3: Biased Coin with Beta Prior (3)}

% \mycolorbox{
% \bci
% \item If $\Theta \sim \text{Beta}(\alpha,\beta),$ then $\Theta|\{X=k\} \sim \text{Beta}(\redf{k}+\alpha,\redf{n-k}+\beta)$
% \item In other words, Beta prior $\imp$ Beta posterior (why useful?)
% \item $\fthck \propto \theta^{\alpha+k-1}(1-\theta)^{\beta + n-k -1}$
% \eci
% }

% \plitemsep 0.07in
% \bci

% \item<2-> MAP estimate: Taking the logarithm,
% \aleq{
% \onslide<3->{\hth_\MAP &= \arg \max_{\theta} \Bl [(\alpha+k-1) \log \theta + (\beta + n-k+1) \log(1-\theta) \Bl ]
% &=\frac{\alpha + k -1}{\alpha + \beta -2 + n}}
% }

% \item<4-> When $\alpha=\beta =1$ (i.e., $\set{U}[0,1]$ prior), \orangef{$\hth_\MAP = \frac{k}{n}$}


% \eci

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Parameter Inference with Normal Prior (1)}

\redf{\noindent$\circ$ Inference of a parameter $\theta$}

  \mytwocols{0.7}
  {
    \plitemsep 0.07in
    \bci
    
  \item<6-> \magenf{Single} observation

  \item<2-> $X$: noisy observation of $\theta$, modeled as:

    \bluef{$X = \theta + W$, where $W \sim \set{N}(0,\sigma^2)$}

  \item<3-> Model $\theta$ with a rv $\Theta \sim \set{N}(x_0,\sigma_0^2)$
    (normal prior)
  \item<4-> $\Theta$ and $W$ are indendent
    
  \item<5-> \question Given an observation $x,$ what is the posterior $\fthcx$?
      
    \eci
  }
  {
    \plitemsep 0.03in
    \bci
    
  \item<6-> \magenf{Multiple} $n$ observations

  \item<7-> $n$ observations of $\theta$: $W_i \sim \set{N}(0,\sigma_i^2)$
\bluef{ 
    \aleq{
    X_1 &= \theta + W_1, \quad W_1 \sim \set{N}(0,\sigma_1^2)\cr
       \vdots & \cr
      X_n &= \theta + W_n, \quad  W_n \sim \set{N}(0,\sigma_n^2)
    }
}    
\vspace{-0.6cm}
\item<8-> Model $\theta$ with $\Theta \sim \set{N}(x_0,\sigma_0^2)$
  \item<8-> $\Theta, W_1, \ldots, W_n$ are indendent
    
  \item<9-> \question Given an observation $x,$ what is
    the posterior $\fthcx$? 
\bci
    \item $X = (X_1, \ldots, X_n)$ and $x=(x_1, \ldots, x_n),$      
\eci
    \eci

  }


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Background: The PDF Form of Gaussian}

\onslide<2->{
  \mycolorbox{
  \redf{Lemma.} Up to recaling, the pdf of the form
  $\displaystyle e^{-\frac{1}{2}(ax^2 -2bx +c)}$ is $\set{N}(\frac{b}{a},\frac{1}{a}).$  
}
}

  \plitemsep 0.2in
\bci 

\item<3-> \bluef{(Rough) Proof.} Note that the pdf of $\set{N}(\mu,\sigma^2)$: $\displaystyle \fx =
  e^{-(x-\mu)^2/2\sigma^2}$ up to rescaling. Then,

  \bci
\item<4-> $\displaystyle - \frac{1}{2 \sigma^2} ( x^2 - 2\mu x + \mu^2) = -\frac{1}{2}(ax^2 -2bx +c)$
  
\item<5-> Thus, $\sigma^2 = \dfrac{1}{a}$ and $\dfrac{\mu}{\sigma^2} = b
  \implies \mu = b \sigma^2 = \dfrac{b}{a}  $

  \eci
  
  
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Background: Product of Two Gaussian Densities}

\mycolorbox{
  \thm The product of two Gaussian pdfs $\set{N}(\mu_0,\nu_0)$ and
  $\set{N}(\mu_1,\nu_1)$ is $\displaystyle \set{N}\left(\frac{\nu_1\mu_0 + \nu_0\mu_1}{\nu_0 + \nu_1}, \frac{\nu_0\nu_1}{\nu_0+\nu_1} \right).$
}

\plitemsep 0.1in
\bci 

\item<2->[\bluef{Proof.}] Using the Lemma in the previous slide, i.e.,
up to recaling, the pdf of the form $\displaystyle e^{-\frac{1}{2}(ax^2 -2bx +c)}$ is $\set{N}(\frac{b}{a},\frac{1}{a}),$  
\aleq{
&\onslide<3->{\exp \left(-(x-\mu_0)^2/2\nu_0 \right) \times \exp\left(-(x-\mu_1)^2/2\nu_1\right)} \cr
& \onslide<4->{=\exp\left[-\frac{1}{2}\left(\Bl(\frac{1}{\nu_0} + \frac{1}{\nu_1}\Br)x^2 -2 \Bl(\frac{\mu_0}{\nu_0} + \frac{\mu_1}{\nu_1}\Br)x + c \right) \right ]} \cr
& \onslide<5->{\implies \set{N}\left(
\nu \left(\frac{\mu_0}{\nu_0} + \frac{\mu_1}{\nu_1}\right),
  \overbrace{\frac{1}{\nu_0^{-1} + \nu_1^{-1}}}^{= \nu}
\right)
= \set{N}\left(\frac{\nu_1\mu_0 + \nu_0\mu_1}{\nu_0 + \nu_1}, \frac{\nu_0\nu_1}{\nu_0+\nu_1} \right)}
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Background: Product of $n+1$ Gaussian Densities}

\mycolorbox{
  \thm The product of $n+1$ Gaussian pdfs $\set{N}(\mu_0,\nu_0),$
  $\set{N}(\mu_1,\nu_1), \ldots,$ $\set{N}(\mu_n,\nu_n),$
  is $\displaystyle \set{N}(\mu,\nu),$ where
$$
\onslide<2->{  \mu = \frac
    {\sum_{i=0}^n \frac{\mu_i}{\nu_i}}
    {\sum_{i=0}^n \frac{1}{\nu_i}},} \qquad
  \onslide<3->{\nu =
    \frac{1}{\sum_{i=0}^n \frac{1}{\nu_i^2}}}
$$
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Parameter Inference with Normal Prior (2)}

    \plitemsep 0.03in
    \bci
    
  \item $n$ observations of $\theta$: $W_i \sim
    \set{N}(0,\sigma_i^2),$ and $\theta$ with the normal prior $\Theta \sim \set{N}(x_0,\sigma_0^2)$
    \mycolorbox{
\centering
      $X_i = \theta + W_i, \quad W_i \sim \set{N}(0,\sigma_i^2),$ 
    $\quad i=1, \ldots, n$
    }

  \item $\Theta, W_1, \ldots, W_n$ are indendent and
    let  $X = (X_1, \ldots, X_n),$ $x=(x_1, \ldots, x_n).$      

  \item Our interest. The \redf{poterior pdf} $\fthcx.$ 


    \medskip
  \item<2-> \bluef{Prior.} $\displaystyle \fth = c_1\cdot \exp\left\{ -
        \frac{(\theta - x_0)^2}{2 \sigma_0^2}  \right\}$
\item<3-> \bluef{Observation model.} Noting that $X_1, X_2, \ldots, X_n$
  are indpendent, 
  \aleq
  {
    \fxcth = c_2 \cdot \exp \left\{ -
        \frac{(\theta - x_1)^2}{2 \sigma_1^2} \right\}\cdots \exp \left\{ -
        \frac{(\theta - x_n)^2}{2 \sigma_n^2}\right\}
    }
\eci



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Parameter Inference with Normal Prior (3)}

  \begin{textblock*}{\textwidth}(9cm,0.8\textheight)
    \mysizebox{0.44}{
       $\fthcx  = \frac{\fth \fxcth}{        \int f_\Theta(\theta')f_{X|\Theta}(x|\theta') d\theta' }$
     }
  \end{textblock*}


  \plitemsep 0.03in
  \bci
  
\item<2-> \bluef{Numerator:} $\fth \fxcth = c_1c_2\cdot \exp \left\{ -
    \sum_{i=0}^n \frac{(x_i - \theta)^2}{2 \sigma_i^2}  \right\}$,
  which can be reexpressed as the following, using the \magenf{product of
  $n+1$ Gaussians}:
  \aleq{
c_1c_2\cdot \exp \left\{ -
  \sum_{i=0}^n \frac{(x_i - \theta)^2}{2 \sigma_i^2}  \right\} =
d\cdot 
\exp \left\{ -
    \frac{(\theta-m)^2}{2 v}  \right\},
  }
  where
$
\displaystyle
    m = \frac
    {\sum_{i=0}^n \frac{x_i}{\sigma_i^2}}
    {\sum_{i=0}^n \frac{1}{\sigma_i^2}}, \qquad v =
    \frac{1}{\sum_{i=0}^n \frac{1}{\sigma_i^2}}
$
  

\item<3-> \bluef{Denominator:} just a constant, not a function of $\theta$
% \item Thus, the posterior pdf $\displaystyle \fthcx = a \cdot \exp \left\{ -
%     \frac{(\theta=m)^2}{2 v}  \right\}$

  \eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Parameter Inference with Normal Prior (4)}

  % \begin{textblock*}{\textwidth}(9cm,0.75\textheight)
  %   \mysizebox{0.44}{
  %      $\fthcx  = \frac{\fth \fxcth}{        \int f_\Theta(\theta')f_{X|\Theta}(x|\theta') d\theta' }$
  %    }
  % \end{textblock*}

  \plitemsep 0.03in
  \bci
  
\item<1-> Thus, the posterior pdf $\displaystyle \fthcx$ =
\onslide<2->{
  $a \cdot \exp \left\{ -
    \frac{(\theta - m)^2}{2 v}  \right\},$ where
  $$
  m = \frac
  {\sum_{i=0}^n \frac{x_i}{\sigma_i^2}}
  {\sum_{i=0}^n \frac{1}{\sigma_i^2}}, \qquad v =
  \frac{1}{\sum_{i=0}^n \frac{1}{\sigma_i^2}}
  $$
 }
\item<3-> \redf{Prior: Normal, Posterior: Normal}

\item<4-> Special case when $\bluef{\sigma^2} = \sigma_0^2= \sigma_1^2=
  \cdots = \sigma_n^2.$ Then,  
  $$
  m = \frac{x_0 + x_1 + \ldots x_n}{n+1}, \qquad v = \frac{\sigma^2}{n+1}
  $$
  \bci
\item<5-> the prior mean $x_0$ acts just as \magenf{another observation}.
\item<6-> the standard deviation of the posterior goes to 0, at the
  rough rate of $1/\sqrt{n}.$
  \eci

  \eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Why Prior == Posteror is Useful?}

  % \begin{textblock*}{\textwidth}(9cm,0.75\textheight)
  %   \mysizebox{0.44}{
  %      $\fthcx  = \frac{\fth \fxcth}{        \int f_\Theta(\theta')f_{X|\Theta}(x|\theta') d\theta' }$
  %    }
  % \end{textblock*}

  \plitemsep 0.07in
  \bci
  
\item<2-> \redf{Recursive inference} is possible.
  
\item<3-> Suppose that after $X_1, \ldots, X_n$ are observed, an
  additional observation $X_{n+1}$ is observed.

\item<4-> Instead of solving the inference problem from scratch, we can
  \redf{view $f_{\Theta | X_1, \ldots, X_n}$ as our prior}, use the new
  observation to obtain the new posterior  $f_{\Theta | X_1, \ldots, X_n,X_{n+1}}$

\item<5-> In the example of parameter inference with the Normal prior,
  with the new observation $x_{n+1} \sim \set{N}(x_{n+1},
  \sigma^2_{n+1})$, the posterior pdf is nothing but the Normal pdf of:
  \aleq
  {
\onslide<6->{
    \text{\bluef{mean}} = \frac{(m/v) + (x_{n+1}/\sigma^2_{n+1})}{(1/v) +
      (1/\sigma^2_{n+1})}, \qquad
    \text{\bluef{variance}} = \frac{1}{(1/v) +
      (1/\sigma^2_{n+1})}
}
  }
  \eci

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(4)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Overview on Statistical Inference}

\item \grayf{Bayesian Inference: Framework}

\item \grayf{Examples}

\item \redf{MAP (Maximum A Posteriori) Estimator}

\item \grayf{LMS (Least Mean Squares) Estimator

\item LLMS (Linear LMS) Estimator

\item Classical Inference: ML Estimator}
%\ece
  % \medskip

% \item Classical Inference

%   \bce
% \item ML (Maximum Likelihood) Estimator
%   \ece
  
  \ece

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Point Estimation}

\mypic{0.55}{L6_BI_framework.png}
\vspace{-0.9cm}
\mypic{0.5}{L6_posterior.png}

\plitemsep 0.1in
\bci

\item<2-> \redf{Point Estimate}
\bci

\item   Given observation $x$, which \bluef{single} value $\theta$ are you
  going to choose as your inference result? People often want just the
  summary and a simple answer. 

\item Very often, $\theta$, our inference target, is by nature a
  single value, i.e., mass of the electron. 
  \eci
% \item Using the posterior distribution, apply one of the
%   \redf{inferernce methods}
%   of estimating the final $\hat{\theta}.$ 

% \item<3-> \orangef{Multiple} observations and \orangef{multiple} parameters are possible
%   \bci
% \item   Vectors: $X = (X_1, \ldots, X_n),$ $\Theta = (\Theta_1, \ldots, \Theta_n)$
%   \eci

  \eci

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Two Natural Point Estimates}

\vspace{-0.6cm}
\mypic{0.6}{L6_posterior.png}
\vspace{-0.6cm}

\plitemsep 0.05in
\bci
% \item<1-> Given observation $x$, which single value $\theta$ are you
%   going to choose as your inference result?

\item<2->[M1.] Choose the \redf{largest}: Maximum a posteriori probability (MAP) rule

\begin{center}
\redblank{3}{
$
\hat{\theta}_{\text{MAP}} =  \arg \max_{\theta} p_{\Theta | X}(\theta | x), \quad \hat{\theta}_{\text{MAP}} =  \arg \max_{\theta} f_{\Theta | X}(\theta | x)
$
}
\end{center}

\item<4->[M2.] Choose the \redf{mean}: Conditional expectation, aka LMS (Least Mean Square)

\begin{center}
\redblank{5}{
$\hat{\theta}_{\text{LMS}} = \expect{\Theta | X=x}$
}
\end{center}

\item<6-> Why MAP and LMS are good? Not mathematically clear yet (We
  will discuss later)

\item<7-> Notation: The community uses $\hat{\theta}$ to mean the
  estiamted value, i.e., \magenf{hat} for estimated value.   

  % \item<7-> Estimate as a number: $\hat{\theta} = g(x)$

% \item<8-> Estimator as a random variable number: $\hat{\Theta} = g(X)$

% \item Any theoretical support for optimality of MAP and LMS in some sense?

% - discuss later

\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Estimator as a Function}

\plitemsep 0.3in
\bci
\item Random observation: $X$ 

\item Observation instance: $x$

\item<1-> \redf{Estimate} as a mapping from $x$ to a number
$$\hth = g(x),\quad \hth_\MAP = g_\MAP(x),\quad \hth_\LMS = g_\LMS(x)$$

\item<2-> \redf{Estimator} as a mapping from $X$ to a random variable
$$\hTH = g(X), \quad \hTH_\MAP = g_\MAP(X), \quad \hTH_\LMS = g_\LMS(X)$$

% \item Any theoretical support for optimality of MAP and LMS in some sense?

% - discuss later

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{}

\vspace{2cm}
  {\Large From now on we focus on the MAP estimate, mainly based on the
  examples that we've discussed in the previous section.} 

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet}

\hfill \lecturemark{Slide 16 for more details}
  
\plitemsep 0.05in
\bci
\item<1-> Romeo and Juliet start dating, where 
Romeo is late by $X \sim \set{U}[0,\theta].$
\item<1-> Unknown: $\theta$ modeled by a rv $\Theta \sim \set{U}[0,1].$
\item<1-> Observation: Romeo was late by $x.$
\item<1-> \question Given the observation sample $x$, what is $\hat{\theta}_\text{MAP}$? 
\item<2-> \bluef{Intuition.} As $x$ grows, $\hat{\theta}_\text{MAP}$
  decreases or increases?  \onslide<3->{\magenf{Increases. Why?}}

% \item<6-> Prior \onslide<7->{and observation model (likelihood)}
%   \aleq{
%     \onslide<6->{\fth =
%       \begin{cases}
%         1, & 0 \le \theta \le 1 \cr
%         0, & \text{otherwise}
%       \end{cases},}
%     \qquad 
%     \onslide<7->{\fxcth =
%       \begin{cases}
%         \frac{1}{\theta}, & 0 \le x \le \theta\cr
%         0, & \text{otherwise}
%       \end{cases}}
%   }
  
\item<4-> Posterior:    $
    \displaystyle
     \fthcx = 
      \begin{cases}
        \frac{1}{\theta |\log x|},&  x \le \theta \le 1, \cr
        0, & \theta < x \text{ or } \theta >1
      \end{cases}
      $

    \item<5-> Given $x,$ $\fthcx$ is decreasing in $\theta$ over $[x,1].$
$\implies $ \orangef{$\hat{\theta}_{\text{MAP}} = x.$}

\eci

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example 1: Romeo and Juliet (2)}

% \plitemsep 0.07in
% \bci

% \item<1-> MAP estimate

%   \bci
% \item<2-> Given $x,$ $\fthcx$ is decreasing in $\theta$ over $[x,1].$
% $\implies $ \orangef{$\hat{\theta}_{\text{MAP}} = x.$}
%   \eci
  
% \item<1-> Conditional expectation estimate
%   \bci
% \item<3->[]
% \vspace{-0.5cm}
%   \aleq{
% \orangef{\hat{\theta}_{\text{LMS}}} = \expect{\theta | X=x} = \int_{x}^1 \theta \frac{1}{\theta |\log x|} d\theta  = \orangef{(1-x)/|\log x|}
% }
%   \eci


% \onslide<4->{\mypic{0.3}{L6_romeo_juliet.png}}
% \eci


% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example 1: Romeo and Juliet (3)}

% {\large $\circ$ What happens if we have more observation samples?}
  
  

% \plitemsep 0.1in
% \bci
% \item Romeo was late \redf{$n$ times} by $\magenf{\vec{X}=(X_1,X_2, \ldots, X_n)}, \ X_i \sim \set{U}[0,\theta].$
% \item $X_1,\ldots, X_n$ are conditionally independent, given $\Theta = \theta.$
% \item Unknown: $\theta$ modeled by a rv $\Theta \sim \set{U}[0,1].$
% \item Observation: Romeo was late \redf{$n$ times} by $\magenf{\vec{x} = (x_1, x_2, \ldots, x_n)}$
% \item \question Given the observation sample $\vec{x}$, what is $\hat{\theta}_\text{MAP}$? 
% \item \bluef{Intuition.} We will do more accurate inference due to
%   more observation samples. 
% \item Refer to the answers in Example 8.2 at pp. 414.  

%   \eci

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Spam Filtering}

\hfill \lecturemark{Slide 18 for more details}
  \plitemsep 0.05in
\bci
\item<1-> E-mail: \redf{spam} (1) or \bluef{legitimate} (2), $\Theta  \in \{1,2\},$ with prior $p_{\Theta}(1)$ and  $p_{\Theta}(2).$

\item<1-> $\{w_1, w_2, \ldots, w_n \}$: a collection of words which suggest ``spam".

\item<1-> For each $i,$ a Bernoulli $X_i=1$ if $w_i$ appears and 0 otherwise.

\item<1-> Assumption: Conditioned on $\Theta,$ $X_i$ are independent.

\item<1-> Posterior PMF
 \aleq{
 \bprob{\Theta = m | (x_1,\ldots, x_n)} = \frac{p_{\Theta}(m) \prod_{i=1}^n p_{X_i|\Theta}(x_i|m)  }{
 \sum_{j=1,2} p_{\Theta}(j) \prod_{i=1}^n p_{X_i|\Theta}(x_i|j) 
 }, \quad m=1,2
}
\item<2-> MAP rule for this hypothesis testing problem. Decided that the message is \redf{spam} if
\aleq{
p_{\Theta}(1) \prod_{i=1}^n p_{X_i|\Theta}(x_i|1) > p_{\Theta}(2) \prod_{i=1}^n p_{X_i|\Theta}(x_i|2)
}
\eci

\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Biased Coin with Beta Prior}

\hfill \lecturemark{Slide 21 for more details}

  \plitemsep 0.05in
\bci

\item<1-> Biased coin with probability of head $\theta$
\item<1-> Unknown $\theta$: modeled by $\Theta$ with some prior $\fth$
\item<1-> Observation $X$: number of heads out of $n$ tosses
\mycolorbox{
\bci
\item If $\Theta \sim \text{Beta}(\alpha,\beta),$ then $\Theta|\{X=k\} \sim \text{Beta}(\redf{k}+\alpha,\redf{n-k}+\beta)$
\item $\fthck \propto \theta^{\alpha+k-1}(1-\theta)^{\beta + n-k -1}$
  \eci
}
\item<2-> MAP estimate: Taking the logarithm,
\aleq{
\onslide<3->{\orangef{\hth_\MAP} &= \arg \max_{\theta} \Bl [(\alpha+k-1) \log \theta + (\beta + n-k+1) \log(1-\theta) \Bl ]
&=\orangef{\frac{\alpha + k -1}{\alpha + \beta -2 + n}}}
}

\item<4-> When $\alpha=\beta =1$ (i.e., $\set{U}[0,1]$ prior), \orangef{$\hth_\MAP = \frac{k}{n}$}

\eci

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Parameter Inference with Normal Prior}

\hfill \lecturemark{Slide 27 for more details}

\plitemsep 0.05in
\bci

\item<1-> The posterior pdf $\displaystyle \fthcx = a \cdot \exp \left\{ -
    \frac{(\theta - m)^2}{2 v}  \right\},$ where
  $$
  m = \frac
  {\sum_{i=0}^n \frac{x_i}{\sigma_i^2}}
  {\sum_{i=0}^n \frac{1}{\sigma_i^2}}, \qquad v =
  \frac{1}{\sum_{i=0}^n \frac{1}{\sigma_i^2}}
  $$


\item<2-> The pdf is normal, so it is maximized when $\theta =
  \text{mean}.$

\item<3-> Thus, \orangef{$\hat{\theta}_\map = m$}.

  \eci

\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example 3: Biased Coin with Beta Prior (3)}

% \mycolorbox{
% \bci
% \item If $\Theta \sim \text{Beta}(\alpha,\beta),$ then $\Theta|\{X=k\} \sim \text{Beta}(\redf{k}+\alpha,\redf{n-k}+\beta)$
% \item In other words, Beta prior $\imp$ Beta posterior (why useful?)
% \item $\fthck \propto \theta^{\alpha+k-1}(1-\theta)^{\beta + n-k -1}$
% \eci
% }

% \plitemsep 0.07in
% \bci

% \item<2-> MAP estimate: Taking the logarithm,
% \aleq{
% \onslide<3->{\hth_\MAP &= \arg \max_{\theta} \Bl [(\alpha+k-1) \log \theta + (\beta + n-k+1) \log(1-\theta) \Bl ]
% &=\frac{\alpha + k -1}{\alpha + \beta -2 + n}}
% }

% \item<4-> When $\alpha=\beta =1$ (i.e., $\set{U}[0,1]$ prior), \orangef{$\hth_\MAP = \frac{k}{n}$}


% % \item MAP estimate
% % \aleq{
% % \hth_\MAP = \arg \max_{\theta} (k \log \theta + (n-k) \log (1-\theta))
% % = \frac{k}{n}
% % }

% % \item Conditional expectation estimate = $\frac{k+1}{n+2}$ (homework problem)

% \eci

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Why MAP Is Good? (1)}

\plitemsep 0.1in
\bci
\item<1-> MAP estimate is intuitive, but we need more mathematical
  evidence for its performance guarantee. We would trust its quality
  if it is \redf{optimal in some sense}. 

  \mytwocols{0.65}
  {
\onslide<2->{
    \mypic{0.7}{L6_map_ex.png}
}
    \bci
\item<3-> MAP: $\hat{\theta}_\map = 2$
    \eci
  }
  {
    \small
    \plitemsep 0.1in
    \bci
  \item<4-> Given $X=x,$ $\theta$ that minimizes the probability of
    incorrect decision?
    $$
    \onslide<5->{\orangef{\hth_\map}} = \onslide<4->{\bluef{\arg \min_{\hat{\theta} = 1, 2, 3} \cprob{\hth \neq \Theta| X=x}}}
    $$

    \vspace{-0.6cm}
  \item<6-> Average probability of incorrect decision
    \aleq{
      \onslide<7->{\cprob{\hat{\Theta} \neq \Theta} &= \sum_{x}\cprob{\hat{\Theta} \neq
        \Theta| X=x} \px} \cr
      &\onslide<8->{= \sum_{x}\cprob{\hat{\theta} \neq
        \Theta| X=x} \px}\cr
      &\onslide<9->{ \redf{\ge} \sum_{x}\cprob{\redf{\hth_\map} \neq
        \Theta| X=x} \px}
    }
    
    \eci
  }

  \eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Why MAP Is Good? (2)}

\mycolorbox{
\plitemsep 0.05in
\bci
\item<1-> \redf{Claim 1.} \bluef{For a given $x,$} the MAP rule minimizes the probability of an incorrect decision.

\item<2-> \redf{Claim 2.} The MAP rule minimizes the overall probability of an incorrect decision, \bluef{averaged over $x.$ }
\eci
}


\plitemsep 0.1in
\bci
% \item<1-> MAP estimate is intuitive, but we need more mathematical
%   evidence for its good inference quality. 


\item<3-> \redf{Proof.}
\onslide<4->{Let $I$ and $I_\map$ be the indicator rv, representing
  the correct decision by any general estimator and the MAP estimator, respectively.}
\aleq{
\onslide<5->{\expect{I | X=x}=}
\onslide<6->{\bprob{g(X) = \Theta | X=x} \le }
\onslide<7->{\bprob{g_\map (X) = \Theta | X=x} =}
\onslide<8->{\expect{I_\map|X=x}}
}
\onslide<9->{Thus, \redf{Claim 1} holds. We now take the expectation of the above equations, the law of iterated expectations leads to \redf{Claim 2.}}
\eci

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(5)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Overview on Statistical Inference}

\item \grayf{Bayesian Inference: Framework}

\item \grayf{Examples}

\item \grayf{MAP (Maximum A Posteriori) Estimator}

\item \redf{LMS (Least Mean Squares) Estimator}

\item \grayf{LLMS (Linear LMS) Estimator

\item Classical Inference: ML Estimator}
  
%\ece
  % \medskip

% \item Classical Inference

%   \bce
% \item ML (Maximum Likelihood) Estimator
%   \ece
  
  \ece

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}{Other Estimate than MAP?}
  
  \plitemsep 0.2in
  \bci
  
\item<2-> MAP: the estimate which maximizes the posterior pdf,
  which solves the following optimization problem (minimizing the prob. of incorrect decision):
  \mycolorbox{
    \centering   
    $\displaystyle \min_{\hat{\theta}} \bprob{\Theta \neq \hth | X = x}$
  }
  
\item<3->  What about applying other objective function? Like the
  following one (mean squared error)?

  \mycolorbox{
    \centering   
\onslide<4->{$\displaystyle \min_{\hat{\theta}} \bexpect{(\Theta - \hat{\theta})^2 | X=x}$}
  }

\bci
\item<5-> Least Mean Square (LMS) Estimate 
\eci
  \eci

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{What's the Form?: LMS Estimator (1)}

\plitemsep 0.05in
\bci

\item<1-> Unknown: $\theta$ modeled by $\Theta$ with prior
  $f_{\Theta}(\cdot).$ Assume $\Theta \sim  \set{U}[4,10].$

\item<2-> Assume that \redf{no observations} available

\item<3-> MAP estimate

  - Any value $\hat{\theta}_{\map} \in [4,10]$ (why? posterior = prior), not very useful

\item<4-> What is the other choice?

\onslide<5->{- Expectation: \redf{$\hat{\theta} = \expect{\Theta} = 7$}

- looks reasonable, but why?}

\item<6-> First, it makes sense, but, second, it also minimizes the mean squared error (MSE)
\small
\aleq{
\min_{\hat{\theta}} \bexpect{(\Theta - \hat{\theta})^2} = \min_{\hat{\theta}}
\lf ( \cvar{\Theta - \hat{\theta}} + \Bl(\expect{\Theta - \hat{\theta}}\Bl)^2 \ri )=  \min_{\hat{\theta}} \lf ( \cvar{\Theta} + \Bl(\expect{\Theta - \hat{\theta}}\Bl)^2 \ri )
}

\onslide<7->{- minimized when \redf{$\hat{\theta} = \expect{\Theta}.$}}

%- Why this conditional expectation estimate is also called LMS (Least Mean Squares) estimator
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What's the Form?: LMS Estimator (2) }

\plitemsep 0.1in
\bci

\item<1-> Unknown: $\theta$ modeled by $\Theta$ with prior $f_{\Theta}(\cdot).$

\item<1-> \redf{Observation $X=x$} with model $f_{X|\Theta}(x|\theta)$

\item<2-> Minimizing conditional mean squared error

$$
\min_{\hat{\theta}} \bexpect{(\Theta - \hat{\theta})^2 | X=x}
$$

\bci
\item<3-> minimized when \redf{$\hat{\theta} = \expect{\Theta | X=x}.$}
\item<4->  LMS estimator \redf{$\hat{\Theta} = \expect{\Theta | X}$}
\eci

\item<5-> What is the mean squared error of the LMS estimate?
\bci
\item<6-> When $X=x$, $\bexpect{\big(\Theta - \bluef{\expect{\Theta | X=x}} \big)^2 | X=x} = \orangef{\cbvar{\Theta | X=x}}$
\item<7-> Averaged over $X$: $\bexpect{(\Theta - \expect{\Theta | X}
    )^2} =  \orangef{\bexpect{\cvar{\Theta | X}}}$
\eci



\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet}

\hfill \lecturemark{Slides 17 and 35 for more details}
  
\plitemsep 0.02in
\bci
\item<1-> Romeo and Juliet start dating, where 
Romeo is late by $X \sim \set{U}[0,\theta].$
\item<1-> Unknown: $\theta$ modeled by a rv $\Theta \sim \set{U}[0,1].$
\item<1-> Observation: Romeo was late by $x.$
  
\item<2-> Posterior:    $
    \displaystyle
     \fthcx = 
      \begin{cases}
        \frac{1}{\theta |\log x|},&  x \le \theta \le 1, \cr
        0, & \theta < x \text{ or } \theta >1
      \end{cases}
      $

      \mytwocols{0.4}
      {
        \small
        \plitemsep 0.05in
        \bci
        
      \item<2-> $\hat{\theta}_{\text{MAP}} = x.$
        
      \item<3-> \bluef{LMS estimator:}


        $\displaystyle \hat{\theta}_{\text{LMS}} = \expect{\theta | X=x} = \int_{x}^1 \theta \frac{1}{\theta |\log x|} d\theta        = \redf{(1-x)/|\log x|}$
        \eci
      }
      {
        \onslide<4->{\mypic{0.55}{L6_romeo_juliet.png}}
      }
      
      
      
 % \vspace{-0.6cm}
% \centering



\eci

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Example: Biased Coin with Beta Prior}

\hfill \lecturemark{Slides 21 and 37 for more details}

  \plitemsep 0.05in
\bci

\item<1-> Biased coin with prob. of head $\theta.$ Unknown $\theta$ modeled by $\Theta$ with prior $\fth.$
\item<1-> Observation $X$: number of heads out of $n$ tosses
\item<1-> If $\Theta \sim \text{Beta}(\alpha,\beta),$ then $\Theta|\{X=k\} \sim \text{Beta}(\redf{k}+\alpha,\redf{n-k}+\beta)$

\medskip
  \myvartwocols{0.5}{0.3}{0.6}
  {
    \small
    \plitemsep 0.02in
    \bci
  \item<2-> MAP estimate

    \medskip
    $\displaystyle \onslide<2->{\orangef{\hth_\MAP} = \orangef{\frac{\alpha + k -1}{\alpha + \beta -2 + n}}}$
    
  \item<2-> For $\alpha=\beta =1$

    ($\set{U}[0,1]$ prior),

    \medskip
    \orangef{$\displaystyle \hth_\MAP = \frac{k}{n}$}
    \eci
  }
  {
    \small
    \plitemsep 0.02in
    \bci
  \item<3-> \bluef{Fact.} If $\Theta \sim \text{Beta}(\alpha,\beta),$
    \aleq{
      \expect{\Theta} &= \frac{1}{B(\alpha,\beta)} \int_0^1 \theta \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta = \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} = \frac{\alpha}{\alpha+\beta}
    }
    
  \item<4-> LMS estimate:     $\displaystyle \orangef{\expect{\Theta|
        X=k}} = \frac{k+\alpha}{k+\alpha + n -k +\beta} =
    \orangef{\frac{k+\alpha}{\alpha + \beta + n}}$

  \item<5-> For $\alpha=\beta =1$ ($\set{U}[0,1]$ prior):   $\displaystyle \orangef{\expect{\Theta| X=k} = \frac{k+1}{n+2}}$
    
    \eci
  }

\eci

\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example: Biased Coin with Beta Prior (2)}

% \plitemsep 0.1in
% \bci

% %\item \bluef{Remind.} If $\Theta \sim Beta(\alpha,\beta),$ then $\Theta|\{X=k\} \sim Beta(k+\alpha,n-k+\beta)$

% \item \bluef{Fact.} If $\Theta \sim \text{Beta}(\alpha,\beta),$
% \aleq{
% \expect{\Theta} &= \frac{1}{B(\alpha,\beta)} \int_0^1 \theta \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta = \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} = \frac{\alpha}{\alpha+\beta}
% }
% \item Using the above fact,
% \aleq{
% \expect{\Theta| X=k} = \frac{k+\alpha}{k+\alpha + n -k +\beta} = \frac{k+\alpha}{\alpha + \beta + n}
% }
% \item For $\alpha=\beta=1$ ($\Theta = Uniform[0,1]$),
% \aleq{
% \expect{\Theta| X=k} = \frac{k+1}{n+2}
% }


% % \aleq{
% % \fthck&= d \cdot \theta^{\alpha+k-1}(1-\theta)^{\beta + n-k -1}
% % }

% % \item Using, $\int_0^1 \theta^{\alpha -1}(1-\theta)^{\beta -1} d\theta = \frac{(\alpha-1)! (\beta -1)!}{(\alpha + \beta-1)!}$
% % \aleq{
% % \expect{\Theta | X=k} &= \int_{0}^1 \theta \fthck d\theta = d\cdot \int_{0}^1
% % \theta^{\alpha+k}(1-\theta)^{\beta + n-k -1} d\theta \cr
% % &=
% % }

% % \item MAP estimate
% % \aleq{
% % \hth_\MAP = \arg \max_{\theta} (k \log \theta + (n-k) \log (1-\theta))
% % = \frac{k}{n}
% % }

% % \item Conditional expectation estimate = $\frac{k+1}{n+2}$ (homework problem)

% \eci

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Parameter Inference with Normal Prior}

\hfill \lecturemark{Slides 27 and 38 for more details}

\plitemsep 0.05in
\bci

\item<1-> The posterior pdf $\displaystyle \fthcx = a \cdot \exp \left\{ -
    \frac{(\theta - m)^2}{2 v}  \right\},$ where
  $$
  m = \frac
  {\sum_{i=0}^n \frac{x_i}{\sigma_i^2}}
  {\sum_{i=0}^n \frac{1}{\sigma_i^2}}, \qquad v =
  \frac{1}{\sum_{i=0}^n \frac{1}{\sigma_i^2}}
  $$


\item<1-> The pdf is normal, so it is maximized when $\theta =
  \text{mean}.$

\item<1-> Thus, \orangef{$\hat{\theta}_\map = m$}.

\item<2-> What is the LMS esitmate?  
  $$
  \orangef{\hat{\theta}_{\lms} = \expect{\Theta | X = x} = m}
  $$

  \eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Signal Recovery from Noisy Measurement (1)}


\small
\plitemsep 0.03in
\bci

\item<1-> Send signal $\theta$ with the uniform noise $W \sim
  \set{U}[-1,1].$ Observe $X$

\item $X = \Theta + W,$ where model $\theta$ with $\Theta \sim \set{U}[4,10]$

% $$
% X = \Theta + W
% $$
\item<2-> Given $\Theta = \theta,$ $X = \theta + W \sim \set{U}[\theta-1, \theta+1].$
\aleq{
\onslide<4->{f_{\Theta, X}(\theta,x) = \fth \fxcth =
\begin{cases}
\frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}, & \text{if } 4\le \theta \le 10, \ \theta -1 \le x \le \theta +1, \cr
0, & \text{otherwise}
\end{cases}}
}

\myvartwocols{0.35}{0.4}{0.57}
{
\bigskip
\onslide<5->{$\hth_\LMS = \expect{\Theta | X=x}$:  \redf{midpoint} of the corresponding vertical section}
}
{
\centering
\onslide<3->{\mypic{0.9}{L6_lms_ex1.png}}
}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Signal Recovery from Noisy Measurement (2)}


\plitemsep 0.07in
\bci

\item What is conditional MSE? $\bexpect{(\Theta - \expect{\Theta | X=x})^2 | X=x}$

\item Given $X=3,$ it's the variance of $\set{U}[4,4]$ = 0

\item Given $X=5,$ it's the variance of $\set{U}[4,6]$ = $(6-4)^2/12 = 1/3$

 \item The rising pattern between $X=3$ and $X=5$ is quadratic. This is
   because the expectation increases linearly, where the variance
   increases in a quadratic manner.  

\eci

\vspace{-0.5cm}
\mypic{0.35}{L6_lms_ex2.png}


\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example 2}

% \small
% \plitemsep 0.03in
% \bci

% \item Unknown: $\Theta \sim Uniform[4,10]$

% \item Observe $\Theta$ with random error $W$ as $X.$ $W \sim Uniform[-1,1]$
% $$
% X = \Theta + W
% $$
% \item Given $\Theta = \theta,$ $X = \theta + W \sim Uniform[\theta-1, \theta+1].$
% \aleq{
% f_{\Theta, X}(\theta,x) = \fth \fxcth =
% \begin{cases}
% \frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}, & \text{if } 4\le \theta \le 10, \ \theta -1 \le x \le \theta +1, \cr
% 0, & \text{otherwise}
% \end{cases}
% }

% \mypic{0.5}{L6_lms_ex1.png}
% \mypic{0.35}{L6_lms_ex2.png}

% $$
% \min_{\hat{\theta}} \bexpect{(\Theta - \hat{\theta})^2 | X=x}
% $$

% \bci
% \item minimized when $\hat{\theta} = \expect{\Theta | X=x}.$
% \item  LMS estimator $\hat{\Theta} = \expect{\Theta | X}$
% \eci

% \item Performance (MSE: Mean Squared Error)
% \bci
% \item When $X=x$, $\bexpect{(\Theta - \expect{\Theta | X=x} )^2 | X=x} = \cbvar{\Theta | X=x}$
% \item Averaged over $X$: $\bexpect{(\Theta - \expect{\Theta | X} )^2} = \bexpect{\cvar{\Theta | X=x}}$
% \eci

% \eci

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hardness of LMS Estimation}

\aleq
{
\fthcx &= \frac{\fth \fxcth}{ \fx}\cr
\fx &= \int f_{\Theta}(\theta') f_{X | \Theta}(x | \theta') d\theta'
}


\plitemsep 0.1in
\bci

\item<2-> Observation model $\fxcth$ may not be always available

\item<3-> Finding the posterior distribution is hard for multi-dimensional $\Theta$

\item<4-> $\Theta$ is very often high-dimensional, especially in the era of big data and deep learning

- {\small AlexNet in image recognition: \bluef{61M} parameters}

- {\small GPT-3 in natural language processing: \bluef{175B} parameters}


\item<5-> Any alternative to LMS estimator?

\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(6)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Overview on Statistical Inference}

\item \grayf{Bayesian Inference: Framework}

\item \grayf{Examples}

\item \grayf{MAP (Maximum A Posteriori) Estimator}

\item \grayf{LMS (Least Mean Squares) Estimator}

\item \redf{LLMS (Linear LMS) Estimator}

\item \grayf{Classical Inference: ML Estimator}
%\ece
  % \medskip

% \item Classical Inference

%   \bce
% \item ML (Maximum Likelihood) Estimator
%   \ece
  
  \ece

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linear LMS (LLMS) Estimator: Approach}

\plitemsep 0.1in
\bci

\item<2-> Give up optimality, but choose a simple, but good one.

\item<3-> General estimators $\hat{\Theta} = g(X)$, LMS estimator $\hat{\Theta}_{LMS} = \expect{\Theta | X}$

\item<4-> We consider a restricted class of $g(X)$

  \bci
\item Estimator: $\hat{\Theta} =$ \redblank{5}{$aX +b$}.
\item Estimate: Given $X = x,$ $\hat{\theta} =$ \redblank{5}{$ax +b$}.
  \eci


\item<6-> Our goal is to try our best within this restricted class:
  $$
\min_{a,b} \bexpect{(\Theta - aX -b)^2| X=x}, \qquad \min_{a,b} \bexpect{(\Theta - aX -b)^2}
$$

\item<7-> Linear models are always the first choice for a simple design in engineering.

\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{LLMS Estimator: Result and Interpretation}

\onslide<1->{
\myblock{LLMS}
{
\aleq
{
\hat{\Theta}_{L} =\cexpect{\Theta} + \frac{\cov{\Theta,X}}{\cvar{X}}\Bl(X - \cexpect{X}\Bl)
=\cexpect{\Theta} + \rho \frac{\sigma_{\Theta}}{\sigma_{X}}\Bl(X - \cexpect{X}\Bl),
}
where the correlation coefficient $\rho =
\frac{\cov{\Theta,X}}{\sigma_\Theta \sigma_X}.$
}
}

\vspace{-0.4cm}

\plitemsep 0.1in
\bci
\item<2-> \redf{No need of distributions} on $\Theta$ and $X$: only means, variances, and covariances


\medskip

\mytwocols{0.25}
{
\plitemsep 0.05in

  \bci
\item<3-> If $\rho >0:$


- Baseline ($\expect{\Theta}$) + correction term

- If $X > \expect{X}$ $\imp$ $\hat{\Theta}_L > \expect{\Theta}$

- If $X < \expect{X}$ $\imp$ $\hat{\Theta}_L < \expect{\Theta}$
\eci
}
{
\plitemsep 0.05in
\bci
\item<4-> If $\rho =0$ (uncorrelated):

- Just baseline ($\expect{\Theta}$)

- $\hat{\Theta}_L = \expect{\Theta}$

- No use of data $X$
\eci

}

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{LLMS Estimator: Mean Squared Error}

 \begin{textblock*}{\textwidth}(9cm,0.8\textheight)
    \mysizebox{0.44}{
       $\hat{\Theta}_{L} = \cexpect{\Theta} + \rho \frac{\sigma_{\Theta}}{\sigma_{X}}\Bl(X - \cexpect{X}\Bl)$
     }
  \end{textblock*}
  

  \bci
\item<1-> MSE $\expect{(\hat{\Theta}_L - \Theta)^2}$?
  \bci
  
  \item<2-> Assume   $\expect{\Theta} = \expect{X}=0$ (for
    simplicity). Then, MSE = $\bexpect{(\Theta - \rho \frac{\sigma_\Theta}{\sigma_X} X)^2}$
  \item<3-> Note that $\var{\Theta} = \sigma^2_\Theta =
    \cexpect{\Theta^2}$ and  $\var{X} = \sigma^2_X =
    \cexpect{X^2}$

    \aleq{
      \onslide<4->{\bexpect{(\Theta -
          \rho\frac{\sigma_\Theta}{\sigma_X} X)^2} &= \cvar{\Theta -
          \rho\frac{\sigma_\Theta}{\sigma_X} X}} \cr
      \onslide<5->{&=
      \cvar{\Theta} + \Big(\rho\frac{\sigma_\Theta}{\sigma_X} \Big)^2 \cvar{X}
      - 2\Big(\rho\frac{\sigma_\Theta}{\sigma_X}\Big) \cov{\Theta,X}}
      \onslide<6->{= (1-\rho^2) \var{\Theta}}
    }
    
  % \item<6-> $\cov{\Theta,X} = \cexpect{\Theta X}$ and $\cvar{\Theta} = \cexpect{\Theta^2}$


\eci


% \item $\bexpect{(\hat{\Theta}_{\text{L}} - \Theta)^2} = \bexpect{(\Theta - \rho \frac{\sigma_\Theta}{\sigma_X} X)^2} =
%   (1-\rho^2) \var{\Theta} $

\item<7-> Uncertainty about $\Theta$ after observation \bluef{decreases} by the factor of \bluef{$1-\rho^2$}


\item<8-> What happens if $|\rho|=1$ or $\rho=0$?



  \eci

%\medskip


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linear LMS (LLMS) Estimator: Proof}

  \small
  \begin{align}
    \hat{\Theta}_{L} &= \cexpect{\Theta} + \frac{\cov{\Theta,X}}{\cvar{X}}\Bl(X - \cexpect{X}\Bl)\label{eq:1} \\
&= \cexpect{\Theta} + \rho \frac{\sigma_{\Theta}}{\sigma_{X}}\Bl(X - \cexpect{X}\Bl) \label{eq:2}
\end{align}

\vspace{-0.2cm}
\mytwocols{0.6}
{
\small

\aleq{
\onslide<2->{\min_{a,b} \text{ERR}(a,b) = \min_{a,b} \bexpect{(\Theta - aX -b)^2}}
}

\onslide<3->{- Assume $a$ was found.
\aleq
{
\bexpect{(Y -b)^2}, \quad Y= \Theta - aX
}}

\vspace{-0.3cm}
\onslide<4->{- Minimized when $b = \cexpect{Y} = \cexpect{\Theta} - a
  \cexpect{X}.$ \hfill \lecturemark{\scriptsize Slide pp. 43}}
\begin{align}
\onslide<5->{&\text{ERR}(a,b) = \expect{(Y - \expect{Y})^2} = \cvar{Y} \nonumber \\
&=\var{\Theta} + a^2 \var{X} -2a \cov{\Theta, X} \label{eq:3}}
\end{align}
}
{
\small

\onslide<6->{- \eqref{eq:3} is minimized when $a = \frac{\cov{\Theta,X}}{\var{X}}.$}
\onslide<7->{Then,
\aleq{
\hat{\Theta}_{L} &= aX + b = aX + \cexpect{\Theta} - a \cexpect{X}\cr
& = \cexpect{\Theta} +a(X  -  \cexpect{X}) = \text{\eqref{eq:1}}
}}

\onslide<8->{- Using $\rho = \frac{\cov{\Theta,X}}{\sigma_\Theta \sigma_X},$ we get:
\aleq{
a = \frac{\rho \sigma_\Theta \sigma_X}{\sigma_X^2} = \frac{\rho \sigma_\Theta}{\sigma_X}
}
- Then, we have \eqref{eq:2}.}


}




\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet (1)}

\hfill \lecturemark{Slides 17, 35, and 45 for more details}
  
\plitemsep 0.1in
\bci
\item<1-> Romeo and Juliet start dating, where 
Romeo is late by $X \sim \set{U}[0,\theta].$
\item<1-> Unknown: $\theta$ modeled by a rv $\Theta \sim \set{U}[0,1].$
\item<1-> Random observation: $X$
  
\item<1-> $\hat{\Theta}_{\map} = X,$ and $\hat{\Theta}_{\lms} = (1-X)/|\log X.$

\item<2-> \question What is the LLMS estimator $\hat{\Theta}_{\text{L}}$?  
  
  \eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet (2)}

\mytwocols{0.7}
{
\small
\plitemsep 0.07in
\bci

\item[] $\hat{\Theta}_{\text{L}} = \cexpect{\Theta} + \frac{\cov{\Theta,X}}{\cvar{X}}\Bl(X - \cexpect{X}\Bl)$

\medskip

\item<2-> $\expect{X} = \expect{\expect{X|\Theta}}= \expect{\Theta/2} = 1/4$

\item<3->  Using $\expect{\Theta} = 1/2$ and $\expect{\Theta^2} = 1/3,$
\aleq{
\var{X}& = \expect{\var{X|\Theta}} + \var{\expect{X|\Theta}}\cr
&= \frac{1}{12}\expect{\Theta^2} + \frac{1}{4}\var{\Theta}= \frac{7}{144}
}

\item<4-> $\cov{\Theta, X} = \expect{\Theta X} - \expect{\Theta} \expect{X}$
\aleq{
\expect{\Theta X} &= \expect{\expect{\Theta X | \Theta}} = \expect{\Theta \expect{X | \Theta}}\cr
&= \expect{\Theta^2/2} = 1/6
}


\eci
}
{
\small
\plitemsep 0.1in
\bci

\item<4->[] $\cov{\Theta, X} = 1/6 - 1/2\cdot 1/4 = 1/24$

\item<5-> $\hat{\Theta}_{\text{L}} = \frac{1}{2} + \frac{1/24}{7/144}(X - \frac{1}{4}) = \frac{6}{7}X + \frac{2}{7}$

%\vspace{1cm}  
\eci

\onslide<6->{\mypic{0.9}{L6_romeo_juliet_llms.png}}

}

\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{Example: Biased Coin with Beta Prior}

% \hfill \lecturemark{Slides 21 and 37 for more details}

%   \plitemsep 0.05in
% \bci

% \item<1-> Biased coin with prob. of head $\theta.$ Unknown $\theta$ modeled by $\Theta$ with prior $\fth.$
% \item<1-> Observation $X$: number of heads out of $n$ tosses
% \item<1-> If $\Theta \sim \text{Beta}(\alpha,\beta),$ then $\Theta|\{X=k\} \sim \text{Beta}(\redf{k}+\alpha,\redf{n-k}+\beta)$


%   \medskip
%   \myvartwocols{0.5}{0.3}{0.6}
%   {
%     \small
%     \plitemsep 0.02in
%     \bci
%   \item<2-> MAP estimate

%     \medskip
%     $\displaystyle \onslide<2->{\orangef{\hth_\MAP} = \orangef{\frac{\alpha + k -1}{\alpha + \beta -2 + n}}}$
    
%   \item<2-> For $\alpha=\beta =1$

%     ($\set{U}[0,1]$ prior),

%     \medskip
%     \orangef{$\displaystyle \hth_\MAP = \frac{k}{n}$}
%     \eci
%   }
%   {
%     \small
%     \plitemsep 0.02in
%     \bci
%   \item<3-> \bluef{Fact.} If $\Theta \sim \text{Beta}(\alpha,\beta),$
%     \aleq{
%       \expect{\Theta} &= \frac{1}{B(\alpha,\beta)} \int_0^1 \theta \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta = \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} = \frac{\alpha}{\alpha+\beta}
%     }
    
%   \item<4-> LMS estimate:     $\displaystyle \orangef{\expect{\Theta|
%         X=k}} = \frac{k+\alpha}{k+\alpha + n -k +\beta} =
%     \orangef{\frac{k+\alpha}{\alpha + \beta + n}}$

%   \item<5-> For $\alpha=\beta =1$ ($\set{U}[0,1]$ prior):   $\displaystyle \orangef{\expect{\Theta| X=k} = \frac{k+1}{n+2}}$
    
%     \eci
%   }


% \eci

% \end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Example: Biased Coin with Uniform Prior}

\mytwocols{0.7}
{
\small
\plitemsep 0.05in
\bci
\item Biased coin with probability of head $\theta$
\item Unknown $\Theta \sim \set{U}[0,1],$

- $\expect{\Theta} = 1/2$, $\var{\Theta} = 1/12$
\item $n$ tosses, $X$: number of heads.
\item $p_{X|\Theta}(k|\theta) \sim \text{Binomial}(n,\theta)$
\item<2-> $\expect{X} = \expect{\expect{X|\Theta}} = \expect{n\Theta} = n/2$
\aleq{
\onslide<3->{\cvar{X} &= \expect{\cvar{X | \Theta}} + \cvar{\expect{X | \Theta}}\cr
&= \expect{n\Theta(1-\Theta)} + \var{n\Theta} \cr
&= \frac{n}{2} - \frac{n}{3} + \frac{n^2}{12} = \frac{n(n+2)}{12}}
}
\eci
}
{

\small
\onslide<4->{$\cov{\Theta, X} = \expect{\Theta X} - \expect{\Theta} \expect{X} = \expect{\Theta X} - n/4$}
\aleq{
\onslide<5->{\expect{\Theta X} &= \expect{\expect{\Theta X | \Theta}} = \expect{\Theta \expect{X | \Theta}}\cr
&= \expect{n\Theta^2} = n/3}
}
\onslide<6->{$\cov{\Theta, X} = \frac{n}{3} - \frac{n}{4} = \frac{12}{n}$}

\aleq{
\onslide<7->{\hat{\Theta}_L = \frac{1}{2} + \frac{n/12}{n(n+2)/12}(X-\frac{n}{2}) = \frac{X+1}{n+2}}
}

\vspace{-0.3cm}
\plitemsep 0.03in
\bci
\item<8-> $\hat{\Theta}_{\map} = \frac{X}{n}$
\item<8-> $\hat{\Theta}_{\lms}= \frac{X+1}{n+2}$
\item<9-> $\hat{\Theta}_{\text{L}} = \hat{\Theta}_{\lms}$! Intuitive?
\item<10-> Yes, because the LMS esitmator was linear.
  \eci

  }
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example 2: Common Mean of Normal rvs}

% 나중에 normal의 linear model을 할때 다시 정리하자.


% \mytwocols{0.8}
% {
% \small
% \plitemsep 0.05in
% \bci
% \item Observe a collection $X=(X_1, X_2, \ldots, X_n)$
% \item Unknown: common mean $\theta$, modeled by $\Theta \sim N(x_0, \sigma_0^2)$
% \item $X_i$ are normal, independent with known variance $\sigma_1^2, \ldots, \sigma_n^2.$
% \aleq{
% X_i = \Theta + W_i, i=1, \ldots, n,
% }
% where $W_i$ are independent and normal.



% \item Then,
% \eci
% }
% {
% \small
% \plitemsep 0.07in
% \bci

% \item MAP rule

% - Given $x,$ $\fthcx$ is decreasing in $\theta$ over $[x,1].$

% - $\hat{\theta}_{\text{MAP}} = x.$

% \item Conditional expectation estimator
% \vspace{-0.3cm}
% \aleq{
% \hat{\theta}_{\text{LMS}} &= \expect{\theta | X=x} = \int_{x}^1 \theta \frac{1}{\theta |\log x|} d\theta \cr
% & = (1-x)/|\log x|
% }
% \vspace{-0.6cm}
% \centering
% \mypic{0.6}{L6_romeo_juliet.png}
% \eci
% }

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(7)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Overview on Statistical Inference}

\item \grayf{Bayesian Inference: Framework}

\item \grayf{Examples}

\item \grayf{MAP (Maximum A Posteriori) Estimator}

\item \grayf{LMS (Least Mean Squares) Estimator}

\item \grayf{LLMS (Linear LMS) Estimator}

\item \redf{Classical Inference: ML Estimator}
%\ece
  % \medskip

% \item Classical Inference

%   \bce
% \item ML (Maximum Likelihood) Estimator
%   \ece
  
  \ece

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Framework of Classical Inference (1)}

\plitemsep 0.08in

\begin{center}
\mypic{0.5}{L6_classical_framework.png}
\end{center}
\vspace{-0.5cm}
\bci
\item Unknown $\theta$

\bci
\item<2-> \redf{deterministic (not random)} quantity (thus, no prior distribution)
\item<2-> No prior, No posterior probabilities
\eci



\item Observations or measurements $X$

\bci
\item<3-> Random observation $X$'s distribution just depends on $\theta$
\item<4-> Notation: \redf{$p_X(x;\theta)$} and \redf{$f_X(x;\theta)$}, $\theta$-parameterized distribution of observations
\eci

\item<5-> Choosing one among multiple probabilistic models

\bci
\item Each $\theta$ corresponds to a probabilistic model
\eci

\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Framework of Classical Inference (2)}

\plitemsep 0.1in

\bci
\item<2-> Problem types

\bci
\item \redf{Estimation}
\item Hypothesis testing
\item Significance testing
\eci



\item<3-> Key inference methods

\bci
\item \redf{ML (Maximum Likelihood) estimation}
\item Linear regression
\item Likelihood ratio test
\item Significant testing
\eci

\item<4-> Just a taste in this course.
\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Maximum Likelihood Estimation}

\plitemsep 0.1in

\bci
\item<2-> Random observation $x=(x_1, x_2, \ldots, x_n)$ of $X=(X_1, X_2, \ldots, X_n)$

- Assume a scalar $\theta$ and a vector of observations in this lecture.

\item<3-> \redblank{4}{Likelihood $p_X(x_1, x_2, \ldots, x_n;\theta)$}

\bci
\item $p_X(x_1, x_2, \ldots, x_n;\theta)$

- The probability that \bluef{ the observed value $x$ arises when the parameter is $\theta.$}

- NOT the probability that the unknown parameter is equal to $\theta.$




\item<5-> \magenf{ML (Maximum Likelihood) estimation}

\mycolorbox{
\centering
  $\hat{\theta}_{\ml} = \arg \max_{\theta} p_X(x_1, x_2, \ldots, x_n;\theta)$
}


\eci

\item<6-> Very often, $X_i$s are independent. Then, ML equals to maximizing the log-likelihood:
\aleq{
\log p_X(x_1, x_2, \ldots, x_n;\theta) = \log \prod_{i=1}^n p_{X_i}(x_i;\theta) = \sum_{i=1}^n \log p_{X_i}(x_i;\theta)
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{ML vs. MAP}

\plitemsep 0.1in

\bci
\item<1-> ML and MAP: How are they related?

\item<2-> MAP in the Bayesian inference
\aleq{
\hat{\theta}_{\map} &= \arg \max_{\theta} p_{\Theta | X} (\theta | x)= \arg \max_{\theta} \frac{p_{X| \Theta}(x|\theta) p_\Theta(\theta)}{\px} = \frac{1}{\px}\arg \max_{\theta} p_{X| \Theta}(x|\theta) p_\Theta(\theta)
}

\item<3-> ML in the classical inference
\aleq{
\hat{\theta}_{\ml} = \arg \max_{\theta} p_X(x;\theta)
}

\item<4-> $p_{X| \Theta}(x|\theta)$ in the Bayesian setting corresponds to $p_X(x;\theta)$ in the classical setting.

\item<5-> Thus, when $\Theta$ is \redf{uniform} (complete ignorance of
  $\Theta$) in MAP, \bluef{MAP == ML}

\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet}

\plitemsep 0.1in
\bci
\item Romeo and Juliet start dating. Romeo: late by $X \sim U[0,\theta].$
\item Unknown: $\theta$ modeled by a rv $\Theta \sim U[0,1].$

\medskip
\item MAP:  $\hat{\theta}_{\text{MAP}} = x$

\item LMS: $\hat{\theta}_{\text{LMS}} = (1-x)/|\log x|$


\item LLMS: $\hat{\theta}_{\text{L}} = \frac{6}{7}x + \frac{2}{7}$

\item ML: \onslide<2->{\redf{$\hat{\theta}_{\text{ML}} = \hat{\theta}_{\text{MAP}} = x$}}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Estimation of Parameter of Exponential rv}

\plitemsep 0.1in
\bci
\item $n$ identical, independent exponential rvs, $X_1, X_2,\ldots, X_n$ with parameter $\theta.$

\item<2-> Observation $x_1, x_2, \ldots, x_n$

\item<3-> What is the ML estimate of $\theta$?

\item<4-> \redf{Reminder.} $X \sim \exp(\lambda)$
\aleq{
\fx =
 \begin{cases}
 \lambda \elambdax, & x \ge 0 \cr
 0, & x <0
 \end{cases}
\quad \expect{X} = 1/\lambda
}

\item<5-> Any guess? \onslide<6->{$\hat{\theta}_{\text{ML}} = \frac{n}{x_1 + x_2 \ldots x_n}$}
\aleq{
\onslide<7->{\arg \max_{\theta} f_X(x;\theta) = \arg \max_{\theta} \prod_{i=1}^n \theta e^{-\theta x_i} = \arg \max_{\theta} \Bl(n \log \theta - \theta \sum_{i=1}^n x_i \Bl)}
}

\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\vspace{2cm}
\LARGE Questions?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Review Questions}

\bce[1)]
\item What is statistical inference?

\item Draw the building blocks of Bayesian inference and explain how it works.

\item What are MAP and LMS estimators and their underlying philosophies?

\item What is LLMS estimator and why is it useful?

\item Compare the classical and Bayesian inference.

\item What is the ML estimator and how is it related to the MAP estimator?
\ece

\end{frame}

\end{document}
