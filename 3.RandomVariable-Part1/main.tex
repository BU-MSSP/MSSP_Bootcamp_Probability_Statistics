
\input{../mode.tex}
\csname\pdfmode\endcsname

\mode<presentation>
{
  \usetheme{default}
  \usecolortheme{default}
  \usefonttheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]  % or "page number"
  \setbeamercolor{frametitle}{fg=white}
  \setbeamercolor{footline}{fg=black}
} 


\input{../myhead.tex}

\title[]{Lecture 3: Random Variable, Part I}
\author{Yi, Yung (이융)}
\institute{EE210: Probability and Introductory Random Processes\\ KAIST EE}
\date{\today}

\input{../mymath}


\begin{document}

\input{../mydefault.tex}


\begin{frame}
  \titlepage
\end{frame}

% START START START START START START START START START START START START START


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)]

\item Random variable: Idea and formal definition

\item Popular discrete random variables

\item Summarizing random variables: Expectation and Variance

\item (Functions of) multiple random variables 

\item Conditioning for random variables

\item Independence for random variables 

\ece 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L3(1)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)]

\item \redf{Random variable: Idea and formal definition}

\item \grayf{Popular discrete random variables

\item Summarizing random variables: Expectation and Variance

\item (Functions of) multiple random variables 

\item Conditioning for random variables

\item Independence for random variables}

\ece 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Random Variable: Idea}


\myvartwocols{0.7}{0.4}{0.57}
{
\plitemsep 0.1in

\bci 
\item In reality, many outcomes are \redblank{2}{numerical}, e.g., stock price.

\item Even if not, very convenient if we map numerical values to random outcomes, e.g., `0' for male and `1' for female.

\eci 
}
{
\centering
\includegraphics[width=0.9\textwidth]{L3_RV_ex.png}

{\scriptsize (b) Two rolls of tetrahedral dice}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Random Variable: More Formally}

\plitemsep 0.1in

\bci 
\item<2-> Mathematically, a random variable $X$ is a \redblank{3}{function} which maps from $\Omega$ to $\real.$

\item<4-> \bluef{Notation.} Random variable $X$, numerical value $x.$

\item<5-> Different random variables can be defined on the same sample space. 

\item<6-> For a fixed value $x,$ we can associate an \redblank{7}{event} that a random variable $X$ has the value $x,$ i.e., 
\redblank{8}{$\{ \omega \in \Omega \mid X(\omega) = x \}$}

\item<8-> Assume that values $x$ are discrete\footnote{Finite or countably infinite.} such as $1, 2, 3, \ldots.$

For notational convenience,  
$$
p_X(x) \ \eqdef \ \cprob{X = x} \ \eqdef \ \cbprob{\{ \omega \in \Omega \mid X(\omega) = x \}} 
$$

\item<9-> For a discrete random variable $X$, we call $p_X(x)$ \redblank{10}{probability mass function} (PMF).

\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example}

\plitemsep 0.1in

\bci 
\item Rolls a dice, $\Omega = \{1, 2,3, 4, 5, 6 \}$
\item Define a random variable $X = 1$ for even numbers and $X=0$ for odd numbers
\item Event $A_1 = \{\omega \in \Omega \mid  X(\omega) = 1 \} = \{2,4,6 \} \subset \Omega,$ but simply $A_1 = \{X =1\}$

\item Event $A_0 = \{\omega \in \Omega \mid  X(\omega) = 0 \} = \{1,3,5 \} \subset \Omega,$ but simply $A_0 = \{X =0\}$

\bigskip
\item Remember that the random variable $X$ is a \redf{\bf function} from $\Omega$ to $\real$
\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L3(2)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)]

\item \grayf{Random variable: Idea and formal definition}

\item \redf{Popular discrete random variables}

\item \grayf{Summarizing random variables: Expectation and Variance

\item (Functions of) multiple random variables 

\item Conditioning for random variables

\item Independence for random variables} 

\ece 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Bernoulli $X$ with parameter $p \in [0,1]$}

\plitemsep 0.1in

\bci
\item<1-> Only \redf{binary} values
\onslide<2->{
$$
X = \begin{cases}
0, & \text{w.p.} \quad 1-p, \cr
1, & \text{w.p.} \quad p
\end{cases}
$$
In other words, $p_X(0)=1-p$ and $p_X(1)=p$ from our PMF notation. }
\footnotetext{w.p.: with probability}

\item<3-> Models a trial that results in binary results, e.g., success/failure, head/tail

\item<4-> Very useful for an \redblank{5}{indicator rv} of an event $A.$ 
\onslide<5->{
Define a rv $\indi{A}$ as:
$$
\indi{A} = 
\begin{cases}
1, & \text{if $A$ occurs}, \cr
0, & \text{otherwise}
\end{cases}
$$
}
\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Uniform $X$ with parameter $a,b$}

\plitemsep 0.1in

\bci 
\item integers $a,b$, where $a \le b$

\item<2-> Choose a number out of $\Omega = \{ a, a+1, \ldots, b \}$ uniformly at random. 

\item<3-> $p_X(i) =$ \onslide<4->{$\frac{1}{b-a+1}, \ i \in \Omega$}

%\item $X(\omega) = \omega$

\onslide<4->{
\centering
\includegraphics[width=0.7\textwidth]{L3_uniform_ex.png}
}

\item<5-> Models complete \orangef{ignorance} (I don't know anything about $X$)

\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Binomial $X$ with parameter $n,p$}
\footnotetext{$\binom{n}{k} = \frac{n!}{k!(n-k)!}$, which we read `$n$ choose $k$'.}

\mytwocols{0.6}
{
\plitemsep 0.1in
\bci 
\item<2-> Models the number of \redf{successes} in a given number of \redf{independent} trials
\item<3-> $n$ independent trials, where one trial has the success probability $p.$

$$
\onslide<3->{p_X(k) =} \onslide<5->{{n \choose k} p^k (1-p)^{n-k}}
$$
\eci 
}
{
\onslide<4->{
\centering
\includegraphics[width=0.8\textwidth]{L3_binomial_ex.png}
}
}
\end{frame}

\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Poisson $X$ with parameter $\lambda$}

%\setdefaultleftmargin{0.1em}{}{}{}{}{}.

\plitemsep 0.05in
\bci 
\item<2-> $Binomial(n,p)$: Models the number of successes in a given number $n$ of independent trials with success probability $p.$

\item<3-> Very large $n$ and very small $p,$ such that $np =\lambda$
$$
p_X(k) = e^{-\lambda}\frac{\lambda^k}{k!}, \quad k=0,1, \ldots
$$

\item<4-> Is this a legitimate PMF?
\aleq{
\sum_{k=0}^\infty e^{-\lambda}\frac{\lambda^k}{k!} = e^{-\lambda} \left(1+ \lambda + \frac{\lambda^2}{2!} + \frac{\lambda^3}{3!} \ldots \right) = e^{-\lambda} e^\lambda = 1
}

\item<5-> Prove this:
$$
\lim_{n \rightarrow \infty} p_X(k) = {n \choose k} (1/n)^k (1-1/n)^{n-k} = e^{-\lambda}\frac{\lambda^k}{k!}
$$


\eci 

\end{frame}

\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Geometric $X$ with parameter $p$}

\mytwocols{0.5}
{
\plitemsep 0.1in
\bci 
\item<1-> Infinitely many independent Bernoulli trials, where each trial has success probability $p$

\item<2-> Random variable: number of trials until the \redf{first success.} 
$$
p_X(k) =  \onslide<3->{(1-p)^{k-1} p}
$$

\item<4-> Models \orangef{waiting} times until something happens. 
\eci 
}
{
\onslide<3->{
\centering
\includegraphics[width=0.8\textwidth]{L3_geo_ex.png}
}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L3(3)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)]

\item \grayf{Random variable: Idea and formal definition}

\item \grayf{Popular discrete random variables}

\item \redf{Summarizing random variables: Expectation and Variance}

\item \grayf{(Functions of) multiple random variables 

\item Conditioning for random variables

\item Independence for random variables} 

\ece 

\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{}
% \vspace{2cm}
% \LARGE Summarizing Random Variables: Expectation and Variance

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expectation/Mean}

\plitemsep 0.1in
\bci 
\item Average 
\begin{block}{Definition}
$$
\expect{X} = \sum_{x} x p_X(x)
$$
\end{block}

\item $p_X(x)$: relative frequency of value $x$ (trials with $x$/total trials)

\item<2-> \exam Bernoulli rv with $p$
$$
\expect{X} = 1\times p + 0 \times (1-p) = p = p_X(1)
$$
\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Properties of Expectation}

Not very surprising. Easy to prove using the definition.

\plitemsep 0.7in
\bci [$\circ$]

\item If $X \geq 0,$ $\expect{X} \geq 0.$

\item If $a \le X \le b,$ $a \le \expect{X} \le b.$

\item For a constant $c$, $\expect{c} = c.$
\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expectation of a function of a RV}

\plitemsep 0.05in
\bci 

\item<1-> For a rv $X,$ $Y =g (X)$ is also a r.v.

\item<2-> $\expect{Y} = \expect{g(X)} = \sum_{x} g(x) p_X(x)$ 

\item<3-> Compute $\expect{Y}$ for the following:

\mytwocols{0.4}{
\centering
\includegraphics[width=0.7\textwidth]{L3_rvfunc_ex.png}
}
{
\onslide<4->{
\begin{multline*}
4\times (0.4+0.3) + 3 \times (0.1+0.2) \cr 
= 2.8 + 0.9 = 3.7
\end{multline*}
}
}

%\item Using the above,
\onslide<5->{
\myblock{Linearity of Expectation}
{$\expect{aX +b} = a\expect{X} + b$}
}
\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Variance}

\plitemsep 0.1in
\bci 

\item<1-> Measures how much the \redblk{spread} of a PMF is. 

\item<2-> What about $\expect{X - \mu},$ where $\mu = \expect{X}$? Zero

\item<3-> Then, what about $\expect{(X - \mu)^2}$?


\onslide<4->{
\myblock{Variance, Standard Deviation}
{
\aleq{
    \var{X} &= \expect{(X-\mu)^2} \cr
    \sigma_X &= \sqrt{\var{X}}
}
\vspace{-0.3cm}
}
}
% \item Useful formula
% $$
%     \var{X} = \expect{X^2} - (\expect{X})^2
% $$
\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Variance: Useful Property}


\mytwocols{0.6}
{
\plitemsep 0.1in
\small

\bci 

\item  $\var{X} = \expect{X^2} - (\expect{X})^2$
\aleq{
\onslide<2->{
&\var{X} = \expect{X^2 - 2\mu X + \mu^2}\cr
&= \expect{X^2} -2\mu\expect{X} + \mu^2 = \expect{X^2} - \mu^2
}}
\item $Y= X+b$, $\var{Y} = \var{X}$
\aleq{
\onslide<3->{
\var{Y} = \expect{(X+b)^2} - (\expect{X+b})^2
}}

\item $Y = aX,$ $\var{Y} = a^2\var{X}$
\aleq{
\onslide<4->{
\var{Y} = \expect{a^2X^2} - (a\expect{X})^2
}
}


\eci 
}
{
\onslide<5->{Example: Variance of a Bernoulli rv ($p$)}
\aleq{
\onslide<6->{
\mu = \expect{X} &= 1\times p + 0 \times (1-p) = p\cr
\expect{X^2}&= 1 \times p + 0 \times (1-p) = p \cr
\var{X} &= \expect{X^2} - \mu^2 = p - p^2\cr
&= p(1-p)
}
}

}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L3(4)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)]

\item \grayf{Random variable: Idea and formal definition}

\item \grayf{Popular discrete random variables}

\item \grayf{Summarizing random variables: Expectation and Variance}

\item \redf{(Functions of) multiple random variables} 

\item \grayf{Conditioning for random variables

\item Independence for random variables} 

\ece 

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Joint PMF}

\mytwocols{0.75}
{
\plitemsep 0.1in
\bci 

\item<2-> \redblank{3}{Joint PMF.} For two random variables $X,Y,$ consider two events
$\{X = x \}$ and $\{Y = y \},$ and
\aleq{
\onslide<3->{\redf{\pxy} \ \eqdef} \ \cbprob{\{X = x \} \cap \{Y = y \}}
}
\item<4-> $\sum_x \sum_y \pxy = 1$

\item<5-> \redblank{6}{Marginal PMF.} 
\aleq{
    \px &= \sum_{y} \pxy, \cr
    \py &= \sum_{x} \pxy
}

\eci 
}
{
\onslide<6->{
\exam \hfill \lecturemark{VIDEO PAUSE}

\medskip
\centering
\includegraphics[width=0.5\textwidth]{L3_joint_ex.png}
}
\aleq{
\onslide<6->{p_{X,Y}(1,3)=} \onslide<7->{2/20}
}
\aleq{
\onslide<6->{p_{X}(4) =} \onslide<7->{2/20 + 1/20 = 3/20}
}
\aleq{
\onslide<6->{\cprob{X=Y}=} \onslide<7->{1/20 + 4/20 + 3/20 = 8/20}
}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Functions of Multiple RVs}

\plitemsep 0.1in
\bci

\item<1-> Consider a rv $Z = g(X,Y).$ (Ex) $X+Y,$ $X^2 + Y^2.$  Then, PMF of $Z$ is:
$$
\onslide<2->{\pz = \cprob{g(X,Y) = z} = \sum_{(x,y): g(x,y) = z} \pxy}
$$

\item Similarly, 
$$
\expect{Z} = \expect{g(X,Y)} = \onslide<2->{\sum_{x} \sum_{y} g(x,y) \pxy}
$$


\eci 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linearity of Expectation for Multiple RVs }


\mytwocols{0.7}
{
\plitemsep 0.15in
\bci 

\item Remember: $\expect{aX + b} = a\expect{X} +b$

\item<2-> Similarly, 
$$\expect{X+Y} = \expect{X} + \expect{Y}$$ 
(easy to prove, using the definition.)

\bigskip

\bigskip

\item<3-> $\expect{X_1 \ldots + X_n} = \expect{X_1} + \ldots + \expect{X_n}$

\item<3-> $\expect{2X + 3Y - Z} = 2\expect{X} + 3 \expect{Y} - \expect{Z}$
\eci 
}
{
\plitemsep 0.15in
\bci 

\item<4-> \exam Mean of a binomial rv $Y$ with $(n,p)$

\item<4-> $Y$: number of successes in $n$ Bernoulli trials with $p$

\item<5-> $Y = X_1 + \ldots X_n,$ where $X_i$ is a Bernoulli rv. 

\item<5-> $\expect{Y} = n \expect{X_i} = n \cprob{X_i = 1} = np$

\eci 

\medskip
\onslide<6->{\msg When some rv $X$ is written as a linear combination of other rvs, $X$ becomes easy to handle.}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L3(5)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)]

\item \grayf{Random variable: Idea and formal definition}

\item \grayf{Popular discrete random variables}

\item \grayf{Summarizing random variables: Expectation and Variance}

\item \grayf{(Functions of) multiple random variables} 

\item \redf{Conditioning for random variables}

\item \grayf{Independence for random variables} 

\ece 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conditional PMF: Conditioning on an event}

\onslide<1->{Remember two probability laws: $\cprob{\cdot}$ and $\cprob{\cdot | A}$ \bluef{for an event $A.$}}

\medskip

\mytwocols{0.7}
{
\plitemsep 0.2in
\bci 

\item<2-> $\px \eqdef \cprob{X=x}$

\item<3-> $\expect{X} = \sum_x x \px$

\item<4-> $\expect{g(X)} = \sum_x g(x) \px$

\item<5-> $\var{X} = \expect{X^2} - (\expect{X})^2$
\eci 
}
{
\plitemsep 0.2in
\bci 

\item<2-> $\redf{\pxA} \ \eqdef \ \cprob{X=x | A}$ 

\item<3-> $\redf{\expect{X | A}} \ \eqdef \ \sum_x x \pxA$

\item<4-> $\redf{\expect{g(X) | A}} \ \eqdef \ \sum_x g(x) \pxA$

\item<5-> $\redf{\var{X | A}} \ \eqdef \ \expect{X^2 |A } - (\expect{X|A})^2$

\medskip
\item<6-> \redf{(Note)} $\pxA,$ $\expect{X | A}$, $\expect{g(X) | A},$ and $\var{X |A}$ are all just notations! 

\eci 
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Conditional PMF}

\hfill $A = \{X \geq 2 \}$

\medskip

\mytwocols{0.7}
{
\includegraphics[width=0.55\textwidth]{L3_cond_pmf_ex1.png}

\medskip
\aleq{
\expect{X} = \onslide<2->{\frac{1}{4} \left( 1+2+3+4 \right) = 2.5}
}
\aleq{
\var{X} &= \onslide<3->{\expect{X^2} - (\expect{X})^2 \cr
&=\frac{1}{4}(1+2^2 +3^2+4^2) - 2.5^2}
}
}
{
\includegraphics[width=0.55\textwidth]{L3_cond_pmf_ex2.png}
\medskip

\aleq{
\expect{X | A} = \onslide<4->{\frac{1}{3}(2+3+4) = 3}
}
\aleq{
\var{X |A} &= \onslide<5->{\expect{X^2|A}-(\expect{X|A})^2}\cr
\onslide<6->{&= \frac{1}{3}(2^2 + 3^2 + 4^2) - 3^2 = 2/3}\cr
}


}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conditional PMF: Conditioning on a RV }

What do we mean by \redf{``conditioning on a rv"}? Consider $A = \{Y = y\}$ for a rv $Y.$

\medskip

\mytwocols{0.5}
{
\plitemsep 0.2in
\bci 

\item<2-> $\pxA \ \eqdef \ \cprob{X=x | A}$ 

\item<3-> $\expect{X | A} \ \eqdef \ \sum_x x \pxA$

\item<4-> $\expect{g(X) | A} \ \eqdef \ \sum_x g(x) \pxA$

\item<5-> $\var{X | A} \ \eqdef \ \expect{X^2 |A } - (\expect{X|A})^2$
\eci 
}
{
\plitemsep 0.2in
\bci 

\item<2-> $\redf{\pxcy} \ \eqdef \ \cprob{X=x |{Y = y }}$ 

\item<3-> $\redf{\expect{X | Y=y}} \ \eqdef \ \sum_x x \pxcy$

\item<4-> $\redf{\expect{g(X) | Y=y}} \ \eqdef \ \sum_x g(x) \pxcy$

\item<5-> $\redf{\var{X | Y=y}} \ \eqdef$ \\ $\expect{X^2 |Y=y } - (\expect{X|Y=y})^2$
\eci 

}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conditional PMF}


\mytwocols{0.7}
{
\plitemsep 0.1in
\small

\bci 
\item \redf{Conditional PMF} 
\onslide<2->{
$$\pxcy \ \eqdef \ \cprob{X=x |{Y = y }} = \frac{\pxy}{\py}$$ 
for $y$ such that $\py >0.$}

\item<3-> $\sum_{x} \pxcy = 1$

\item \redf{Multiplication rule}
\aleq{
\pxy &= \onslide<4->{\py \pxcy\cr    
     &= \px \pycx} 
}


\item $p_{X,Y,Z}(x,y,z) = \onslide<5->{\px \pycx p_{Z|X,Y}(z | x,y)}$
\eci 
}
{

\onslide<5->{
\hfill \lecturemark{VIDEO PAUSE}
\mypic{0.5}{L3_joint_ex.png}
}

\small

$ \onslide<5->{p_{X|Y}(2|2) =} \onslide<6->{\frac{1}{1+3+1}}$

\bigskip
$ \onslide<5->{p_{X|Y}(3|2)=} \onslide<6->{\frac{3}{1+3+1}}$

\bigskip

$ \onslide<5->{\expect{X | Y = 3} =} \onslide<6->{1(2/9)+ 2(4/9)+3(1/9)+4(2/9)} $

}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Remind: Total Probability Theorem (from Lecture 2)}

\mytwocols{0.7}
{
\plitemsep 0.1in
\bci 

\item Partition of $\Omega$ into $A_1,A_2,A_3$

\item Known: $\cprob{A_i}$ and $\cprob{B | A_i}$ 

\item What is $\cprob{B}$? 

\bigskip
\medskip

\begin{block}{Total Probability Theorem}
$$
\cprob{B} = \sum_{i} \cprob{A_i} \cprob{B | A_i}
$$
\end{block}

\eci 
}
{
\centering
\includegraphics[width=0.65\textwidth]{L2_total_ex.png}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Total Probability Theorem: $B = \{X=x \}$}

\myvartwocols{0.7}{0.7}{0.28}
{
\plitemsep 0.1in
\bci [$\circ$]

\item Partition of $\Omega$ into $A_1,A_2,A_3$


\bigskip
\medskip

\begin{block}{Total Probability Theorem}
$$
\redf{\px} = \sum_{i} \cprob{A_i} \cprob{\redf{X=x} | A_i} = \sum_{i} \cprob{A_i} \redf{p_{X|A_i}(x)} 
$$
\end{block}

\eci 
}
{
\centering
\includegraphics[width=0.8\textwidth]{L2_total_ex.png}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Total Expectation Theorem for $\{A_i \}$}

\myvartwocols{0.75}{0.7}{0.28}
{
\plitemsep 0.1in
\bci 

\item Partition of $\Omega$ into $A_1,A_2,A_3$


\bigskip
\begin{block}{Total Probability Theorem}
$$
\px = \sum_{i} \cprob{A_i} \cprob{X=x | A_i} = \sum_{i} \cprob{A_i} p_{X|A_i}(x) 
$$
\end{block}

\begin{block}{Total Expectation Theorem}
$$
\expect{X} = \sum_{i} \cprob{A_i} \expect{X | A_i}
$$
\end{block}

\item Weighted average of expectations from $A_i$'s perspective

\eci 
}
{
\centering
\includegraphics[width=0.8\textwidth]{L2_total_ex.png}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Total Expectation Theorem for $\{Y=y \}$}

\myvartwocols{0.7}{0.7}{0.28}
{
\plitemsep 0.1in
\bci

\item Partition of $\Omega$ into $A_1,A_2,A_3$


\bigskip
\begin{block}{Total Expectation Theorem}
$$
\expect{X} = \sum_{i} \cprob{\redf{A_i}} \expect{X | \redf{A_i}}
$$
\end{block}

\begin{block}<2->{Total Expectation Theorem}
$$
\expect{X} = \sum_{y} \cprob{\redf{Y=y}} \expect{X | \redf{Y= y}} = \sum_{y} \py \expect{X | \redf{Y=y}} 
$$
\end{block}


\eci 
}
{
\centering
\includegraphics[width=0.8\textwidth]{L2_total_ex.png}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 1: Total Expectation Theorem}

\myvartwocols{0.7}{0.6}{0.38}
{
\plitemsep 0.1in
\bci 

\item \question What is $\cexpect{X}$?
\item[(1)] Just using the definition of expectation,
\aleq{
\expect{X} &= \onslide<2->{\frac{1}{9}( 0 + 1 + 2) + \frac{2}{9} ( 6+7+8)\cr
& = \frac{3 + 12 + 14 + 16}{9} =  5}
}


\item<3->[(2)] Let's use TET, for which consider
$$A_1 = \{X \in \{0,1,2\} \}, \ A_2 = \{ X \in \{6,7,8\} \}$$ 

\aleq{
\onslide<4->{\expect{X} &= \sum_{i=1,2}\cprob{A_i} \expect{X | A_i}\cr
&= 1/3 \cdot 1 + 2/3 \cdot 7 = 5}
}

\eci 
}
{
\centering
\includegraphics[width=0.95\textwidth]{L3_total_exp_ex.png}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 2: Mean of Geometric rv}

\plitemsep 0.1in
\bci 

\item<1-> Write softwares over and over, and each time w.p. $p$ of working correctly (independent from previous programs). 

\item<2-> $X$: number of trials until the program works correctly. 

\item<3-> \redf{(Q)} $\cexpect{X}$?

\item<4-> $X$ is a geometric rv

\item<5-> Direct computation is boring.
\aleq{
\expect{X} = \sum_{k=1}^\infty k(1-p)^{k-1} p = p + 2(1-p)p + 3(1-p)^2 p + \cdots
}

\item<6-> Total expectation theorem and a notion of \orangef{memorylessness} helps a lot. 
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Memoryless Property: Motivating Example}

\mypic{0.9}{L3_memoryless_ex.png}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Background: Memoryless Property}

\plitemsep 0.2in
\bci 

\item<1-> Some random variable often does not have \redf{memory.}

\item<2-> \bluef{Definition.} A random variable $X$ is called \redblk{memoryless} if, for any $n,m \ge 0,$ 
$$\cprob{X > n+m | X > m} = \cprob{X > n}$$

\item<3-> \bluef{Meaning.} Conditioned on $X > m,$ $X-m$'s distribution is the same as the original $X.$
$$\cprob{X -m > n | X > m} = \cprob{X > n}$$

% \item<4-> Suppose that $X$ is the time of waiting for a bus and $X$ is memoryless. 
% At the bus stop, I have waited for the bus for 10 mins. 
% Then, the time until the bus arrival does not depend on how much I have waited for a bus. \orangef{No memory.}   


\eci

\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Background: Memoryless Property of Geometric rv (1)}

% \plitemsep 0.1in
% \bci 

% \item<1-> Some random variable often does not have \redf{memory.}

% \item<2-> \bluef{Definition.} A random variable $X$ is called \redblk{memoryless} if, for any $n,m \ge 0,$ $$\cprob{X > n+m | X > m} = \cprob{X > n}$$

% \item<3-> \bluef{Meaning.} Conditioned on $X > m,$ $X-m$'s distribution is the same as the original $X.$

% \item<4-> \bluef{Remind.} Geometric rv $X$ with parameter $p$
% \aleq{
% \cprob{X = k}& = (1-p)^{k-1} p\cr
% \cprob{X > k}& = 1 - \sum_{k'=1}^k (1-p)^{k'-1} p = (1-p)^k
% }
% \eci

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Background: Memoryless Property of Geometric RVs}

\plitemsep 0.07in
\bci 

\item \bluef{Theorem.} Any \orangef{\bf geometric} random variable is \redf{memoryless.}

\item<2-> \bluef{Remind.} Geometric rv $X$ with parameter $p$
\aleq{
\cprob{X = k}& = (1-p)^{k-1} p, \quad \magenf{\cprob{X > k}} = \sum_{i=k+1}^\infty (1-p)^{i-1} p = \magenf{(1-p)^k}
}


\item<3-> \proff 
\begin{align*}
\cprob{X > n+m | X > m} &= \frac{\cprob{X > n+m \text{ and } X > m}}{\cprob{X > m}} 
 = \frac{\cprob{X > n+m}}{\cprob{X > m}}\cr
& = \frac{(1-p)^{n+m}}{(1-p)^m} = (1-p)^n = \cprob{X >n}
\end{align*}

\item<4-> \bluef{Meaning.} Conditioned on $X > m,$ $X-m$ is geometric with the same parameter. 

\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Back to Example 2: Mean of Geometric rv}


\plitemsep 0.2in

\bci 

\item<1-> $A_1 = \{X=1 \}$ (first try is success), $A_2 = \{X >1 \}$ (first try is failure).  

\aleq{
    \expect{X} & = 1 + \expect{X-1} \cr
 &= \onslide<2->{1 + \cprob{A_1} \expect{X-1 | X=1} + \cprob{A_2} \expect{X-1 | X >1}} &\quad \text{(from TET)}\cr
&= \onslide<3->{1 + (1-p) \expect{X}} & \quad \text{(from memorylessness)}
}

\item<4-> Thus, $\expect{X} = \dfrac{1}{p}$
\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L3(6)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)]

\item \grayf{Random variable: Idea and formal definition}

\item \grayf{Popular discrete random variables}

\item \grayf{Summarizing random variables: Expectation and Variance}

\item \grayf{(Functions of) multiple random variables} 

\item \grayf{Conditioning for random variables}

\item \redf{Independence for random variables} 

\ece 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Independence, Conditional Independence}

%\small
\plitemsep 0.02in

\bci 
\item<2-> Two events
\aleq{
\cprob{A \cap B} &= \cprob{A} \cdot \cprob{B} \cr
\cprob{A \cap B \redf{| C}} &= \cprob{A \redf{|C}} \cdot \cprob{B \redf{|C}}
}

\item<3-> A rv and an event
\aleq{
\cprob{ \{X = x \} \cap B} &= \cprob{X=x} \cdot \cprob{B}, \quad \text{for all $x$} \cr
\cprob{ \{X = x \} \cap B\redf{| C} } &= \cprob{X=x\redf{| C}} \cdot \cprob{B\redf{| C}}, \quad \text{for all $x$}
}

\item<4-> Two rvs
\aleq{
\cprob{ \{X = x \} \cap \{Y=y \}} &= \cprob{X=x} \cdot \cprob{Y=y}, \quad \text{for all $x,y$}\cr
\onslide<5->{\pxy&= \px \cdot \py}
}
\aleq{
\cprob{ \{X = x \} \cap \{Y=y \}\redf{|{Z=z}}} &= \cprob{X=x\redf{|{Z=z}}} \cdot \cprob{Y=y\redf{|{Z=z}}}, \quad \text{for all $x,y$}\cr
\onslide<5->{p_{X,Y\redf{|Z}}(x,y)&=p_{X\redf{|Z}}(x) \cdot p_{Y\redf{|Z}}(y)}
}

\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example}


\mytwocols{0.7}
{
\plitemsep 0.05in
\bci 

\item $X \indep Y$?
\onslide<2->{
\aleq{
p_{X,Y}(1,1) &= 0 \cr
p_X(1) &= 3/20\cr
p_Y(1) &= 1/20
}}

\item $X \indep Y | \{X \leq 2\text{ and } Y \ge 3\}$? 

\hfill \only<3>{\lecturemark{\small VIDEO PAUSE}}

\small
\onslide<3->{
\begin{tabular}{|c|c|c|} \hline 
$Y=4$ \onslide<4->{\red (1/3)}& \onslide<4->{\blue 1/9} & \onslide<4->{\blue 2/9}\\ \hline
$Y=3$ \onslide<4->{\red (2/3)}& \onslide<4->{\blue 2/9}& \onslide<4->{\blue 4/9}\\ \hline
& $X=1$ \onslide<4->{\red (1/3)}&$X=2$ \onslide<4->{\red (2/3)} \\ \hline
\end{tabular}
}

\onslide<5->{- Yes.}

\eci 
}
{
\centering
\includegraphics[width=0.6\textwidth]{L3_joint_ex.png}

% \medskip

% \small
% \onslide<3->{
% \begin{tabular}{|c|c|c|} \hline 
% $Y=4$ (1/3)& 1/9 & 2/9\\ \hline
% $Y=3$ (2/3)& 2/9& 4/9\\ \hline
% & $X=1$ (1/3)&$X=2$ (2/3) \\ \hline
% \end{tabular}
% }
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expectation and Variance}

\mytwocols{0.8}
{
%\small
\plitemsep 0.05in
\bci 

\item<1-> Always true. 

% $\expect{aX +b}$, 
$\expect{X+Y} = \expect{X} + \expect{Y}$

\item<2-> Generally,
$\expect{g(X,Y)} \neq g(\expect{X},\expect{Y})$

\medskip

\item<3-> However, if $X \indep Y,$
\aleq{
\expect{XY} &= \expect{X} \expect{Y} \cr
\expect{g(X)h(Y)} &= \expect{g(X)} \expect{g(Y)}
}
\item<4-> \bluef{Proof.}
\aleq{
\expect{g(X)h(Y)} &= \sum_{x}\sum_{y} g(x)h(y) \pxy \cr
&= \sum_x g(x) \px \sum_y h(y) \py
}

\eci
}
{
\plitemsep 0.1in
\bci 

\item<5-> Always true. 

$\var{aX} = a^2 \var{X}$, $\var{X + a} = \var{X}$

\item<6-> Generally,
$\var{X+Y} \neq \var{X} + \var{Y}$ (next slide)

\medskip

\item<7-> However, if $X \indep Y,$
\aleq{
\var{X+Y} &= \var{X} + \var{Y}
}

\item<7-> \redf{Practice.}

\plitemsep 0.02in
\bci
\item<8-> $X=Y$ $\imp$ $\var{X+Y} = 4 \var{X}$
\item<9-> $X=-Y$ $\imp$ $\var{X+Y} = 0$
\item<10-> $X \indep Y$ $\imp$ $\var{X-3Y} = \var{X} + 9 \var{Y}$
\eci

\eci

}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{$\var{X + Y} \neq \var{X} + \var{Y}$}

\plitemsep 0.1in
\bci [$\circ$]

\item<1-> Why not generally true?

\onslide<2->{
\aleq{
\var{X+Y} &= \expect{(X+Y)^2} - (\expect{X+Y})^2 \cr
& = \expect{X^2 + Y^2 + 2XY} - \left( (\expect{X})^2 +(\expect{Y})^2 + 2 \expect{X}\expect{Y} \right ) \cr
& = \var{X} + \var{Y} +2 \left(\expect{XY} - \expect{X}\expect{Y} \right)
}}

\item<3-> \redblank{4}{$X \indep Y$} is a sufficient condition for $\expect{XY} = \expect{X}\expect{Y}$ 

\item<5-> Also, a necessary condition? we will see later, when we study \redf{covariance.} 
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: The hat problem (1)}

\plitemsep 0.1in
\bci 
\item<1-> $n$ people throw their hats in a box and then pick one at random

\item<1-> $X$: number of people with their own hat

\item<2->  $\redf{\expect{X}}$? $\redf{\var{X}}$?

\item<3-> All permutations are equally likely as $1/n!.$ Thus, this equals to picking one hat at a time.

\item<4-> \bluef{Key step 1.} Define a rv $X_i=1$ if $i$ selects its own hat and $0$ otherwise. 
$$X = \sum_{i=1}^n X_i.$$

\item<5-> $\{X_i\}, i=1, 2, \ldots, n$: identically distributed (from symmetry)

% \item $\expect{X} = n \expect{X_1} = n \cprob{X_1 =1} = 1/n.$

% \item \bluef{Key step 2.} Are $X_i$s are independent? If yes, easy to get $\cvar{X}.$

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: The hat problem (2)}

\plitemsep 0.1in
\bci 
\item<1-> $\expect{X} = n \expect{X_1} = n \cprob{X_1 =1} = n\times \frac{1}{n} = 1.$ 

\item<2-> \bluef{Key step 2.} Are $X_i$s are independent? If yes, easy to get $\cvar{X}.$

\item<3-> Assume $n=2.$ Then, $X_1=1 \rightarrow X_2=1,$ and $X_1=0 \rightarrow X_2=0.$ Thus, \redf{dependent}.
\aleq{
\onslide<4->{\cvar{X} &= \expect{X^2} - (\expect{X})^2 = \bexpect{\sum_{i}X_i^2 + \sum_{i,j:i\neq j} X_i X_j} - (\expect{X})^2}\cr
\onslide<5->{\expect{X_i^2} &= \expect{X_1^2} = 1 \times \frac{1}{n} + 0 \times \frac{n-1}{n} = \frac{1}{n}}\cr
\onslide<6->{\expect{X_i X_j} &= \expect{X_1 X_2} = 1 \times \cprob{X_1 X_2 = 1} = \cprob{X_1=1}\cprob{X_2 =1 | X_1 =1}, \quad \text{($i \neq j$)}}
}

\item<7-> $\expect{X^2} = n \expect{X_1^2} + n(n-1) \expect{X_1 X_2} = n\frac{1}{n} + n(n-1)\frac{1}{n(n-1)} = 2$

\item<8-> $\cvar{X} = 2-1 =1$ 
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\vspace{2cm}
\LARGE Questions?

\end{frame}

\begin{frame}{Review Questions}

\bce[1)]
\item What is Random Variable? Why is it useful?

\item What is PMF (Probability Mass Function)?

\item Explain Bernoulli, Binomial, Poisson, Geometric rvs, when they are used and what their PMFs are. 

\item What are joint and marginal PMFs?

\item Describe and explain the total probability/expectation theorem for random variables?

\item When is it useful to use total probability/expectation theorem?

\item What is conditional independence? 
\ece
\end{frame}



\end{document}
