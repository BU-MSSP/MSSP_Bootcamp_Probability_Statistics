
\input{../mode.tex}
\csname\pdfmode\endcsname

\mode<presentation>
{
  \usetheme{default}
  \usecolortheme{default}
  \usefonttheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]  % or "page number"
  \setbeamercolor{frametitle}{fg=white}
  \setbeamercolor{footline}{fg=black}
}

\input{../myhead.tex}

\title[]{Lecture 6: Statistical Inference}
\author{Yi, Yung (이융)}
\institute{EE210: Probability and Introductory Random Processes\\ KAIST EE}
\date{\today}

\input{../mymath}


\begin{document}

\input{../mydefault}

\begin{frame}
  \titlepage
\end{frame}

% % Uncomment these lines for an automatically generated outline.
% \begin{frame}{Outline}
% % \tableofcontents
% \plitemsep 0.1in
% \bci
% \item

% \item
% \eci
% \end{frame}

% START START START START START START START START START START START START START

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bci

\item Basics on Statistic Inference

\item Framework of Bayesian Inference

\item MAP (Maximum A Posteriori) Estimator

\item LMS (Least Mean Squares) Estimator

\item LLMS (Linear LMS) Estimator

\medskip

\item Framework of Classical Inference
\item ML (Maximum Likelihood) Estimator
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bci

\item \redf{Basics on Statistic Inference}

\item \redf{Framework of Bayesian Inference}

\item MAP (Maximum A Posteriori) Estimator

\item LMS (Least Mean Squares) Estimator

\item LLMS (Linear LMS) Estimator

\medskip
\item Framework of Classical Inference
\item ML (Maximum Likelihood) Estimator

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Inference: Big Picture}

\begin{center}
\mypic{0.5}{L6_bigpicture.png}
\end{center}

\vspace{-0.5cm}
\plitemsep 0.1in
\bci
\item<2-> Inference

- Using data, probabilistic models or parameters for models are determined.

\item<3-> Why building up models?

- Analysis is possible, so that predictions and decisions are made.

\item<4-> Recently, deep learning

- Connecting big data and big model building

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What to Infer?: Unknown Model vs. Unknown Variable}

\begin{center}
\mypic{0.5}{L6_comm_ex.png}
\end{center}

\vspace{-0.5cm}
\plitemsep 0.05in
\bci
\item $X = aS + W$

\item<2-> Modeling building
\bci
\item know the original signal $S$, observe $X$
\item infer the model parameter $a$
\eci

\item<3-> Variable estimation
\bci
\item know $a,$ observe $X$
\item infer the original signal $S$
\eci


\item<4-> Same mathematical structure, because the parameters in models are variables in many cases

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What Kind of Inference?: Hypothesis testing vs. Estimation}

\plitemsep 0.1in
\bci
\item<2-> Hypothesis testing

\bci
\item<3-> Unknown: a few possible ones
\item<4-> Goal: small probability of incorrect decision
\item<5-> \bluef{(Ex)} Something detected on the radar. Is it a bird or an airplane?
\eci

\item<2-> Estimation
\bci
\item<3-> Unknown: a value included in  an infinite,  typically continuous set
\item<4-> Goal: Finding the value close to the true value
\item<5-> \bluef{(Ex)} Biased coin with unknown probability of head $\theta \in [0,1]$. Data of heads and tails. What is $\theta$?
\item<6-> \bluef{(Note)} If you have the candidate values of $\theta = \{1/4, 1/2, 3/4 \},$ then it's a hypothesis testing problem
\eci

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Inference with Different Views: Bayesian vs. Classical (1)}

\onslide<1->{- Biased coin with parameter $\theta$ (probability of head). Assume that $\theta \in \{1/4,3/4\}.$}

\onslide<2->{- Throw the coin 3 times and get $(H,H,H).$ Goal: infer $\theta,$ $1/4$ or $3/4$?}

% \plitemsep 0.1in
% \bci
% \item
% \eci
\medskip

\mytwocols{0.55}
{
\small
\plitemsep 0.05in
\bci
\item<3-> Distribution of $\theta$ (\redf{prior}) e.g.,
\aleq{
\cprob{\theta = {3 \over 4}} = 1/2, \quad \cprob{\theta = \frac{1}{4}} = 1/2
}
\item<4-> Use Bayes' rule and find the \redf{posterior}:
\aleq{
\bprob{\theta = \frac{3}{4} \Big | (HHH)}=\frac{27}{28}, \ \bprob{\theta = \frac{1}{4} \Big | (HHH)} = \frac{1}{28}
}
\item<5-> Choose $\theta$ with larger posterior probability.

\item<8-> \empr{Bayesian approach} (Chapter 8)
\eci

}
{
\small
\plitemsep 0.05in
\bci
\item<6-> Find the probability of $(H,H,H),$ if $\theta=\frac{1}{4}$ or $\frac{3}{4}$ (\redf{likelihood})
\aleq{
\bprob{ (HHH) | \theta=\frac{3}{4}} &= \lf(\frac{3}{4} \ri)^3 \cr
\bprob{ (HHH) | \theta=\frac{1}{4}} &= \lf (\frac{1}{4} \ri)^3
}

\item<7-> Choose $\theta$ with a larger likelihood.

\item<8-> \empr{Classical approach} (Chapter 9)

\eci

}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Inference with Different Views: Bayesian vs. Classical (2)}

\mytwocols{0.55}
{
\bluef{Bayesian approach}


\plitemsep 0.08in
\bci
\item<2-> Unknown: random variable with some distribution (prior)
\item<3-> Unknown model as chosen randomly from a give model class
\item<4-> Observed data $x$ gives: posterior distribution $p_{\Theta | X}(\theta | x)$
\item<5-> Choose $\theta$ with larger posterior probability \onslide<7->{(other methods exist)}
\eci
}
{
\bluef{Classical approach}

\plitemsep 0.08in
\bci
\item<2-> Unknown: deterministic value
\item<3-> Unknown model as one of multiple probabilistic models
\item<4-> Observed data $x$ gives: likelihood $p(X;\theta)$
\item<5-> Choose $\theta$ with larger likelihood\\ \onslide<7->{(other methods exist)}
\eci
}
\medskip


\centering
\onslide<6->{
- Who is the winner? A century-long debate (see p. 409 for discussion)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Framework of Bayesian Inference}

\begin{center}
\mypic{0.65}{L6_BI_framework.png}
\end{center}


\mytwocols{0.55}
{
\plitemsep 0.08in
\bci
\item<2-> Unknown $\Theta$

\bci
\item physical quantity or model parameter
\item random variable
\item prior distribution $p_{\Theta}$ and $f_{\Theta}$
\eci

\item<3-> Observations or measurements $X$

\bci
\item observation model $p_{X|\Theta}$ and $f_{X|\Theta}$
\eci

\item<4-> That is, the joint distribution of $X$ and $\Theta$, $p_{X,\Theta}$ and $f_{X,\Theta}$, is given
\eci
}
{
\plitemsep 0.08in
\bci
\item<5-> Find the posterior distribution $p_{X|\Theta}$ and $f_{X|\Theta}.$

\bci
\item Use Bayes' rule
%\item $\Theta$ and $X$: continuous and discrete (4 cases)
\eci

\item<6-> Using the posterior distribution, apply one of the methods of choosing the final $\hat{\theta}$ for estimation and hypothesis testing.
\eci
}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bci

\item \bluef{Basics on Statistic Inference}

\item \bluef{Framework of Bayesian Inference}

\item \redf{MAP (Maximum A Posteriori) Estimator}

\item LMS (Least Mean Squares) Estimator

\item LLMS (Linear LMS) Estimator

\medskip
\item Framework of Classical Inference
\item ML (Maximum Likelihood) Estimator

\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Point Estimation}

\begin{center}
\mypic{0.6}{L6_posterior.png}
\end{center}
\vspace{-0.6cm}

\plitemsep 0.05in
\bci
\item<1-> Given observation $x$, which $\theta$ are you going to choose?

\item<2->[M1.] Choose the largest: Maximum a posteriori probability (MAP) rule

\begin{center}
\redblank{3}{
$
\hat{\theta}_{\text{MAP}} =  \arg \max_{\theta} p_{\Theta | X}(\theta | x), \quad \hat{\theta}_{\text{MAP}} =  \arg \max_{\theta} f_{\Theta | X}(\theta | x)
$
}
\end{center}

\item<4->[M2.] Choose the mean: Conditional expectation, aka LMS (Least Mean Square)

\begin{center}
\redblank{5}{
$\hat{\theta}_{\text{LMS}} = \expect{\Theta | X=x}$
}
\end{center}

\item<6-> Why MAP and LMS are good? Not mathematically clear yet (later)

% \item<7-> Estimate as a number: $\hat{\theta} = g(x)$

% \item<8-> Estimator as a random variable number: $\hat{\Theta} = g(X)$

% \item Any theoretical support for optimality of MAP and LMS in some sense?

% - discuss later

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Estimator as a function}

\plitemsep 0.3in
\bci
\item Random observation: $X$

\item Observation instance: $x$

\item<1-> Estimate as a mapping from $x$ to a number
$$\hth = g(x),\quad \hth_\MAP = g_\MAP(x),\quad \hth_\LMS = g_\LMS(x)$$

\item<2-> Estimator as a mapping from $X$ to a random variable
$$\hTH = g(X), \quad \hTH_\MAP = g_\MAP(X), \quad \hTH_\LMS = g_\LMS(X)$$

% \item Any theoretical support for optimality of MAP and LMS in some sense?

% - discuss later

\eci

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 1: Romeo and Juliet}

\mytwocols{0.85}
{
\small
\plitemsep 0.05in
\bci
\item<2-> Romeo and Juliet start dating.

- Romeo: late by $X \sim U[0,\theta].$
\item<3-> Unknown: $\theta$ modeled by a rv $\Theta \sim U[0,1].$
\vspace{-0.3cm}
\aleq{
\onslide<4->{\fth &=
\begin{cases}
1, & 0 \le \theta \le 1 \cr
0, & \text{otherwise}
\end{cases}}\cr
\onslide<5->{\fxcth &=
\begin{cases}
\frac{1}{\theta}, & 0 \le x \le \theta\cr
0, & \text{otherwise}
\end{cases}}
}
\vspace{-0.5cm}
\onslide<6->{
\aleq{
\fthcx &= \frac{\fth \fxcth}{\int_0^1 f_{\Theta}(\theta')f_{X|\Theta}(x | \theta') d\theta'}\cr
&= \frac{1/\theta}{\int_{x}^1 \frac{1}{\theta'} d\theta'} = \frac{1}{\theta |\log x|}, \ x \le \theta \le 1,
}
and
$\fthcx = 0, \ \theta < x \text{ or } \theta >1.$
}
\eci
}
{
\small
\plitemsep 0.07in
\bci

\item<7-> MAP rule

- Given $x,$ $\fthcx$ is decreasing in $\theta$ over $[x,1].$

- $\hat{\theta}_{\text{MAP}} = x.$

\item<8-> Conditional expectation estimator
\vspace{-0.3cm}
\aleq{
\hat{\theta}_{\text{LMS}} &= \expect{\theta | X=x} = \int_{x}^1 \theta \frac{1}{\theta |\log x|} d\theta \cr
& = (1-x)/|\log x|
}
\vspace{-0.6cm}
\centering
\onslide<9->{\mypic{0.6}{L6_romeo_juliet.png}}
\eci
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 2: Biased Coin with Beta Prior (1)}

\plitemsep 0.15in
\bci
\item<1-> Biased coin with probability of head $\theta$
\item<2-> Unknown $\theta$: modeled by $\Theta$ with some prior $\fth$
\item<3-> Observation $X$: number of heads out of $n$ tosses
\item<4-> Posterior PDF
\aleq{
f_{\Theta | X}(\theta | k) &= c \fth \redf{p_{X|\Theta}(k | \theta)}= c \redf{{n \choose k}} \fth \redf{\theta^k(1-\theta)^{n-k}}, \ \text{$c$ the normalizing constant}
}
\item<5-> If $\Theta \sim Beta(\alpha,\beta),$ what is $\hth_\MAP$?

\item<6-> What is $Beta(\alpha,\beta)$?
\eci


% \mytwocols{0.3}
% {
% \small
% }
% {
% \small
% \plitemsep 0.07in
% \bci

% \item
% \eci
% }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 2: Biased Coin with Beta Prior (2)}

\onslide<1->{
\myblock{Beta distribution}
{
\small
A continuous rv $\Theta$ follows a beta distribution with integer parameters $\alpha,\beta >0,$ if
\aleq{
\fth =
\begin{cases}
\frac{1}{B(\alpha,\beta)} \theta^{\alpha -1}(1-\theta)^{\beta -1}, & 0< \theta < 1, \cr
0, & \text{otherwise},
\end{cases}
}
where $B(\alpha,\beta)$, called Beta function, is a normalizing constant, given by
\aleq{
B(\alpha,\beta) = \int_0^1 \theta^{\alpha -1}(1-\theta)^{\beta -1} d\theta = \frac{(\alpha-1)! (\beta -1)!}{(\alpha + \beta-1)!}
}
\vspace{-0.3cm}
}}


\plitemsep 0.05in
\bci

\item<2-> A special case of $Beta(1,1)$ is $Uniform[0,1]$
% \item<2-> If $\Theta \sim Beta(\alpha,\beta),$ then $\Theta|\{X=k\} \sim Beta(k+\alpha,n-k+\beta)$

% - Very useful: \redblank{3}{Beta prior $\imp$ Beta posterior} (homework problem)

% \item MAP estimate
% \aleq{
% \hth_\MAP = \arg \max_{\theta} (k \log \theta + (n-k) \log (1-\theta))
% = \frac{k}{n}
% }

% \item Conditional expectation estimate = $\frac{k+1}{n+2}$ (homework problem)

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 2: Biased Coin with Beta Prior (3)}

\plitemsep 0.05in
\bci

\item<2-> If $\Theta \sim Beta(\alpha,\beta),$ then $\Theta|\{X=k\} \sim Beta(k+\alpha,n-k+\beta)$


- Very useful: Beta prior $\imp$ Beta posterior

\item<3-> \redf{Proof.} For $Beta(\alpha,\beta)$ prior,
\aleq{
\fth &= \frac{1}{B(\alpha,\beta)}\theta^{\alpha-1} (1-\theta)^{\beta -1}\cr
\fthck&= c {n \choose k} \fth \theta^k(1-\theta)^{n-k} = \frac{d}{B(\alpha,\beta)} \cdot \theta^{\alpha+k-1}(1-\theta)^{\beta + n-k -1}
}
where $d = c{n \choose k}.$
\item<4-> Taking the logarithm,
\aleq{
\onslide<4->{\hth_\MAP &= \arg \max_{\theta} \Bl [(\alpha+k-1) \log \theta + (\beta + n-k+1) \log(1-\theta) \Bl ]
&=\frac{\alpha + k -1}{\alpha + \beta -2 + n}}
}

\item<5-> When $\alpha=\beta =1$ (i.e., $U[0,1]$ prior), $\hth_\MAP = \frac{k}{n}$


% \item MAP estimate
% \aleq{
% \hth_\MAP = \arg \max_{\theta} (k \log \theta + (n-k) \log (1-\theta))
% = \frac{k}{n}
% }

% \item Conditional expectation estimate = $\frac{k+1}{n+2}$ (homework problem)

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 3: Spam Filtering}

\plitemsep 0.05in
\bci
\item<1-> E-mail: spam (1) or legitimate (2), $\Theta  \in \{1,2\},$ with prior $p_{\Theta}(1)$ and  $p_{\Theta}(2).$

\item<2-> $\{w_1, w_2, \ldots, w_n \}$: a collection of words which suggest ``spam".

\item<3-> For each $i,$ a Bernoulli $X_i=1$ if $w_i$ appears and 0 otherwise.

\item<4-> Observation model $p_{X_i|\Theta(x_i|1)}$ and $p_{X_i|\Theta(x_i|2)}$ are known. Conditioned on $\Theta,$ $X_i$ are independent.
\item<5-> Posterior PMF
 \aleq{
 \cbprob{\Theta = m | (x_1,\ldots, x_n)} = \frac{p_{\Theta}(m) \prod_{i=1}^n p_{X_i|\Theta}(x_i|m)  }{
 \sum_{j=1,2} p_{\Theta}(j) \prod_{i=1}^n p_{X_i|\Theta}(x_i|j), \quad m=1,2
 }
}
\item<6-> MAP rule for this hypothesis testing problem. Decided that the message is spam if
\aleq{
p_{\Theta}(1) \prod_{i=1}^n p_{X_i|\Theta}(x_i|1) > p_{\Theta}(2) \prod_{i=1}^n p_{X_i|\Theta}(x_i|2)
}
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MAP's Performance Guarantee}

\plitemsep 0.1in
\bci
\item<1-> MAP estimate is intuitive, but we need more mathematical support.

\item<2-> \redf{Claim 1.} \bluef{For a given $x,$} the MAP rule minimizes the probability of an incorrect decision.

\item<3-> \redf{Claim 2.} The MAP rule minimizes the overall probability of an incorrect decision, \bluef{averaged over $x.$ }

\item<4-> \redf{Proof.}
\onslide<5->{Let $I$ and $I_{map}$ be the indicator rv, representing the correct decision by any general estimator and the MAP, respectively.}
\aleq{
\onslide<6->{\expect{I | X=x}=}
\onslide<7->{\bprob{g(X) = \Theta | X=x} \le }
\onslide<8->{\bprob{g_{map}(X) = \Theta | X=x} =}
\onslide<9->{\expect{I_{map}|X=x}}
}
\onslide<10->{Thus, \redf{Claim 1} holds. We now take the expectation of the above equations, the law of iterated expectations leads to \redf{Claim 2.}}
\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bci

\item \bluef{Basics on Statistic Inference}

\item \bluef{Framework of Bayesian Inference}

\item \bluef{MAP (Maximum A Posteriori) Estimator}

\item \redf{LMS (Least Mean Squares) Estimator}

\item LLMS (Linear LMS) Estimator

\medskip
\item Framework of Classical Inference
\item ML (Maximum Likelihood) Estimator

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Least Mean Squares Estimator (1)}

\plitemsep 0.05in
\bci

\item<1-> Unknown: $\theta$ modeled by $\Theta$ with prior $f_{\Theta}(\cdot).$ Assume $\Theta \sim Uniform [4,10].$

\item<2-> No observations available

\item<3-> MAP estimate

- Any value $\hat{\theta}_{map} \in [4,10]$ (why? posterior = prior), not very useful

\item<4-> What is your other choice?

\onslide<5->{- Expectation: \redf{$\hat{\theta} = \expect{\Theta} = 7$}

- looks reasonable, but why?}

\item<6-> Because it minimizes mean squared error (MSE)
\small
\aleq{
\min_{\hat{\theta}} \bexpect{(\Theta - \hat{\theta})^2} = \min_{\hat{\theta}}
\lf ( \cvar{\Theta - \hat{\theta}} + \Bl(\expect{\Theta - \hat{\theta}}\Bl)^2 \ri )=  \min_{\hat{\theta}} \lf ( \cvar{\Theta} + \Bl(\expect{\Theta - \hat{\theta}}\Bl)^2 \ri )
}

\onslide<7->{- minimized when \redf{$\hat{\theta} = \expect{\Theta}.$}}

%- Why this conditional expectation estimate is also called LMS (Least Mean Squares) estimator
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Least Mean Squares Estimator (2)}

\plitemsep 0.1in
\bci

\item<1-> Unknown: $\theta$ modeled by $\Theta$ with prior $f_{\Theta}(\cdot).$

\item<1-> Observation $X=x$ with model $f_{X|\Theta}(x|\theta)$

\item<2-> Minimizing conditional mean squared error

$$
\min_{\hat{\theta}} \bexpect{(\Theta - \hat{\theta})^2 | X=x}
$$

\bci
\item<3-> minimized when \redf{$\hat{\theta} = \expect{\Theta | X=x}.$}
\item<4->  LMS estimator \redf{$\hat{\Theta} = \expect{\Theta | X}$}
\eci

\item<5-> Performance (MSE: Mean Squared Error)
\bci
\item<6-> When $X=x$, $\bexpect{(\Theta - \expect{\Theta | X=x} )^2 | X=x} = \cbvar{\Theta | X=x}$
\item<7-> Averaged over $X$: $\bexpect{(\Theta - \expect{\Theta | X} )^2} = \bexpect{\cvar{\Theta | X=x}}$
\eci



\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet}

\mytwocols{0.8}
{
\small
\plitemsep 0.05in
\bci
\item Romeo and Juliet start dating.

- Romeo: late by $X \sim U[0,\theta].$
\item Unknown: $\theta$ modeled by a rv $\Theta \sim U[0,1].$
\vspace{-0.3cm}
\aleq{
\fth &=
\begin{cases}
1, & 0 \le \theta \le 1 \cr
0, & \text{otherwise}
\end{cases}\cr
\fxcth &=
\begin{cases}
\frac{1}{\theta}, & 0 \le x \le \theta\cr
0, & \text{otherwise}
\end{cases}
}
\vspace{-0.5cm}
\aleq{
\fthcx &= \frac{\fth \fxcth}{\int_0^1 f_{\Theta}(\theta')f_{X|\Theta}(x | \theta') d\theta'}\cr
&= \frac{1/\theta}{\int_{x}^1 \frac{1}{\theta'} d\theta'} = \redf{\frac{1}{\theta |\log x|}}, \ x \le \theta \le 1,
}
and
$\fthcx = 0, \ \theta < x \text{ or } \theta >1.$

\eci
}
{
\small
\plitemsep 0.07in
\bci

\item MAP rule

- $\hat{\theta}_{\text{MAP}} = x.$

\item<2-> LMS estimator
\vspace{-0.3cm}
\aleq{
\hat{\theta}_{\text{LMS}} &= \expect{\theta | X=x} = \int_{x}^1 \theta \frac{1}{\theta |\log x|} d\theta \cr
& = \redf{(1-x)/|\log x|}
}
\vspace{-0.6cm}
\centering
\onslide<3->{\mypic{0.7}{L6_romeo_juliet.png}}
\eci
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Biased Coin with Beta Prior}

\plitemsep 0.1in
\bci

\item \bluef{Remind.} If $\Theta \sim Beta(\alpha,\beta),$ then $\Theta|\{X=k\} \sim Beta(k+\alpha,n-k+\beta)$

\item<2-> \bluef{Fact.} If $\Theta \sim Beta(\alpha,\beta),$
\aleq{
\expect{\Theta} &= \frac{1}{B(\alpha,\beta)} \int_0^1 \theta \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta = \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} = \frac{\alpha}{\alpha+\beta}
}
\item<3-> Using the above fact,
\aleq{
\expect{\Theta| X=k} = \frac{k+\alpha}{k+\alpha + n -k +\beta} = \frac{k+\alpha}{\alpha + \beta + n}
}
\item<4-> For $\alpha=\beta=1$ ($\Theta = Uniform[0,1]$),
\aleq{
\expect{\Theta| X=k} = \frac{k+1}{n+2}
}


% \aleq{
% \fthck&= d \cdot \theta^{\alpha+k-1}(1-\theta)^{\beta + n-k -1}
% }

% \item Using, $\int_0^1 \theta^{\alpha -1}(1-\theta)^{\beta -1} d\theta = \frac{(\alpha-1)! (\beta -1)!}{(\alpha + \beta-1)!}$
% \aleq{
% \expect{\Theta | X=k} &= \int_{0}^1 \theta \fthck d\theta = d\cdot \int_{0}^1
% \theta^{\alpha+k}(1-\theta)^{\beta + n-k -1} d\theta \cr
% &=
% }

% \item MAP estimate
% \aleq{
% \hth_\MAP = \arg \max_{\theta} (k \log \theta + (n-k) \log (1-\theta))
% = \frac{k}{n}
% }

% \item Conditional expectation estimate = $\frac{k+1}{n+2}$ (homework problem)

\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Signal Recovery from Noisy Measurement (1)}


\small
\plitemsep 0.03in
\bci

\item<1-> Unknown: $\Theta \sim Uniform[4,10]$

\item<1-> Observe $\Theta$ with random error $W$ as $X.$ $W \sim Uniform[-1,1]$
$$
X = \Theta + W
$$
\item<2-> Given $\Theta = \theta,$ $X = \theta + W \sim Uniform[\theta-1, \theta+1].$
\aleq{
\onslide<4->{f_{\Theta, X}(\theta,x) = \fth \fxcth =
\begin{cases}
\frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}, & \text{if } 4\le \theta \le 10, \ \theta -1 \le x \le \theta +1, \cr
0, & \text{otherwise}
\end{cases}}
}

\myvartwocols{0.35}{0.4}{0.57}
{
\bigskip
\onslide<5->{- $\hth_\LMS = \expect{\Theta | X=x}$ = midpoint of the corresponding vertical section}
}
{
\centering
\onslide<3->{\mypic{0.9}{L6_lms_ex1.png}}
}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Signal Recovery from Noisy Measurement (2)}


\small
\plitemsep 0.03in
\bci

\item Unknown: $\Theta \sim Uniform[4,10]$

\item Observe $\Theta$ with random error $W$ as $X.$ $W \sim Uniform[-1,1]$
$$
X = \Theta + W
$$
\item Given $\Theta = \theta,$ $X = \theta + W \sim Uniform[\theta-1, \theta+1].$
\aleq{
f_{\Theta, X}(\theta,x) = \fth \fxcth =
\begin{cases}
\frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}, & \text{if } 4\le \theta \le 10, \ \theta -1 \le x \le \theta +1, \cr
0, & \text{otherwise}
\end{cases}
}

\myvartwocols{0.35}{0.4}{0.57}
{
\bigskip
- Conditional MSE
$$\bexpect{(\Theta - \expect{\Theta | X=x})^2 | X=x}$$
}
{
\centering
\mypic{0.65}{L6_lms_ex2.png}
}



\eci

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example 2}

% \small
% \plitemsep 0.03in
% \bci

% \item Unknown: $\Theta \sim Uniform[4,10]$

% \item Observe $\Theta$ with random error $W$ as $X.$ $W \sim Uniform[-1,1]$
% $$
% X = \Theta + W
% $$
% \item Given $\Theta = \theta,$ $X = \theta + W \sim Uniform[\theta-1, \theta+1].$
% \aleq{
% f_{\Theta, X}(\theta,x) = \fth \fxcth =
% \begin{cases}
% \frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}, & \text{if } 4\le \theta \le 10, \ \theta -1 \le x \le \theta +1, \cr
% 0, & \text{otherwise}
% \end{cases}
% }

% \mypic{0.5}{L6_lms_ex1.png}
% \mypic{0.35}{L6_lms_ex2.png}

% $$
% \min_{\hat{\theta}} \bexpect{(\Theta - \hat{\theta})^2 | X=x}
% $$

% \bci
% \item minimized when $\hat{\theta} = \expect{\Theta | X=x}.$
% \item  LMS estimator $\hat{\Theta} = \expect{\Theta | X}$
% \eci

% \item Performance (MSE: Mean Squared Error)
% \bci
% \item When $X=x$, $\bexpect{(\Theta - \expect{\Theta | X=x} )^2 | X=x} = \cbvar{\Theta | X=x}$
% \item Averaged over $X$: $\bexpect{(\Theta - \expect{\Theta | X} )^2} = \bexpect{\cvar{\Theta | X=x}}$
% \eci

% \eci

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hardness of LMS Estimation}

\aleq
{
\fthcx &= \frac{\fth \fxcth}{ \fx}\cr
\fx &= \int f_{\Theta}(\theta') f_{X | \Theta}(x | \theta') d\theta'
}


\plitemsep 0.1in
\bci

\item<2-> Observation model $\fxcth$ may not be always available

\item<3-> Finding the posterior distribution is hard for multi-dimensional $\Theta$

\item<4-> $\Theta$ is very often high-dimensional, especially in the era of big data and deep learning

- {\small AlexNet in image recognition: \bluef{61M} parameters (though not a Bayesian inference)}

\item<5-> Any alternative to LMS estimator?

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bci

\item \bluef{Basics on Statistic Inference}

\item \bluef{Framework of Bayesian Inference}

\item \bluef{MAP (Maximum A Posteriori) Estimator}

\item \bluef{LMS (Least Mean Squares) Estimator}

\item \redf{LLMS (Linear LMS) Estimator}

\medskip
\item Framework of Classical Inference
\item ML (Maximum Likelihood) Estimator

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linear LMS (LLMS) Estimator: Approach}

\plitemsep 0.15in
\bci

\item<2-> Give up optimality, but choose a simple, but good one.

\item<3-> General estimators $\hat{\Theta} = g(X)$, LMS estimator $\hat{\Theta}_{LMS} = \expect{\Theta | X}$

\item<4-> We consider a restricted class of $g(X)$: $\hat{\Theta} =$ \redblank{5}{$aX +b$}.

\item<6-> Our goal is:
$$
\min_{a,b} \bexpect{(\Theta - aX -b)^2}
$$

\item<7-> Linear models are always the first choice for a simple design in engineering.

\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linear LMS (LLMS) Estimator: Solution First}

\onslide<2->{
\myblock{LLMS}
{
\aleq
{
\hat{\Theta}_{L} = \cexpect{\Theta} + \frac{\cov{\Theta,X}}{\cvar{X}}\Bl(X - \cexpect{X}\Bl)
= \cexpect{\Theta} + \rho \frac{\sigma_{\Theta}}{\sigma_{X}}\Bl(X - \cexpect{X}\Bl)
}
\vspace{-0.3cm}
}}
\vspace{-0.5cm}

\small
\plitemsep 0.05in
\bci
\item<3-> No distributions on $\Theta$ and $X$: only means, variances, and covariances

\item<4-> MSE $\expect{(\hat{\Theta}_L - \Theta)^2}$? Assume $\expect{\Theta} = \expect{X}=0.$
\onslide<5->{$
\bexpect{(\Theta - \rho \frac{\sigma_\Theta}{\sigma_X} X)^2} = (1-\rho^2) \var{\Theta}
$

- Uncertainty about $\Theta$ \bluef{decreases} by the factor of \bluef{$1-\rho^2$}}

\onslide<6->{- What happens if $|\rho|=1$ or $\rho=0$?}

\eci

%\medskip
\mytwocols{0.2}
{
\plitemsep 0.05in
\bci
\item<7-> If $\rho >0:$

- Baseline ($\expect{\Theta}$) + correction term

- If $X > \expect{X}$ $\imp$ $\hat{\Theta}_L > \expect{\Theta}$

- If $X < \expect{X}$ $\imp$ $\hat{\Theta}_L < \expect{\Theta}$
\eci
}
{
\plitemsep 0.05in
\bci
\item<8-> If $\rho =0$ (uncorrelated):

- Just baseline ($\expect{\Theta}$)

- $\hat{\Theta}_L = \expect{\Theta}$

- No use of data $X$
\eci
}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linear LMS (LLMS) Estimator: Proof}

\small
\begin{align}
\hat{\Theta}_{L} &= \cexpect{\Theta} + \frac{\cov{\Theta,X}}{\cvar{X}}\Bl(X - \cexpect{X}\Bl)\label{eq:1} \\
&= \cexpect{\Theta} + \rho \frac{\sigma_{\Theta}}{\sigma_{X}}\Bl(X - \cexpect{X}\Bl) \label{eq:2}
\end{align}

\vspace{-0.2cm}
\mytwocols{0.6}
{
\small

\aleq{
\onslide<2->{\min_{a,b} \text{ERR}(a,b) = \min_{a,b} \bexpect{(\Theta - aX -b)^2}}
}

\onslide<3->{- Assume $a$ was found.
\aleq
{
\bexpect{(Y -b)^2}, \quad Y= \Theta - aX
}}

\onslide<4->{- Minimized when $b = \cexpect{Y} = \cexpect{\Theta} - a \cexpect{X}.$}
\begin{align}
\onslide<5->{&\text{ERR}(a,b) = \expect{(Y - \expect{Y})^2} = \cvar{Y} \nonumber \\
&=\var{\Theta} + a^2 \var{X} -2a \cov{\Theta, X} \label{eq:3}}
\end{align}
}
{
\small

\onslide<6->{- \eqref{eq:3} is minimized when $a = \frac{\cov{\Theta,X}}{\var{X}}.$}
\onslide<7->{Then,
\aleq{
\hat{\Theta}_{L} &= aX + b = aX + \cexpect{\Theta} - a \cexpect{X}\cr
& = \text{\eqref{eq:1}}
}}

\onslide<8->{- Using $\rho = \frac{\cov{\Theta,X}}{\sigma_\Theta \sigma_X},$ we get:
\aleq{
a = \frac{\rho \sigma_\Theta \sigma_X}{\sigma_X^2} = \frac{\rho \sigma_\Theta}{\sigma_X}
}
- Then, we have \eqref{eq:2}.}


}




\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet}

\mytwocols{0.8}
{
\small
\plitemsep 0.05in
\bci
\item Romeo and Juliet start dating. Romeo: late by $X \sim U[0,\theta].$
\item Unknown: $\theta$ modeled by a rv $\Theta \sim U[0,1].$

\item<2-> $\expect{X} = \expect{\expect{X|\Theta}}= \expect{\Theta/2} = 1/4$

\item<3->  Using $\expect{\Theta} = 1/2$ and $\expect{\Theta^2} = 1/3,$
\aleq{
\var{X}& = \expect{\var{X|\Theta}} + \var{\expect{X|\Theta}}\cr
&= \frac{1}{12}\expect{\Theta^2} + \frac{1}{4}\var{\Theta}= \frac{7}{144}
}

\item<4-> $\cov{\Theta, X} = \expect{\Theta X} - \expect{\Theta} \expect{X}$
\aleq{
\expect{\Theta X} &= \expect{\expect{\Theta X | \Theta}} = \expect{\Theta \expect{X | \Theta}}\cr
&= \expect{\Theta^2/2} = 1/6
}
$\cov{\Theta, X} = 1/6 - 1/2\cdot 1/4 = 1/24$

\eci
}
{
\small
\plitemsep 0.05in
\bci

\item<5-> LLMS estimator is:
\aleq{
\hat{\Theta}_{L} &= \cexpect{\Theta} + \frac{\cov{\Theta,X}}{\cvar{X}}\Bl(X - \cexpect{X}\Bl) \cr
& = \frac{1}{2} + \frac{1/24}{7/144}(X - \frac{1}{4}) = \frac{6}{7}X + \frac{2}{7}
}
\centering
\onslide<6->{\mypic{0.85}{L6_romeo_juliet_llms.png}}

\eci



}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Biased Coin with Uniform Prior}

\mytwocols{0.8}
{
\small
\plitemsep 0.05in
\bci
\item Biased coin with probability of head $\theta$
\item Unknown $\Theta \sim uniform[0,1],$

- $\expect{\Theta} = 1/2$, $\var{X} = 1/12$
\item $n$ tosses, $X$: number of heads.
\item $p_{X|\Theta}(k|\theta)$: $Binomial(n,\theta)$
\item<2-> $\expect{X} = \expect{\expect{X|\Theta}} = \expect{n\Theta} = n/2$
\aleq{
\onslide<3->{\cvar{X} &= \expect{\cvar{X | \Theta}} + \cvar{\expect{X | \Theta}}\cr
&= \expect{n\Theta(1-\Theta)} + \var{n\Theta} \cr
&= \frac{n}{2} - \frac{n}{3} + \frac{n^2}{12} = \frac{n(n+2)}{12}}
}
\eci
}
{

\small
\onslide<4->{$\cov{\Theta, X} = \expect{\Theta X} - \expect{\Theta} \expect{X} = \expect{\Theta X} - n/4$}
\aleq{
\onslide<5->{\expect{\Theta X} &= \expect{\expect{\Theta X | \Theta}} = \expect{\Theta \expect{X | \Theta}}\cr
&= \expect{n\Theta^2} = n/3}
}
\onslide<6->{$\cov{\Theta, X} = \frac{n}{3} - \frac{n}{4} = \frac{12}{n}$}

\aleq{
\onslide<7->{\hat{\Theta}_L = \frac{1}{2} + \frac{n/12}{n(n+2)/12}(X-\frac{n}{2}) = \frac{X+1}{n+2}}
}

\onslide<8->{- What was the LMS estimator? $\frac{X+1}{n+2}$

- Same! Intuitive?}

\onslide<9->{Yes, because the LMS esitmator was linear.}
}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Example 2: Common Mean of Normal rvs}

% 나중에 normal의 linear model을 할때 다시 정리하자.


% \mytwocols{0.8}
% {
% \small
% \plitemsep 0.05in
% \bci
% \item Observe a collection $X=(X_1, X_2, \ldots, X_n)$
% \item Unknown: common mean $\theta$, modeled by $\Theta \sim N(x_0, \sigma_0^2)$
% \item $X_i$ are normal, independent with known variance $\sigma_1^2, \ldots, \sigma_n^2.$
% \aleq{
% X_i = \Theta + W_i, i=1, \ldots, n,
% }
% where $W_i$ are independent and normal.



% \item Then,
% \eci
% }
% {
% \small
% \plitemsep 0.07in
% \bci

% \item MAP rule

% - Given $x,$ $\fthcx$ is decreasing in $\theta$ over $[x,1].$

% - $\hat{\theta}_{\text{MAP}} = x.$

% \item Conditional expectation estimator
% \vspace{-0.3cm}
% \aleq{
% \hat{\theta}_{\text{LMS}} &= \expect{\theta | X=x} = \int_{x}^1 \theta \frac{1}{\theta |\log x|} d\theta \cr
% & = (1-x)/|\log x|
% }
% \vspace{-0.6cm}
% \centering
% \mypic{0.6}{L6_romeo_juliet.png}
% \eci
% }

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bci

\item \bluef{Basics on Statistic Inference}

\item \bluef{Framework of Bayesian Inference}

\item \bluef{MAP (Maximum A Posteriori) Estimator}

\item \bluef{LMS (Least Mean Squares) Estimator}

\item \bluef{LLMS (Linear LMS) Estimator}

\medskip
\item \redf{Framework of Classical Inference}
\item \redf{ML (Maximum Likelihood) Estimator}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Framework of Classical Inference (1)}

\plitemsep 0.08in

\begin{center}
\mypic{0.5}{L6_classical_framework.png}
\end{center}
\vspace{-0.5cm}
\bci
\item Unknown $\theta$

\bci
\item<2-> \redf{deterministic (not random)} quantity (thus, no prior distribution)
\item<2-> No prior, No posterior probabilities
\eci



\item Observations or measurements $X$

\bci
\item<3-> Random observation $X$'s distribution just depends on $\theta$
\item<4-> Notation: \redf{$p_X(x;\theta)$} and \redf{$f_X(x;\theta)$}, $\theta$-parameterized distribution of observations
\eci

\item<5-> Choosing one among multiple probabilistic models

\bci
\item Each $\theta$ corresponds to a probabilistic model
\eci

\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Framework of Classical Inference (2)}

\plitemsep 0.1in

\bci
\item<2-> Problem types

\bci
\item \redf{Estimation}
\item Hypothesis testing
\item Significance testing
\eci



\item<3-> Key inference methods

\bci
\item \redf{ML (Maximum Likelihood) estimation}
\item Linear regression
\item Likelihood ratio test
\item Significant testing
\eci

\item<4-> Just a taste in this course due to time constraint.
\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Maximum Likelihood Estimation}

\plitemsep 0.1in

\bci
\item<2-> Random observation $x=(x_1, x_2, \ldots, x_n)$ of $X=(X_1, X_2, \ldots, X_n)$

- Assume a scalar $\theta$ and a vector of observation in this lecture.

\item<3-> \redblank{4}{Likelihood $p_X(x_1, x_2, \ldots, x_n;\theta)$}

\bci
\item $p_X(x_1, x_2, \ldots, x_n;\theta)$

- NOT the probability that the unknown parameter is equal to $\theta.$

- but, the probability that \redf{ the observed value $x$ arises when the parameter is $\theta.$}


\item<5-> ML (Maximum Likelihood) estimation
\aleq{
\hat{\theta}_{ml} = \arg \max_{\theta} p_X(x_1, x_2, \ldots, x_n;\theta)
}


\eci

\item<6-> Very often, $X_i$ are independent. Then, ML equals to maximizing the log-likelihood:
\aleq{
\log p_X(x_1, x_2, \ldots, x_n;\theta) = \log \prod_{i=1}^n p_{X_i}(x_i;\theta) = \sum_{i=1}^n \log p_{X_i}(x_i;\theta)
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{ML vs. MAP}

\plitemsep 0.1in

\bci
\item<1-> ML and MAP: How are they related?

\item<2-> MAP in the Bayesian inference
\aleq{
\hat{\theta}_{map} &= \arg \max_{\theta} p_{\Theta | X} (\theta | x)= \arg \max_{\theta} \frac{p_{X| \Theta}(x|\theta) p_\Theta(\theta)}{\px} = \frac{1}{\px}\arg \max_{\theta} p_{X| \Theta}(x|\theta) p_\Theta(\theta)
}

\item<3-> ML in the classical inference
\aleq{
\hat{\theta}_{ml} = \arg \max_{\theta} p_X(x;\theta)
}

\item<4-> $p_{X| \Theta}(x|\theta)$ in the Bayesian setting corresponds to $p_X(x;\theta)$ in the classical setting.

\item<5-> When $\Theta$ is \redf{uniform} (complete ignorance of $\Theta$), MAP == ML

\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Romeo and Juliet}

\plitemsep 0.1in
\bci
\item Romeo and Juliet start dating. Romeo: late by $X \sim U[0,\theta].$
\item Unknown: $\theta$ modeled by a rv $\Theta \sim U[0,1].$

\medskip
\item MAP:  $\hat{\theta}_{\text{MAP}} = x$

\item LMS: $\hat{\theta}_{\text{LMS}} = (1-x)/|\log x|$


\item LLMS: $\hat{\theta}_{\text{L}} = \frac{6}{7}x + \frac{2}{7}$

\item ML: \onslide<2->{\redf{$\hat{\theta}_{\text{ML}} = \hat{\theta}_{\text{MAP}} = x$}}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Estimation of Parameter of Exponential rv}

\plitemsep 0.1in
\bci
\item $n$ identical, independent exponential rvs, $X_1, X_2,\ldots, X_n$ with parameter $\theta.$

\item<2-> Observation $x_1, x_2, \ldots, x_n$

\item<3-> What is the ML estimate of $\theta$?

\item<4-> \redf{Reminder.} $X \sim \exp(\lambda)$
\aleq{
\fx =
 \begin{cases}
 \lambda \elambdax, & x \ge 0 \cr
 0, & x <0
 \end{cases}
\quad \expect{X} = 1/\lambda
}

\item<5-> Any guess? \onslide<6->{$\hat{\theta}_{\text{ML}} = \frac{n}{x_1 + x_2 \ldots x_n}$}
\aleq{
\onslide<7->{\arg \max_{\theta} f_X(x;\theta) = \arg \max_{\theta} \prod_{i=1}^n \theta e^{-\theta x_i} = \arg \max_{\theta} \Bl(n \log \theta - \theta \sum_{i=1}^n x_i \Bl)}
}

\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\vspace{2cm}
\LARGE Questions?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Review Questions}

\bce[1)]
\item What is statistical inference?

\item Draw the building blocks of Bayesian inference and explain how it works.

\item What are MAP and LMS estimators and their underlying philosophies?

\item What is LLMS estimator and why is it useful?

\item Compare the classical and Bayesian inference.

\item What is the ML estimator and how is it related to the MAP estimator?
\ece

\end{frame}

\end{document}
