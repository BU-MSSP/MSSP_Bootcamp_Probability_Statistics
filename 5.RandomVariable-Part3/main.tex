
\input{../mode.tex}
\csname\pdfmode\endcsname

\mode<presentation>
{
  \usetheme{default}
  \usecolortheme{default}
  \usefonttheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]  % or "page number"
  \setbeamercolor{frametitle}{fg=white}
  \setbeamercolor{footline}{fg=black}
} 

\usepackage[english]{babel}
%\usepackage[utf8x]{inputenc}
\usepackage{tikz}
\usepackage{courier}
\usepackage{array}
\usepackage{bold-extra}
% \usepackage{minted}
% \usepackage[thicklines]{cancel}
%\usepackage{fancyvrb}
\usepackage{kotex}
\usepackage{paralist}
\usepackage{collectbox}
\usepackage{booktabs}
%\usepackage[fleqn]{amsmath}



\setbeamercolor{block body alerted}{bg=alerted text.fg!10}
\setbeamercolor{block title alerted}{bg=alerted text.fg!20}
\setbeamercolor{block body}{bg=structure!10}
\setbeamercolor{block title}{bg=structure!20}
\setbeamercolor{block body example}{bg=green!10}
\setbeamercolor{block title example}{bg=green!20}
\setbeamertemplate{blocks}[rounded][shadow]

\xdefinecolor{dianablue}{rgb}{0.18,0.24,0.31}
\xdefinecolor{darkblue}{rgb}{0.1,0.1,0.7}
\xdefinecolor{darkgreen}{rgb}{0,0.5,0}
\xdefinecolor{darkgrey}{rgb}{0.35,0.35,0.35}
\xdefinecolor{darkorange}{rgb}{0.8,0.5,0}
\xdefinecolor{darkred}{rgb}{0.7,0,0}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\newcommand{\progressbar}{
\pgfmathsetmacro{\theta}{360/\inserttotalframenumber*\inserttotalframenumber}
\begin{tikzpicture}[scale=0.035]
\fill[yellow] (0,0) circle (9);
\fill[red] (0,0) -- (9,0) arc (0:-\theta:9);
\fill[white] (0,0) circle (5);
\end{tikzpicture}
}

%\setbeamertemplate{footline}{\hfill \progressbar}

\title[]{Lecture 5: Random Variable, Part III}
\author{Yi, Yung (이융)}
\institute{EE210: Probability and Introductory Random Processes\\ KAIST EE}
\date{MONTH DAY, 2021}

\usetikzlibrary{shapes.callouts}

\input{../mymath}


\begin{document}

\input{../mydefault}

\logo{\pgfputat{\pgfxy(0.11, 7.4)}{\pgfbox[right,base]{\tikz{\filldraw[fill=dianablue, draw=none] (0 cm, 0 cm) rectangle (50 cm, 1 cm);}\mbox{\hspace{-8 cm}\includegraphics[height=0.7 cm]{../kaist_ee.png}
}}}}

\begin{frame}
  \titlepage
\end{frame}

\logo{\pgfputat{\pgfxy(0.11, 7.4)}{\pgfbox[right,base]{\tikz{\filldraw[fill=dianablue, draw=none] (0 cm, 0 cm) rectangle (50 cm, 1 cm);}\mbox{\hspace{-8 cm}\includegraphics[height=0.7 cm]{../kaist_ee.png}
}}}}

% % Uncomment these lines for an automatically generated outline.
% \begin{frame}{Outline}
% % \tableofcontents
% \plitemsep 0.1in
% \bci
% \item 

% \item 
% \eci
% \end{frame}

% START START START START START START START START START START START START START

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.01in

\bci [$\circ$]
{\scriptsize
\item \bluef{Famous discrete random variables used in the community

- Bernoulli, Uniform, Binomial, Geometric, Poisson, etc. }

\item \bluef{Summarizing a random variable: Expectation and Variance}

\item \bluef{Functions of a single random variable}, \bluef{Functions of multiple random variables} 

\item \bluef{Conditioning for random variables}, \bluef{Independence for random variables} 

\item \bluef{Continuous random variables

- Normal, Uniform, Exponential, etc. }

\item \bluef{Bayes' rule for random variables}
}
\bigskip
\item \redf{(Derived) Distribution of $Y=g(X)$ or $Z=g(X,Y)$}

\medskip
\item Quantifying the degree of dependence between two rvs. 

\medskip
\item Conditional expectation/variance

\medskip
\item (Random) Sum of random variables

\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Derived Distribution: $Y=g(X)$}

\plitemsep 0.1in

\bci 
\item<1-> Given the PDF of $X,$ What is the PDF of $Y=g(X)$?

\item<2-> Wait! Didn't we cover this topic? No. We covered just $\expect{g(X)}.$

\item<3-> Examples: $Y=X,$ $Y=X+1,$ $Y=X^2,$ etc.

\item<4-> What are easy or difficult cases?

\item<5-> Easy cases

\bci
\item Discrete

\item Linear: $Y=aX +b$
\eci
\eci 


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Discrete Case}

\plitemsep 0.1in

\bci 
\item<1-> Take all values of $x$ such that $g(x) = y,$ i.e., 
\aleq{
\py &= \cprob{g(X) = y} \cr
&= \sum_{x:g(x)=y} \px
}

\item[] 
\mytwocols{0.4}
{
\aleq{
\onslide<3->{
p_Y(3) &= p_X(2) + p_X(3) = 0.1 + 0.2 = 0.3\cr
p_Y(4) &= p_X(4) + p_X(5) = 0.3+0.4 = 0.7
}
}
}
{
\centering
\onslide<2->{\mypic{0.8}{L5_derived_disc.png}}
}
\eci 


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linear: $Y = aX +b,$ $a \neq 0$}

\plitemsep 0.1in

\bci []
\item 
\aleq{
\onslide<1->{\text{If $a > 0,$}}& 
\onslide<2->{\quad \Fy = \cprob{aX+b \leq y} = \cprob{X \le \frac{y-b}{a}} = F_X(\frac{y-b}{a})}\cr
& \onslide<3->{\rightarrow \fy = \frac{1}{a} f_X \left (\frac{y-b}{a} \right )}\cr
\onslide<1->{\text{If $a <0,$}}& 
\onslide<4->{\quad \Fy = \cprob{aX+b \leq y} = \cprob{X > \frac{y-b}{a}} 
 = 1- F_X(\frac{y-b}{a})}\cr
& \onslide<5->{\rightarrow \fy = -\frac{1}{a} f_X \left (\frac{y-b}{a} \right )}\cr
}
\vspace{-0.5in}
\item<6-> Therefore,
$
\fy = \frac{1}{|a|} f_X(\frac{y-b}{a})
$

\item<7-> \redf{Special case.} $X$ is normal. Then, $Y$ is also normal, i.e., $Y\sim N(a\mu+b, a^2 \sigma^2)$  
\eci 


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Generally, $Y=g(X)$}

\mytwocols{0.75}
{

\medskip
\plitemsep 0.2in

\bce
\item<2->[Step 1.] Find the CDF of $Y$: $\Fy = \cprob{Y\leq y} = \cprob{g(X) \leq y}$
\item<2->[Step 2.] Differentiate: $\fy = \frac{dF_Y}{dy}(y)$

\item<3->[**] When $Y=g(X)$ is monotonic, a \redf{general formula} can be drawn (see the textbook at pp 207)
\ece

\bigskip

\small
\onslide<4->{\redf{Ex1.} $X \sim uniform[0,1].$ $Y = \sqrt{X}.$
\aleq{
\Fy &= \cprob{\sqrt{X} \le y} = \cprob{X \le y^2} = y^2 \cr
\fy &= 2y, \quad 0\le y \le 1
}
}

}
{
\small

\onslide<5->{
\redf{Ex2.} $X \sim uniform[0,2].$ $Y = X^3.$
\aleq{
\Fy &= \cprob{X^3 \le y} = \cprob{X \le \sqrt[3]{y}} = \frac{1}{2} y^{1/3}\cr
\fy &= \frac{1}{6}y^{-2/3}, \quad 0\le y \le 8
}
}

%\smallskip
\onslide<6->{\redf{Ex3.} $X$ with $\fx.$ $Y = X^2.$
\aleq{
\Fy &= \cprob{X^2 \le y} = \cprob{-\sqrt{y} \le X \le \sqrt{y}} \cr
& = F_X(\sqrt{y}) - F_X(-\sqrt{y})\cr
\fy &= \frac{1}{2\sqrt{y}} f_X(\sqrt{y}) +\cr
& \frac{1}{2\sqrt{y}} f_X(-\sqrt{y}), \quad y \ge 0
}
}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Functions of multiple rvs: $Y=g(X,Y)$ (1)}

\onslide<1->{\noindent Basically, follow two step approach: (i) CDF and (ii) differentiate. }

\medskip

\small
\onslide<2->{\redf{Ex1.} $X,Y \sim uniform[0,1],$ and $X \indep Y.$ $Z = \max(X,Y).$

\medskip
* $\cprob{X \le z} = \cprob{Y \le z} = z,\ z \in [0,1].$
}

\aleq{
\onslide<3->{
F_Z(z) &= \cprob{\max(X,Y) \le z} = \cprob{X \le z, Y \le z}\cr
&= \cprob{X \le z} \cprob{Y \le z} = z^2}\cr
\onslide<4->{
\fz& = \begin{cases}
2z, & \text{if $0 \le z \le 1$}\cr
0, & \text{otherwise}
\end{cases}
}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Functions of multiple rvs: $Y=g(X,Y)$ (2)}

\noindent Basically, follows two step approach: (i) CDF and (ii) differentiate. 

\medskip

\mytwocols{0.6}
{
\small

\onslide<2->{
\redf{Ex2.} 
$X,Y \sim uniform[0,1],$ and $X \indep Y.$
$Z = Y/X.$}
\aleq{
\onslide<3->{\Fz &= \cprob{Y/X \le z}} \cr
\onslide<5->{ &= 
 \begin{cases}
 z/2, & 0 \le z \le 1\cr
 1-1/2z, & z >1 \cr
 0,& \text{otherwise}
 \end{cases}
 }
}
\aleq{
\onslide<6->{
\fz &= 
 \begin{cases}
 1/2, & 0 \le z \le 1\cr
 1/(2z^2), & z >1 \cr
 0,& \text{otherwise}
 \end{cases}
}
}
}
{

\small

\onslide<4->{
- Depending on the value of $z,$ two cases need to be considered separately. 

\medskip
\centering
\mypic{0.95}{L5_pdf_tworvs.png}
}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Functions of multiple rvs: $Z=X+Y,$ $X\indep Y$}

\mytwocols{0.8}
{
\onslide<2->{
- A very basic case with many applications

- Assume that $X,Y \in \integer$
}
\small
\aleq{
\onslide<3->{
\pz &= \cprob{X+Y = z} \cr 
 &= \sum_{\{(x,y): x+y=z \}}\cprob{X=x,Y=y}\cr
 &= \sum_{x}\cprob{X=x,Y=z-x}\cr
 &= \sum_{x}\cprob{X=x} \cprob{Y=z-x}\cr
 &= \sum_{x}\px p_Y(z-x)
}
}

\onslide<5->{
- $\pz$ is called \redblank{6}{convolution} of the PMFs of $X$ and $Y.$
}
}
{
\small

\medskip
\onslide<4->{
- Interpretation (for a given $z$)

(i) Flip (horizontally) $\py$ ($p_{Y}(-x)$) 

(ii) Put it underneath $\px$ ($p_{Y}(-x+z)$) 

\bigskip
\centering
\mypic{0.95}{L5_conv_ex.png}
}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{$Y=X+Y,$ $X\indep Y$: Continuous}

\mytwocols{0.8}
{

\plitemsep 0.1in
\bci 

\item<1-> Same logic as the discrete case

\aleq{
\fz = \int_{-\infty}^\infty \fx f_{Y}(z-x) dx
}

\item<2-> Very special, but useful case

\medskip

- $X$ and $Y$ are \bluef{normal.}
\eci
}
{
\onslide<3->{
\myblock{Sum of two independent normal rvs}
{
$X \sim N(\mu_x, \sigma_x^2)$ and  $Y \sim N(\mu_x, \sigma_x^2)$


Then, $X+Y \sim N(\mu_x + \mu_y, \sigma_x^2 + \sigma_y^2)$
}
}
\bigskip
\plitemsep 0.1in
\bci
\item<4->  Why normal rvs are used to model the sum of random noises. 

\item<4-> (Extension) The sum of \redf{finitely many} independent normals is also normal. 
\eci
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.01in

\bci [$\circ$]

{\scriptsize
\item \bluef{Famous discrete random variables used in the community

- Bernoulli, Uniform, Binomial, Geometric, Poisson, etc. }

\item \bluef{Summarizing a random variable: Expectation and Variance}

\item \bluef{Functions of a single random variable}, \bluef{Functions of multiple random variables} 

\item \bluef{Conditioning for random variables}, \bluef{Independence for random variables} 

\item \bluef{Continuous random variables

- Normal, Uniform, Exponential, etc. }

\item \bluef{Bayes' rule for random variables}
}
\bigskip
\item \bluef{(Derived) Distribution of $Y=g(X)$ or $Z=g(X,Y)$}

\medskip
\item \redf{Quantifying the degree of dependence between two rvs.} 

\medskip
\item Conditional expectation/variance

\medskip
\item (Random) Sum of random variables

\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Making a Metric of Dependence Degree}

\plitemsep 0.05in

\bci 

\item<1-> Goal: Given two rvs $X$ and $Y$, assign some number that quantifies the degree of their dependence

\item<2-> Reqs. 
\bce[a)]
\item<3-> Increases (resp. decreases) as they become more (resp. less) dependent.
\item<4-> 0 when they are independent.
\item<5-> Shows the direction of dependence by + and -
\item<6-> Always bounded by some numbers, e.g., $[-1,1]$
\ece

\item<7-> Good engineers: Good at making good metrics

\smallskip
\bci
\item Metric of how our society is economically polarized
\item A lot of metrics in our professional sports leagues (baseball, basketball, etc)
\item Cybermetrics in MLB (Major League Baseball): \url{http://m.mlb.com/glossary/advanced-stats}
\eci

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{OK. Let's Design!}

\plitemsep 0.1in

\bci 

\item<1-> Simple case: $\expect{X} =\mu_x = 0$ and $\expect{Y}=\mu_Y = 0$
\item<2-> Dependent: Positive (If $X \uparrow$, $Y \uparrow$) or Negative (If $X \uparrow,$ $Y \downarrow$)

\item<3-> What about $\expect{XY}$? Seems good. 

\bci
\item<4-> $\expect{XY} = \expect{X}\expect{Y}=0$ when $X \indep Y$
\item<5-> More data points (thus increases) when $xy >0$ (both positive or negative)
\eci

\mytwocols{0.4}
{
\centering
\onslide<5->{\mypic{0.8}{L5_cov_ex.png}}
}
{
\bigskip
\onslide<6->{\redf{(Q)} What about $\expect{X+Y}$?}
}

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What If $\mu_X \neq 0, \mu_Y \neq 0$?}

\plitemsep 0.1in

\bci 

\item<2-> Solution: Centering. $X \rightarrow X - \mu_X$ and $Y \rightarrow Y-\mu_Y$
\onslide<3->{
\myblock{Covariance}
{
$\cov{X,Y} = \bexpect{(X- \expect{X})\cdot (Y-\expect{Y})}$
}}

\item<4-> After some algebra, $\cov{X,Y} = \expect{XY} - \expect{X}\expect{Y}$

\item<5-> $X \indep Y$ $\imp$ $\cov{X,Y}=0$

\item<6-> $\cov{X,Y}=0$ $\imp$ $X \indep Y$? NO.

\item<7-> When $\cov{X,Y}=0,$ we say that $X$ and $Y$ are \redblank{8}{uncorrelated.}

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: $\cov{X,Y}=0,$ but not independent}

\plitemsep 0.1in

\bci 

\item $p_{X,Y}(1,0) = p_{X,Y}(0,1) = p_{X,Y}(-1,0) = p_{X,Y}(0,-1) = 1/4.$

\item<2-> $\expect{X} = \expect{Y}=0,$ and $\expect{XY}=0.$ So, $\cov{X,Y}=0$

\item<3-> Are they independent? No, because if $X=1$, then we should have $Y=0.$

\eci
\centering
\mypic{0.3}{L5_cov_notind.png}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Some Properties}

\plitemsep 0.07in

\vspace{-0.3in}
\bci []

\item<2-> \aleq{
\cov{X,X}=0
}

\item<3-> \aleq{
\cov{aX +b, Y} &= \expect{(aX+b)Y} - \expect{aX+b}\expect{Y} = a\cdot \cov{X,Y}
}

\item<4->  \aleq{
\cov{X,Y+Z} &= \expect{X(Y+Z)} - \expect{X}\expect{Y+Z}= \cov{X,Y} + \cov{X,Z}
}

\item<5-> \aleq{
\var{X+Y} &= \expect{(X+Y)^2} - (\expect{X+Y})^2 = \var{X} + \var{Y} - 2\cov{X,Y}
}
\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: The hat problem in Lecture 3. Remember?}

\bigskip

\mytwocols{0.7}
{
\small
\plitemsep 0.05in
\bci 

\item $n$ people throw their hats in a box and then pick one at random

\item $X$: number of people with their own hat

\item \redf{(Q)} $\var{X}$

\item \bluef{Key step 1.} Define a rv $X_i=1$ if $i$ selects own hat and $0$ otherwise. Then, $X = \sum_{i=1}^n X_i.$

\item \bluef{Key step 2.} Are $X_i$s are independent? 

\medskip
\item<2-> $X_i \sim Bernoulli(1/n).$ Thus, $\expect{X_i} = 1/n$ and $\var{X_i} = \frac{1}{n}(1-\frac{1}{n})$

\eci
}
{
\small
%\sqeq
\plitemsep 0.01in
\bci [$\circ$]
\item \onslide<4->{For $i \neq j,$}
\setlength{\jot}{0pt}
\aleq{
\onslide<4->{\cov{X_i,X_j} &= \expect{X_i X_j} - \expect{X_i} \expect{X_j}\cr
&=\cprob{X_i =1 \text{ and } X_j=1} - \frac{1}{n^2} \cr
&=\cprob{X_i=1} \cprob{X_j=1 | X_i=1} - \frac{1}{n^2} \cr
&=\frac{1}{n} \frac{1}{n-1} - \frac{1}{n^2} = \frac{1}{n^2(n-1)}}
}
\vspace{-0.2in}
\aleq{
\onslide<3->{\var{X}& = \bvar{\sum X_i}\cr
&= \sum {\var{X_i}} + \sum_{i\neq j} \cov{X_i,X_j}}\cr
\onslide<5->{&= n \frac{1}{n}(1-\frac{1}{n}) + n(n-1)\frac{1}{n^2(n-1)}=1}\cr
}
\vspace{-0.1in}

\eci

}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Bounding the metric: Correlation Coefficient}

\plitemsep 0.05in

\bci 

\item<1-> Reqs. a), b), and c) satisfied. 

\bce
\item<2->[d)] \redf{Always bounded by some numbers, e.g., $[-1,1]$}
\ece

\item<3-> Dimensionless metric. How? \redblank{4}{Normalization,} but by what?

\onslide<5->{
\myblock{Correlation Coefficient}
{
\aleq{
\rho(X,Y) = \bbexpect{\frac{(X - \mu_X)}{\redf{\sigma_X}} \cdot \frac{Y-\mu_Y}{\redf{\sigma_Y}}} = \frac{\cov{X,Y}}{\sqrt{\var{X}\var{Y}}}
}
}
}
\item<6-> $-1 \le \rho \le 1$

\item<7-> $|\rho|=1$ $\imp$ $X-\mu_X = c(Y-\mu_Y)$ (linear relation, VERY related)

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.01in

\bci [$\circ$]

{\scriptsize
\item \bluef{Famous discrete random variables used in the community

- Bernoulli, Uniform, Binomial, Geometric, Poisson, etc. }

\item \bluef{Summarizing a random variable: Expectation and Variance}

\item \bluef{Functions of a single random variable}, \bluef{Functions of multiple random variables} 

\item \bluef{Conditioning for random variables}, \bluef{Independence for random variables} 

\item \bluef{Continuous random variables

- Normal, Uniform, Exponential, etc. }

\item \bluef{Bayes' rule for random variables}
}
\bigskip
\item \bluef{(Derived) Distribution of $Y=g(X)$ or $Z=g(X,Y)$}

\medskip
\item \bluef{Quantifying the degree of dependence between two rvs.} 

\medskip
\item \redf{Conditional expectation/variance}

\medskip
\item (Random) Sum of random variables

\eci 

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{A Special Random Variable}

\mytwocols{0.6}
{
\plitemsep 0.1in

\abovedisplayskip=5pt
\belowdisplayskip=5pt

\bci 
\item<1-> Consider a rv $Y$, such that  
\aleq{
Y &= \begin{cases}
0,& \text{w.p.  } 1/4 \cr
1,& \text{w.p.  } 1/4 \cr
2,& \text{w.p.  } 1/2 
\end{cases}
}

\item<2-> If $h(y) = y^2,$ then a new rv $h(Y)$ is:
\aleq{
h(Y) &= \begin{cases}
0,& \text{w.p.  } 1/4 \cr
1,& \text{w.p.  } 1/4 \cr
4,& \text{w.p.  } 1/2 
\end{cases}
}
\eci 
}
{
\plitemsep 0.05in
\abovedisplayskip=5pt
\belowdisplayskip=5pt

\bci 
\item<3-> Consider other rv $X$, such that  
\aleq{
\onslide<4->{\redf{g(y)}= } \expect{X|Y=y}=\begin{cases}
3,& \text{if $y=0$}\cr
8,& \text{if $y=1$}\cr
9,& \text{if $y=2$}
\end{cases}
}

\item<5-> Then, a rv $\redf{g(Y)}$ is:
\aleq{
g(Y) &= \begin{cases}
3,& \text{w.p.  } 1/4 \cr
8,& \text{w.p.  } 1/4 \cr
9,& \text{w.p.  } 1/2 
\end{cases}
}
\eci
}

\onslide<6->{- The rv $g(Y)$ looks special, so let's notate it with some fancy one.} 

\onslide<7->{- What about? $X_{exp}(Y)$, $\expect{X_Y}$, $\expecti{X}{Y}$?}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conditional Expectation $\expect{X|Y}$}

\plitemsep 0.1in

% \abovedisplayskip=5pt
% \belowdisplayskip=5pt

\myblock{Conditional Expectation}{
A random variable $g(Y) =$ \redblank{2}{$\expect{X|Y}$}, called \redblank{2}{conditional expectation of $X$ given $Y$}, takes the value $g(y) = \expect{X|Y=y},$ if $Y$ happens to take the value $y.$
}

\bci 
\item<2-> A function of $Y$
\item<3-> A random variable
\item<4-> Thus, having a distribution, expectation, variance, all the things that a 
random variable has
\item<5-> Often confusing because of the notation
\eci 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expectation of $\expect{X|Y}$}

\plitemsep 0.1in

% \abovedisplayskip=5pt
% \belowdisplayskip=5pt

\myblock{Expectation of Conditional Expectation}{
\aleq
{
\bexpect{\expect{X|Y}} = \expect{X}, \quad \text{Law of iterated expectations}
}
}

\redf{Proof.} 
\aleq
{
\bexpect{\expect{X|Y}} &= \sum_{y} \expect{X | Y=y} \py \cr
&= \expect{X}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Examples and Meaning}

\mytwocols{0.6}
{
\plitemsep 0.1in

% \abovedisplayskip=5pt
% \belowdisplayskip=5pt

\bci 
\item<2-> Stick of length $l$
\item<2-> Uniformly break at point $Y,$ and break what is left uniformly at point $X.$

\item<3-> $\expect{X | Y=y} = y/2$ 

\item<3-> $\expect{X | Y} = Y/2$ 

\item<4-> $\expect{X} = \expect{\expect{X|Y}} = \expect{Y/2}=\frac{1}{2}\frac{l}{2} = l/4$
\eci
}
{
\plitemsep 0.1in

% \abovedisplayskip=5pt
% \belowdisplayskip=5pt

\bci 
\item<5-> Forecasts on sales: calculating expected value, given any available information

\item<6-> $X:$ February sales

\item<6-> Forecast in the beg. of the year: $\expect{X}$

\item<7-> End of Jan. new information $Y=y$ (Jan. sales)

Revised forecast: $\expect{X|Y=y}$

Revised forecast $\neq$ $\expect{X}$

\item<8-> Law of iterated expectations
$\expect{\text{revised forecast}} = \text{original one}$

\eci

}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conditional Variance $\var{X|Y}$}

\plitemsep 0.1in

% \abovedisplayskip=5pt
% \belowdisplayskip=5pt
\onslide<1->{$\var{X} = \expect{(X-\expect{X})^2}$}

\onslide<2->{$\var{X\redf{|Y=y}} = \expect{(X-\expect{X\redf{|Y=y}})^2 | \redf{Y=y}}$}

\onslide<3->{
\myblock{Conditional Variance}{
A random variable $g(Y) =$ \redblank{4}{$\var{X|Y}$} and called \redblank{4}{conditional variance of $X$ given $Y$}, takes the value $g(y) = \var{X|Y=y},$ if $Y$ happens to take the value $y.$
}}

\bci 
\item<5-> A function of $Y$
\item<5-> A random variable
\item<5-> Thus, having a distribution, expectation, variance, all the things that a 
random variable has
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expectation and Variance of $\expect{X|Y}$ and $\var{X|Y}$}

\Large

\centering

\renewcommand{\arraystretch}{1.7}
\setlength{\tabcolsep}{12pt}
\begin{tabular}{@{}l|l|l@{}} \toprule
 & $\expect{X|Y}$ & $\var{X|Y}$ \\ \midrule \midrule
Expectation& $\bexpect{\cexpect{X|Y}}$& $\bexpect{\cvar{X|Y}}$ \\  \midrule
Variance& $\bvar{\cexpect{X|Y}}$& $\bvar{\cvar{X|Y}}$ \\ \bottomrule
\end{tabular}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Law of Total Variance}

\myblock{Law of total variance}{
$$
\var{X} = \onslide<2->{\bexpect{\cvar{X|Y}} + \var{\cexpect{X|Y}}}
$$
}

\redf{Proof.} 
\begin{align}
\onslide<3->{\cvar{X|Y} &= \expect{X^2 |Y} - (\expect{X|Y})^2} \nonumber \\
\onslide<4->{\bexpect{\cvar{X|Y}} &= \expect{X^2} - \bexpect{(\expect{X|Y})^2} \label{eq:1}}\\
\onslide<5->{\bvar{\cexpect{X|Y}} &= \bexpect{(\expect{X|Y})^2} - (\bexpect{\cexpect{X|Y}})^2 = \bexpect{(\expect{X|Y})^2} - (\expect{X})^2 \label{eq:2}}
\end{align}

\onslide<6->{\eqref{eq:1} + \eqref{eq:2} = $\expect{X^2}$ + $(\expect{X})^2$ = $\var{X}$}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sum of a random number of rvs}

\plitemsep 0.1in

\bci 
\item<1-> $N:$ number of stores visited (\redf{random})

\item<2-> $X_i$: money spent in store $i,$ independent of other $X_j$ and $N,$ $X_i$s are identically distributed with $\expect{X_i} = \mu$

\item<3-> $Y = X_1 + X_2 + \ldots X_N.$ What are $\expect{Y}$ and $\var{Y}$?

\item<4-> $\expect{Y} = \expect{\expect{Y|N}} = \expect{N\expect{X_i}} = \expect{N}\expect{X_i} = \mu \expect{N}$

\item<5-> $\var{Y} = \bexpect{\cvar{Y|N}} + \var{\cexpect{Y|N}}$  \onslide<9->{$=\expect{N}\var{X_i} - \mu^2 \var{N}$}
\aleq{
\onslide<6->{\cvar{\expect{Y|N}} &= \cvar{N\mu} = \mu^2 \var{N}} \cr
\onslide<7->{\var{Y|N} &= N \var{X_i}}\cr
\onslide<8->{\expect{\cvar{Y|N}} &= \expect{N \var{X_i}} = \expect{N}\var{X_i}}
}
% \mytwocols{0.4}
% { 

% }
% { 
% }

\eci


\end{frame}

\begin{frame}{}
\vspace{2cm}
\LARGE Questions?

\end{frame}

\begin{frame}{Review Questions}

\bce[1)]
\item What are the key steps to get the derived distributions of $Y=g(X)$ or $Z=g(X,Y)$?

\item How can we compute the distribution of $Z+X+Y$ when $X$ and $Y$ are independent?

\item What are covariance and correlation coefficient? Why do we need them?

\item Please explain the concepts of conditional expectation and conditional variance.

\ece

\end{frame}

\end{document}
