
\input{../mode.tex}
\csname\pdfmode\endcsname

\mode<presentation>
{
  \usetheme{default}
  \usecolortheme{default}
  \usefonttheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]  % or "page number"
  \setbeamercolor{frametitle}{fg=white}
  \setbeamercolor{footline}{fg=black}
}


\input{../myhead.tex}

\title[]{Lecture 6: Law of Large Numbers and Central Limit Theorem}
\author{Yi, Yung (이융)}
\institute{EE210: Probability and Introductory Random Processes\\ KAIST EE}
\date{\today}



\input{../mymath}

\begin{document}




\input{../mydefault}


\begin{frame}
  \titlepage
\end{frame}


% % Uncomment these lines for an automatically generated outline.
% \begin{frame}{Outline}
% % \tableofcontents
% \plitemsep 0.1in
% \bci
% \item

% \item
% \eci
% \end{frame}

% START START START START START START START START START START START START START

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item Weak Law of Large Numbers: Result and Meaning
\item Central Limit Theorem: Result and Meaning
\item Weak Law of Large Numbers: Proof

- Inequalities: Markov and Chebyshev

\item Central Limit Theorem: Proof

- Moment Generating Function (MGF)

%\item Strong Law of Large Numbers

\medskip

\item[$\circ$] Two most remarkable findings in probability theory
\ece

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L7(1)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \redf{Weak Law of Large Numbers: Result and Meaning}
\item \grayf{Central Limit Theorem: Result and Meaning
\item Weak Law of Large Numbers: Proof

- Inequalities: Markov and Chebyshev

\item Central Limit Theorem: Proof

- Moment Generating Function (MGF)

%\item Strong Law of Large Numbers

}

\ece

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Our interest: Sum of Random Variables}

\plitemsep 0.1in

\bci

\item<1-> \bluef{Example 1.} $n$ students who decides their presence, depending on their feeling. Each student is happy or sad at random, and only happy students will show their presence. How many students will show their presence?

\item<2-> \bluef{Example 2.} I am hearing some sound. There are $n$ noisy sources from outside.

\bigskip

\item<3-> $X_1, X_2, \ldots, X_n$: i.i.d (independent and identically distributed) random variables

\item<4-> $\expect{X_i} = \mu,$ $\var{X_i} = \sigma^2$

\item<5-> Our interest is to understand how the following sum behaves:
\mycolorbox{
\centering
$S_n = X_1 + X_2 + \ldots + X_n$
}
% \bci
% \item Challenging if we intend to approach directly. For $Z=X+Y,$ finding the distribution requires the complex \redf{convolution}.
% \aleq{
% \pz = \cprob{X+Y = z} = \sum_{x}\px p_Y(z-x)
% }
% \eci
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What About Distribution of $S_n$?}
\mycolorbox{
\centering
$S_n = X_1 + X_2 + \ldots + X_n$
}
%\vspace{-0.8cm}
\plitemsep 0.1in
\bci
\item<2-> Figure out the distribution of $S_n$. Very challenging. Even just for $Z=X+Y,$ finding the distribution, for example, requires the complex \redf{convolution}.
\aleq{
\pz = \cprob{X+Y = z} = \sum_{x}\px p_Y(z-x)
}

\item<3-> Easy case: Sum of normal rvs = a normal rv, however, generally very challenging.

\item<4-> \redf{Possible apporach.} Take a certain \bluef{scaling} with respect to $n$ that corresponds to a \bluef{new glass}, and investigate the system for large $n$
% (i.e., $n \rightarrow \infty$ mathematically).


\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sample Mean}

\plitemsep 0.1in

\bci
\item<1-> Consider the \orangef{sample mean}, and try to understand how $S_n$ behaves:

\mycolorbox{
\centering
$
\displaystyle M_n = \frac{S_n}{n} = \frac{X_1 + X_2+ \ldots X_n}{n}
$
}


% \item<2-> \redf{Example.} $n$ coin tossing. $X_i=1$ if head, and 0 otherwise. $S_n$: total number of heads.

\item<2-> $\cexpect{M_n} = \mu,$ $\cvar{M_n} = \sigma^2/n$

\item<3-> For large $n$, the variance $\cvar{M_n}$ decays. We expect that, for large $n,$ $M_n$ looses its randomness and \orangef{concentrates around $\mu.$}

\item<4-> We call this \redf{law of large numbers (LLN)}.
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Let's Establish Mathematically}
\mycolorbox{
\centering
$\displaystyle M_n = \frac{S_n}{n} = \frac{X_1 + X_2+ \ldots X_n}{n}
$
}

\plitemsep 0.04in
\bci
\item<2-> What about this? What's wrong?
$$
M_n \xrightarrow{n \rightarrow \infty} \mu
$$

\item<3-> \bluef{Ordinary convergence} for the sequence of real numbers: $a_n \rightarrow L$

\onslide<4->{
\mycolorbox{
 For every $\epsilon >0,$ there exists $N = N(\epsilon),$ such that for every $n \geq N,$ $|a_n-L| \le \epsilon.$}
 }

\onslide<5->{\url{https://www.youtube.com/watch?v=4nBmsRA6eVw}}

\item<6-> However, $M_n$ is a random variable, which is a function from $\Omega$ to $\real.$

\item<7-> Need to build up the \redf{new} concept of convergence for the sequence of rvs.

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Convergence in Probability (1)}

\plitemsep 0.05in

\bci

\item<1-> What we want: \bluef{a sequence of rvs $(Y_n)_{n=1, 2, \ldots}$ converges to a rv $Y$} in some sense

\item<2-> For any given $\epsilon >0,$ consider the sequence of events $A_n = \{|Y_n -Y| \geq \epsilon \},$ and compute its sequence of probabilities $a_n = \cprob{A_n} = \cprob{|Y_n -Y| \geq \epsilon}.$

\item<3-> Now, $\{a_n\}$ are just the real numbers, and show that $a_n \rightarrow 0$ as $n \rightarrow \infty.$

\item<4-> To show that $a_n \rightarrow 0$ as $n \rightarrow \infty,$ which is just the ordinary convergence, we show:
\bci
\item<5-> For any $\delta >0,$ there exists $N = N(\delta),$ such that for all $n \geq N,$ $|a_n - 0 | \leq \delta$

\eci

\item<6-> \orangef{Convergence in probability}: $Y_n \xrightarrow{\text{in prob.}} Y$
\mycolorbox{
\bci
\item<6-> For any $\epsilon >0$ and for any $\delta >0,$ there exists $N = N(\delta),$ such that for all $n \geq N,$ $\cprob{|Y_n-Y| \geq \epsilon } \leq \delta.$
\item<7-> For any $\epsilon >0,$ $\cbprob{\{|Y_n -Y| \geq \epsilon\}} \xrightarrow{n \rightarrow \infty} 0.$
\eci
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Convergence in Probability (2)}

\mycolorbox{
\bci
\item For any $\epsilon >0,$ $\cbprob{\{|Y_n -\redf{Y}| \geq \epsilon\}} \xrightarrow{n \rightarrow \infty} 0.$

\item For any $\epsilon >0,$ $\cbprob{\{|Y_n -\redf{a}| \geq \epsilon\}} \xrightarrow{n \rightarrow \infty} 0.$

\eci
}

\plitemsep 0.1in

\bci

\item A special case: when $Y=a$ for some constant $a$: $Y_n \xrightarrow{\text{in prob.}} a$

\item \url{https://youtu.be/Ajar_6MAOLw?t=248}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 1: Convergence in Probability}

\mycolorbox{
\bci
\item For any $\epsilon >0,$ $\cbprob{\{|Y_n -\redf{a}| \geq \epsilon\}} \xrightarrow{n \rightarrow \infty} 0.$
\eci
}

\plitemsep 0.1in

\bci

\item<2-> A sequence of iid rvs $X_n \sim \set{U}[0,1]$, and let
$$
Y_n = \min\{X_1, X_2, \ldots, X_n \}
$$

\item<3-> Our intuition: $Y_n$ converges to 0, as $n \rightarrow 0.$ Why?

\item<4-> \proff For any $\epsilon >0,$
\aleq
{
\cprob{|Y_n - 0| \geq \epsilon} &= \cprob{X_1 \geq \epsilon, \ldots, X_n \geq \epsilon}
= \cprob{X_1 \geq \epsilon} \times \cdots \times \cprob{X_n \geq \epsilon} \cr
& = (1-\epsilon)^n \xrightarrow{n \rightarrow \infty} 0
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 2: Convergence in Probability}

\mycolorbox{
\bci
\item For any $\epsilon >0,$ $\cbprob{\{|Y_n -\redf{a}| \geq \epsilon\}} \xrightarrow{n \rightarrow \infty} 0.$
\eci
}

\plitemsep 0.1in

\bci

\item<2-> $Y$: exponential rv with the parameter $\lambda =1$ (Remind: $\cprob{Y > y } = e^{-\lambda y}$)

\item<2-> a sequence of rvs $Y_n = Y/n$ (note that these are dependent)

\item<3-> Our intuition: $Y_n$ converges to 0

\item<4-> \proff For any $\epsilon >0,$
\aleq
{
\cprob{|Y_n - 0| \geq \epsilon} &= \cprob{Y \geq n\epsilon} = e^{-n\epsilon} \xrightarrow{n \rightarrow \infty} 0
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 3: Convergence in Probability}

\mycolorbox{
\bci
\item For any $\epsilon >0,$ $\cbprob{\{|Y_n -\redf{a}| \geq \epsilon\}} \xrightarrow{n \rightarrow \infty} 0.$
\eci
}

\plitemsep 0.1in

\bci

\item<2-> Consider a sequence of rvs $Y_n$ with the following distribution:
\aleq
{
\cprob{Y_n =y} = \begin{cases}
1 - \frac{1}{n}, & \text{for $y=0$} \cr
\frac{1}{n}, & \text{for $y = n^2$} \cr
0, & \text{otherwise}
\end{cases}
}

\item<3-> For any $\epsilon >0$,
\aleq{
\cprob{|Y_n| \geq \epsilon} = \frac{1}{n} \xrightarrow{n \rightarrow \infty} 0
}

\item<4-> Thus, $Y_n$ converges to 0 in probability.
\eci
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Weak Law of Large Numbers (WLLN)}
\mycolorbox{
\centering
$
\displaystyle M_n = \frac{S_n}{n} = \frac{X_1 + X_2+ \ldots X_n}{n}
$
}
 \plitemsep 0.1in
 \bci
\item Roughly, $M_n$ concetrates around $\mu$

\onslide<2->{
\myblock{Weak law of large numbers}
{
$M_n$ converges to $\mu$ in probability, i.e., $M_n \xrightarrow{\text{in prob.}} \mu$
}}


 \item<3-> Why ``Weak"? There exists a stronger version, which we call ``strong" law of large numbers. We will not cover the strong law of large numbers in this class.

\item<4-> The proof requires some knowledge about useful inequalities, which we will cover later.
 \eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{WLLN: Why Useful?}
\mycolorbox{
\centering
$
\displaystyle M_n = \frac{S_n}{n} = \frac{X_1 + X_2+ \ldots X_n}{n}
$
}
 \plitemsep 0.1in
 \bci
\item<2-> If we take the scaling of $S_n$ by $1/n,$ it behaves like a \redf{deterministic} number. This significantly simplifies how we understand the world.

\item<3-> For example, assume that a large number of identically distributed noises come to you. Then, you can roughly approximate it as \redf{$(n \times \text{average noise)}$}

\item<4-> Provides an interpretation of expectations (as well as probabilities) in terms of a long sequence of identical independent experiments. For example, what is the probability of head of a coin? Toss 1000 times, and count the number of heads.
\eci
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L7(2)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Weak Law of Large Numbers: Result and Meaning}
\item \redf{Central Limit Theorem: Result and Meaning}
\item \grayf{Weak Law of Large Numbers: Proof

- Inequalities: Markov and Chebyshev

\item Central Limit Theorem: Proof

- Moment Generating Function (MGF)

%\item Strong Law of Large Numbers
}
\ece

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Central Limit Theorem: Start with Scaling (1)}
% $$
% M_n = \frac{S_n}{n} = \frac{X_1 + X_2+ \ldots X_n}{n}
% $$
% \myblock{Weak law of large numbers}
% {
% $M_n$ converges to $\mu$ in probability.
% }

\plitemsep 0.1in
 \bci
 \item<1-> Loosely speaking, WLLG says:
\aleq{
(M_n - \mu) \xrightarrow{n \rightarrow \infty} 0
}

\item<2-> However, we don't know \redf{how} $M_n -\mu$ converges to 0. For example, what's the speed of convergence?

\item<3-> \redf{Question.} What should be ``something"? Something should blow up for large $n$.
\aleq{
\onslide<3->{\redblk{\text{(something)}} & \times (M_n - \mu) \xrightarrow{n \rightarrow \infty} \text{meaningful thing}}\cr
\onslide<4->{\redblk{$n^\alpha$} &\times (M_n - \mu) \xrightarrow{n \rightarrow \infty} \text{meaningful thing}}
}
\item<5-> What's $\alpha$ for our magic?

\item<6-> The answer is $\redf{\mfrac{1}{2}}$

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Central Limit Theorem: Start with Scaling (2)}

\plitemsep 0.1in
\bci
\item Reshaping the equation:
\aleq{
\redf{\frac{\sqrt{n}}{\sigma}} &\times (M_n - \mu) = \onslide<2->{\sqrt{n}\lf(\frac{S_n - n\mu}{\sigma n}\ri) = \frac{S_n - n\mu}{\sigma\sqrt{n}}.}
}

\item<3-> Let $Z_n = \frac{S_n - n\mu}{\bluef{\sigma} \sqrt{n}}.$ Then, $\expect{Z_n} = 0$ and $\cvar{Z_n} = 1.$

\item<4-> $Z_n$ is well-centered with a variance irrespective of $n.$

\item<5-> We expect that $Z_n$ converges to something meaningful as $n \rightarrow \infty$, but what?

\item<6-> Some deterministic number just like WLLG?

\item<7-> Interestingly, it converges to some \redf{well-known random variable}.
\bci
\item<8-> Need a new concept of convergence: \orangef{``convergence in distribution''}
\eci

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Convergence in Distribution}

\plitemsep 0.1in
\bci
\item Consider a sequence of rvs $(Y_n)_{n=1,2, \ldots}$ and a rv $Y.$

\myblock{Convergence in Distribution: $Y_n \xrightarrow{\text{in dist.}} Y$}
{
\onslide<2->{
For every $y,$
$$
\cprob{Y_n \le y} \xrightarrow{n \rightarrow \infty} \cprob{Y \le y}
$$}
}

\item<3-> Another type of convergence of rvs

\item<4-> Comparison with convergence in probability?
\bci
\item<5-> Convergence in probability $\implies$ Convergence in distribution, but the reverse is not true.

\item<5-> The proof is beyond what this class covers, but it will be interesting to find an example that shows convergence in distribution, which is not convergence in probability.
\eci

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: in Distribution, but not in Probability}

\plitemsep 0.1in
\bci
\item<1-> $X_n \sim \text{Bernoulli}(1/2),$ for all $n \ge 1.$

\item<2-> $X = 1-X_n.$

\item<3-> Note that $X \sim \text{Bernoulli}(1/2).$ It means that the distributions of $X_n$ and $X$ are equal. It is trivial that $X_n$ converges to $X$ in distribution.

\item<4-> What about convergence in probability?
\aleq{
\cprob{|X_n - X| \geq \epsilon} &= \cprob{|X_n - 1 + X_n| \geq \epsilon} =
\cprob{|2X_n -1| \geq \epsilon} \cr
&= \cprob{1 \geq \epsilon} \qquad (\text{because $|2X_n-1| =1$})
}
\item<5-> We can find $\epsilon$ small enough so that the above does not go to zero.
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Central Limit Theorem: Formalism}


\plitemsep 0.15in
\bci
\item $S_n = X_1 + X_2 + \cdots + X_n,$ \hspace{1cm} $\displaystyle Z_n = \frac{S_n - n\mu}{\sigma \sqrt{n}}$

% \myblock{Central Limit Theorem}
% {
% For every $z,$
% $$
% \cprob{Z_n \le z} \xrightarrow{n \rightarrow \infty} \cprob{Z \le z},
% $$
% where $Z \sim N(0,1).$
% }
\myblock{Central Limit Theorem}
{
$Z_n$ convergens to $Z$ in distribution,
where $Z \sim \redf{\set{N}(0,1)}.$
}

\item<2-> Very surprising!

\item<2-> Irrespecitive of the distribution of $X_i,$ $Z$ is normal.
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{LLG vs. CLT: Different Scaling Glasses}


\plitemsep 0.15in
\bci
\item For simplicity, assume that $\cexpect{X_i}=0$ and $\cvar{X_i} =1,$ $i=1, 2, \ldots, n$

\item<2-> Law of Large Numbers
\mycolorbox{
 Scaling $S_n$ by $1/n,$ you go to a \bluef{deterministic} world.
}

\item<3-> Central Limit Theorem
\mycolorbox{
Scaling $S_n$ by $1/\sqrt{n}$, you still stay at the \bluef{random} world, but not an arbitrary random world. That's the \bluef{normal} random world, not depending on the distribution of each $X_i.$
}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Practical Use of CLT}

$Z_n = \dfrac{S_n - n\mu}{\sigma \sqrt{n}},$ \hspace{1cm} $\cprob{Z_n \le z} \xrightarrow{n \rightarrow \infty} \cprob{Z \le z}, \ Z \sim \set{N}(0,1)$

\plitemsep 0.1in
\bci
\item<2-> Can approximate $Z_n$ with a standard normal rv
\item<4-> Can approximate $S_n$ with a normal rv $\sim  (n\mu, n\sigma^2)$

- $S_n = n \mu + Z_n\sigma\sqrt{n}$
\item<5-> How large should $n$ be?
\bci
\item<6-> A moderate $n$ (20 or 30) usually works, which is the power of CLT.

\item<7-> If $X_i$ resembles a normal rv more, smaller $n$ works: symmetry and unimodality\footnotemark
\footnotetext{Only unique mode. A single maximum or minimum.}
\eci
\eci


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{CLT: Examples of Required $n$}

\centering
\mypic{0.92}{L7_CTL_ex1.png}


\mypic{0.9}{L7_CTL_ex2.png}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Examples of CLT (1)}

\mycolorbox{
\centering
$\cprob{S_\redf{n} \le \redf{a}} \approx \bluef{b}$: Given two parameters, find the third
}

\plitemsep 0.1in
\bci
\item Package weights $X_i$: iid exponential $\lambda=1/2$ ($\mu = 1/\lambda = 2$ and $\sigma^2 = 1/\lambda^2 = 4$ )

\item<2-> Load container with $n = 100$ packages

\aleq{
\cprob{S_{100} \geq 210} &= \bprob{\frac{S_{100} - 100\cdot 2}{2\sqrt{100}} \ge \frac{210-200}{20}} = \cprob{Z_{100} \geq 0.5} \cr
&
\onslide<3->{\approx \cprob{Z \ge 0.5} = 1- \cprob{Z \leq 0.5} = 1 - \Phi(0.5)}
}
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Examples of CLT (2)}

\mycolorbox{
\centering
$\cprob{S_\redf{n} \le \bluef{a}} \approx \redf{b}$: Given two parameters, find the third
}

\plitemsep 0.1in
\bci
\item Package weights $X_i$: iid exponential $\lambda=1/2$ ($\mu = 1/\lambda = 2$ and $\sigma^2 = 1/\lambda^2 = 4$ )

\item<2-> $n = 100$ packages, and choose the ``capacity" $a$, so that $\cprob{S_n \geq a} \approx 0.05$
\aleq{
\cprob{S_{100} \geq a} &= \bprob{\frac{S_{100} - 100\cdot 2}{2\sqrt{100}} \ge \frac{a-200}{20}} = \cprob{Z_{100} \geq \frac{a-200}{20}} \cr
& \onslide<3->{\approx \cprob{Z \ge \frac{a-200}{20}} = 1- \cprob{Z \leq \frac{a-200}{20}} = 1 - \Phi(\frac{a-200}{20}) = 0.05}
}

\item<4-> The value of $a$ such that $\Phi(\frac{a-200}{20}) = 0.95$? $\frac{a-200}{20} = 1.645$ and $a = 232.9$
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Examples of CLT (3)}

\mycolorbox{
\centering
$\cprob{S_\bluef{n} \le \redf{a}} \approx \redf{b}$: Given two parameters, find the third
}

\plitemsep 0.1in
\bci
\item Package weights $X_i$: iid exponential $\lambda=1/2$ ($\mu = 1/\lambda = 2$ and $\sigma^2 = 1/\lambda^2 = 4$ )

\item<2-> How large $n$, so that $\cprob{S_n \geq 210} \approx 0.05$?
\aleq{
\cprob{S_n \geq 210} &= \bprob{\frac{S_n - 2n}{2\sqrt{n}} \ge \frac{210-2n}{2\sqrt{n}}} \onslide<3->{\approx 1 - \Phi(\frac{210-2n}{2\sqrt{n}}) = 0.05}
}

\item<4-> The value of $n$ such that $\frac{210-2n}{2\sqrt{n}} = 1.645$? $n = 89$
\eci

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L7(3)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Weak Law of Large Numbers: Result and Meaning}
\item \grayf{Central Limit Theorem: Result and Meaning}
\item \redf{Weak Law of Large Numbers: Proof

- Inequalities: Markov and Chebyshev}

\item \grayf{Central Limit Theorem: Proof

- Moment Generating Function (MGF)

%\item Strong Law of Large Numbers

}
\ece

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Markov Inequality}

\plitemsep 0.1in
\bci
\item<1-> \redf{(Q)} Knowing \magenf{$\cexpect{X},$} can we say something about \magenf{the distribution of $X$}?

\item<2-> Intuition: small $\cexpect{X}$ $\imp$ small $\cprob{X \ge a}$

\onslide<3->{
\myblock{Markov Inequality}
{
If $X \geq 0$ and $a >0,$ then $\cprob{X \geq a} \le \frac{\cexpect{X}}{a}.$
}}
\mytwocols{0.4}
{
\onslide<4->{
\redf{Proof.} For any $a>0,$ define $Y_a$ as:
\aleq{
Y_a &\eqdef \begin{cases}
0, & \text{if} \ X < a,\cr
a,& \text{if} \ X \ge a
\end{cases}
}}
\onslide<5->{Then, using non-negativity of $X,$ $Y_a \le X,$ which leads to $\expect{Y_a} \leq \expect{X}.$ }
}
{
\onslide<6->{Note that we have:
\aleq{
\expect{Y_a} = a \cprob{Y_a = a} = a \cprob{X \ge a}.
}}
\onslide<7->{Thus, $a \cdot \cprob{X \ge a} \leq \expect{X}.$} \qed

% \medskip
% - \redf{(Q)} Why do we need $X >0$?
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Chebyshev Inequality}

\plitemsep 0.1in
\bci
\item<2-> \redf{(Q)} Knowing both \magenf{$\cexpect{X}$} and \magenf{$\cvar{X},$} can we say something about the \magenf{distribution of $X$}?

\item<3-> Intuition: small $\cvar{X}$ $\imp$ $X$ is unlikely to be too far away from its mean.

\item<4-> $\cexpect{X} = \mu,$ $\cvar{X} = \sigma^2.$
\onslide<5->{\myblock{Chebyshev Inequality}
{
$$
\cbprob{|X-\mu| \geq c} \leq \frac{\sigma^2}{c^2}, \quad \text{for all $c >0$}
$$
}}

\item<6-> \redf{Proof.}
\aleq{
\onslide<6->{\cbprob{|X-\mu| \geq c} = \cbprob{(X-\mu)^2 \geq c^2} \leq} \onslide<7->{\frac{\bexpect{(X-\mu)^2}}{c^2} = \frac{\cvar{X}}{c^2}}
}


\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: MI vs. CI}

\onslide<1->{- $X \sim \exp(1).$ Then, $\expect{X} = 1/\lambda = 1$ and $\var{X} = 1/\lambda^2 = 1.$}

\onslide<2->{- Exact CCDF: $\cprob{X \ge a} = e^{-a}$}

\bigskip
\mytwocols{0.3}
{
\plitemsep 0.1in
\bci
\item<3-> \redf{Markov inequality}
\aleq{
\cprob{X \ge a} &\le \frac{\expect{X}}{a} = \frac{1}{a}
}
\eci
}
{
\plitemsep 0.1in
\bci
\item<4-> \redf{Chebyshev inequality}
\aleq{
\cprob{X \ge a} &= \cprob{X-1 \ge a-1} \cr
&\le \cprob{|X-1| \ge a-1} \le \frac{1}{(a-1)^2}
}
\eci
}

\onslide<5->{- For reasonably large $a,$ CI provides much better bound. }

\onslide<6->{- Knowing the variance helps}

\onslide<7->{- Both bounds are the ones that bound the \bluef{probability of rare events.}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Back to WLLN Proof }
$$
M_n = \frac{S_n}{n} = \frac{X_1 + X_2+ \ldots X_n}{n}
$$
\myblock{Weak law of large numbers}
{
$M_n$ converges to $\mu$ in probability.
}


\redf{Proof.} For any given $\epsilon >0,$

\onslide<2->{\aleq{
\cbprob{|M_n - \mu| \ge \epsilon} \le \frac{\cvar{M_n}}{\epsilon^2} = \frac{\sigma^2}{n \epsilon^2} \xrightarrow{n \rightarrow \infty} 0
}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Comparison: WLLN vs. CLT }

{\Large We ask the same question, and try to answer it, using WLLN or CLT.}
\vspace{1cm}

{\Large See how the answers become different.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Polling using \orangef{WLLN}}

\plitemsep 0.07in

\bci

\item $p$: fraction of voters who support ``Yung".

\item<2-> Interview $n$ randomly selected voters and record the result in $M_n = \frac{X_1+ \ldots + X_n}{n}$ which is an estimate of $p,$ where the Bernoulli rv $X_i=1$ if $i$-th interviewee answers ``yes'', and 0 otherwise.

\item<3-> $\cprob{|M_n - p| \geq \epsilon} \leq \frac{\sigma^2}{n\epsilon^2} = \frac{p(1-p)}{n \epsilon^2} \leq \frac{1}{4 n \epsilon^2}$ (because $p(1-p) \leq 1/4$)

\item<4-> \question What is $n$ so that the probability that our estimate is incorrect by more than 0.1 is no larger than 0.25?
\bci
\item<5-> $\epsilon = 0.1$ and $\frac{1}{4 n \epsilon^2} \le 0.25$ $\implies$ $n \geq 100$
\eci

\item<6-> \question What is $n$ so that the probability that our estimate is incorrect by more than 0.01 is no larger than 0.05?
\bci
\item<7-> $\epsilon = 0.01$ and $\frac{1}{4 n \epsilon^2} \le 0.05$ $\implies$ $n \geq 50000$
\eci

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Polling using \orangef{CLT}}

\mycolorbox{
\aleq
{
&\cprob{|M_n - p| \geq \epsilon} = \onslide<2->{\bprob{\left|\frac{S_n - np}{n}\right| \geq \epsilon}} = \onslide<3->{\bprob{\left|\frac{S_n - np}{\sigma\sqrt{n}}\right| \geq \frac{\epsilon\sqrt{n}}{\sigma}}} \cr
&\leq \onslide<4->{\bprob{\left|\frac{S_n - np}{\sigma\sqrt{n}}\right| \geq 2\epsilon\sqrt{n}}}
 = \onslide<5->{\magenf{2\Big(1 - \Phi(2\epsilon \sqrt{n}) \Big)}}
\ \onslide<4->{( \text{because} \ \sigma = \sqrt{p(1-p)} \leq 1/2)}
}
}

\plitemsep 0.1in
\bci

\item<6-> \question What is $n$ so that the probability that our estimate is incorrect by more than 0.01 is no larger than 0.05?
\bci
\item<7-> $\epsilon = 0.01$ and $2\Big(1 - \Phi(2\epsilon \sqrt{n}) \Big) = 0.05$, i.e.,  $\Phi(2\epsilon \sqrt{n}) = 0.975$ $\implies$ $2\times 0.01 \times \sqrt{n}= 1.96$ and thus $n=9604$
\eci

\item<8-> Compare: 50,000 from LLN vs. 9604 from CLT

\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L7(4)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Weak Law of Large Numbers: Result and Meaning}
\item \grayf{Central Limit Theorem: Result and Meaning}
\item \grayf{Weak Law of Large Numbers: Proof

- Inequalities: Markov and Chebyshev}

\item \redf{Central Limit Theorem: Proof

- Moment Generating Function (MGF)}

%\item \grayf{Strong Law of Large Numbers}
\ece

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Moment Generating Function (MGF)}

\plitemsep 0.1in
\bci
\item<1-> For a rv $X,$ it is a kind of transform

\item<2-> The \redf{moment generating function (MGF)} $M_X(s)$ of a rv $X$ is a function of a scalar parameter $s$, defined by:
\mycolorbox{
\centering
$M_X(s) = \expect{e^{sX}}$
}
\aleq{
\onslide<3->{
M(s) &= \sum_x e^{sx} \px \quad \text{(discrete)}\cr
M(s) &= \int e^{sx} \fx dx \quad \text{(continuous)}\cr
}}
\vspace{-1cm}
\item<4-> If the context is clear, we omit $X$ and use just $\orangef{M(s)}.$

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Examples}

\mytwocols{0.75}
{
\small
\plitemsep 0.1in
\bci
\item<1->[Ex1)] Let $\px$ is given as:
\aleq
{
\px = \begin{cases}
1/2, & \text{if} \ x=2 \cr
1/6, & \text{if} \ x=3 \cr
1/3, & \text{if} \ x=5
\end{cases}
}
$M(s) = \cexpect{e^{sX}} = \frac{1}{2} e^{2s} + \frac{1}{6} e^{3s} + \frac{1}{3} e^{5s}$

\item<2->[Ex2)] $X \sim \exp(\lambda),$ $\fx = \lambda \elambdax, x\ge 0$
\aleq{
M(s) &= \lambda \int_{0}^\infty e^{sx} \elambdax dx \cr
&= \lambda \frac{e^{(s-\lambda)x}}{s-\lambda} \Bigg |_0^\infty \quad (\text{if  } s <\lambda) = \frac{\lambda}{\lambda - s}
}
\eci
}
{
\small
\plitemsep 0.1in
\bci

\item<3->[Ex3)] Let a rv $Y = aX +b.$
\aleq{
M_Y(s) &= \cexpect{e^{sY}} = \cexpect{e^{s(aX +b)}} \cr
&= e^{sb}\cexpect{e^{saX}} = e^{sb} M_X(sa)
}

\item<4->[Ex4)] $X \sim \set{N}(0,1)$
\aleq{
M(s) &= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} e^{sy} dy +
\frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{-\frac{y^2}{2} + sy} dy \cr
&= e^{\frac{s^2}{2}}\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty  e^{-\frac{(y-s)^2}{2}} dy\cr
&=e^{s^2/2} \ (\text{because it is the pdf of $\set{N}(s,1)$})
}

\item<5-> \question MGF of $\set{N}(\mu,\sigma^2)$?
\eci
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Useful Properties of MGF}

\plitemsep 0.1in
\bce
\item<1-> $M'(0) = \expect{X}$
\aleq{
\onslide<2->{\frac{d}{ds} M(s) &=\frac{d}{ds} \int_{-\infty}^\infty e^{sx} \fx dx = \int_{-\infty}^\infty \frac{d}{ds} e^{sx} \fx dx = \int_{-\infty}^\infty x e^{sx} \fx dx \cr
&= \frac{d}{ds} M(s) \Bigg |_{s=0} = \expect{X}}
}

\item<3-> Similarly, $M''(0) = \expect{X^2}$

\item<4-> $\frac{d^n}{ds^n} M(s) \Bigg |_{s=0} = \expect{X^n}$

\item<5-> MGF provides a convenient way of generating \orangef{moments}. That's why it is called moment generating function.

\ece


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example}

\plitemsep 0.1in
\bci
\item<1-> Exponential rv with parameter $\lambda.$ We know that $\cexpect{X} = 1/\lambda$ and
$\cvar{X} = 1/\lambda^2,$ which we will compute using the MGF.

\item<2-> Remind: $M(s) = \frac{\lambda}{\lambda -s}$

\item<3-> The first and the second moments are:
\aleq{
M'(s) &= \frac{\lambda}{(\lambda -s)^2}  \quad \rightarrow \quad \cexpect{X} = M'(0) = 1/\lambda \cr
M''(s) &= \frac{2\lambda}{(\lambda -s)^3} \quad \rightarrow \quad \cexpect{X^2} = M''(0) = 2/\lambda^2
}

\item<4-> Thus, $\cvar{X} = 2/\lambda^2 - 1/\lambda^2 = 1/\lambda^2$

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Inversion Property}

\myblock{Inversion Property}
{
The transform \magenf{$M_X(s)$} associated with a random variable $X$ \redf{uniquely} determines
the \magenf{CDF of $X$}, assuming that $M_X(s)$ is finite for all $s$ in some interval $[-a,a],$ where $a$ is a positive number.
}

\plitemsep 0.1in
\bci
\item In easy words, we can recover the distribution if we know the MGF.

\item Thus, each rv has its own unique MGF.

\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 1}

\plitemsep 0.15in
\bci
\item Given the following MGF of rv $X$, what is the distribution of $X$?
\aleq{
M(s) = \frac{1}{4}e^{-s} + \frac{1}{2} + \frac{1}{8} e^{4s} + \frac{1}{8}e^{5s}
}
\item<2-> Note that $M(s) = \sum_x e^{sx} \px$

\item<3-> We can see that
$$
p_X(-1) = \frac{1}{4},  \ p_X(0) = \frac{1}{2}, \ p_X(4) = \frac{1}{8}, \ p_X(5) = \frac{1}{8}
$$
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example 2}


\plitemsep 0.1in
\bci
\item Given the following MGF of rv $X$, what is the distribution of $X$?
\aleq{
M(s) = \frac{pe^s}{1-(1-p)e^s}
}
\item<2-> Note that $M(s) = \sum_x e^{sx} \px$

\item<3-> $M(s)$ can be reexpressed as the following geometric sum: when $(1-p)e^s <1,$
\aleq{
M(s) = pe^s \big(1 + (1-p)e^s + (1-p)^2 e^{2s} + (1-p)^3 e^{3s} + \cdots \big)
}

\item<4-> $p_X(k)$: coefficient of the term $e^{ks}$, which means:
\aleq{
p_X(1) = p, \ p_X(2) = p(1-p), \ p_X(3) = p(1-p)^2, \ p_X(4) = p(1-p)^3, \ldots
}

\item<5-> $X$ is a geometric rv with parameter $p$
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Back to CLT Proof (1)}

\plitemsep 0.1in
\bci
\item<1->  Without loss of generality, assume $\cexpect{X_i}=0$ and $\cvar{X_i} = 1$

\item<2->  $\displaystyle Z_n = \frac{S_n}{\sqrt{n}} = \frac{X_1 + X_2 + \ldots X_n}{\sqrt{n}}$

\item<3->  We will show: MGF of $Z_n$ converges to MFG of $\set{N}(0,1)$ (using inversion property)

\item<4-> \proff

\aleq{
\onslide<4->{\bexpect{e^{s S_n/\sqrt{n}}} &= \bexpect{e^{s X_1/\sqrt{n}}}\times \cdots \times\bexpect{e^{s X_n/\sqrt{n}}}}\cr
&=\onslide<5->{\lf ( \bexpect{e^{s X_1/\sqrt{n}}} \ri )^n} =
\onslide<6->{\lf( M_{X_1}\Bl(\frac{s}{\sqrt{n}}\Bl) \ri)^n}
}
\item<7-> For simplicity, let $M(\cdot) = M_{X_1}(\cdot)$
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Back to CLT Proof (2)}

\plitemsep 0.1in
\bci

\item \onslide<1->{ $M(0) = 1$, $M'(0) = 0,$ $M''(0) = 1$}

\item \onslide<2->{$\lf( M\Bl(\frac{s}{\sqrt{n}}\Bl) \ri)^n \xrightarrow{n\rightarrow \infty} \text{what???}$}

\item \onslide<3->{Taking log, $n \log M\Bl(\frac{s}{\sqrt{n}}\Bl)  \xrightarrow{n\rightarrow \infty} \text{what???}$}
\item \onslide<4->{For convenience, do the change of variable $y = \frac{1}{\sqrt{n}}.$ Then,
$
\displaystyle \lim_{y\rightarrow 0}\frac{\log M (ys)}{y^2}
$
}

\item \onslide<5->{If we apply l'hopital's rule twice (please check), we get
\aleq{
\lim_{y\rightarrow 0}\frac{\log M (ys)}{y^2} = \frac{s^2}{2}
}}
\qed
\eci
\end{frame}

\begin{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L7(5)}
\begin{frame}{Roadmap}

\plitemsep 0.15in

\bce[(1)]

\item \grayf{Weak Law of Large Numbers: Result and Meaning}
\item \grayf{Central Limit Theorem: Result and Meaning}
\item \grayf{Weak Law of Large Numbers: Proof

- Inequalities: Markov and Chebyshev}

\item \grayf{Central Limit Theorem: Proof

- Moment Generating Function (MGF)}

\item \redf{Strong Law of Large Numbers}
\ece

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Almost Sure Convergence}

\plitemsep 0.07in

\bci

\item<1-> What we want: \bluef{a sequence of rvs $(Y_n)_{n=1, 2, \ldots}$ converges to a rv $Y$} in some sense, different from \magenf{convergence in probability} and \magenf{convergence in distribution}.

\item<2-> \orangef{Almost Sure Convergence}: $Y_n \xrightarrow{\text{a.s.}} Y$
\mycolorbox{
\centering
$\bprob{\big \{\omega \in \Omega \mid \lim_{n \rightarrow \infty} Y_n(\omega) = Y(\omega) \big\}} = 1$
}

\bce
\item<3-> Take a random outcome $\omega \in \Omega.$
\item<4-> Then, the sequence $(Y_n(\omega))_{n=1,2, \ldots}$ and $Y(\omega)$ are all just real numbers.

\item<5-> Then, check whether the ordinary convergence $Y_n(\omega) \xrightarrow{n \rightarrow \infty} Y(\omega)$ holds or not. If yes, accept such $\omega$ and put it in $\Omega'$.

\item<6-> Repeat the above until we check all $\omega \in \Omega.$

\item<7->[$\circ$] If $\cprob{\Omega'} =1$, we achieve almost-sure convergence.
\ece


\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Convergence in Probability (2)}

\mycolorbox{
\bci
\item $\bprob{\big \{\omega \in \Omega \mid \lim_{n \rightarrow \infty} Y_n(\omega) = \redf{Y(\omega)} \big\}} = 1$

\item $\bprob{\big \{\omega \in \Omega \mid \lim_{n \rightarrow \infty} Y_n(\omega) = \redf{a} \big\}} = 1$

\eci
}

\plitemsep 0.03in

\bci

\item A special case: when $Y=a$ for some constant $a$: $Y_n \xrightarrow{\text{a.s.}} a$

\item A sequence of iid rvs $X_n \sim \set{U}[0,1]$, and let
$
Y_n = \min\{X_1, X_2, \ldots, X_n \}
$

\item We proved that $Y_n \xrightarrow{\text{in prob.}} 0.$ $Y_n \xrightarrow{\text{a.s.}} 0$?

\item
\eci
\end{frame}
\end{comment}

\begin{frame}{}
\vspace{2cm}
\LARGE Questions?

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Review Questions}

\bce[1)]

\item Explain the meaning of Markov inequality and Chebyshev
  inequality.
  
\item What are the practical values of LLN and CLT?

\item Explain LLN and CLT from the {\em scaling} perspective.

\item Why do we need different concepts of convergence for random variables?

\item Explain what is convergence in probability.

\item Explain what is convergence in distribution.

\item Why is MGF (Moment Generating Function) useful?

\item Prove CLT using MGF. 

\ece

\end{frame}

\end{document}
